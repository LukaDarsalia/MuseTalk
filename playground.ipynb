{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff6a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch models/sd-vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/sd-vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load unet model from ./models/musetalk/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from musetalk.utils.utils import load_all_model\n",
    "\n",
    "\n",
    "vae, unet, pe = load_all_model(\n",
    "    unet_model_path=\"./models/musetalk/pytorch_model.bin\",\n",
    "    vae_type=\"sd-vae\",\n",
    "    unet_config=\"./models/musetalk/musetalk.json\",\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24a976b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionModel(\n",
       "  (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0e5355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e607fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionModel(\n",
       "  (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f4c45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEncoding()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a065055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, feature_extractor_path=\"openai/whisper-tiny\"):\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(feature_extractor_path)\n",
    "\n",
    "    def get_audio_feature(self, wav_path, start_index=0, weight_dtype=None):\n",
    "        if not os.path.exists(wav_path):\n",
    "            return None\n",
    "        librosa_output, sampling_rate = librosa.load(wav_path, sr=16000)\n",
    "        assert sampling_rate == 16000\n",
    "        # Split audio into 30s segments\n",
    "        segment_length = 30 * sampling_rate\n",
    "        segments = [librosa_output[i:i + segment_length] for i in range(0, len(librosa_output), segment_length)]\n",
    "\n",
    "        features = []\n",
    "        for segment in segments:\n",
    "            audio_feature = self.feature_extractor(\n",
    "                segment,\n",
    "                return_tensors=\"pt\",\n",
    "                sampling_rate=sampling_rate\n",
    "            ).input_features\n",
    "            if weight_dtype is not None:\n",
    "                audio_feature = audio_feature.to(dtype=weight_dtype)\n",
    "            features.append(audio_feature)\n",
    "\n",
    "        return features, len(librosa_output)\n",
    "\n",
    "    def get_whisper_chunk(\n",
    "        self,\n",
    "        whisper_input_features,\n",
    "        device,\n",
    "        weight_dtype,\n",
    "        whisper,\n",
    "        librosa_length,\n",
    "        fps=25,\n",
    "        audio_padding_length_left=2,\n",
    "        audio_padding_length_right=2,\n",
    "    ):\n",
    "        audio_feature_length_per_frame = 2 * (audio_padding_length_left + audio_padding_length_right + 1)\n",
    "        whisper_feature = []\n",
    "        # Process multiple 30s mel input features\n",
    "        for input_feature in whisper_input_features:\n",
    "            input_feature = input_feature.to(device).to(weight_dtype)\n",
    "            audio_feats = whisper.encoder(input_feature, output_hidden_states=True).hidden_states\n",
    "            audio_feats = torch.stack(audio_feats, dim=2)\n",
    "            whisper_feature.append(audio_feats)\n",
    "\n",
    "        whisper_feature = torch.cat(whisper_feature, dim=1)\n",
    "        # Trim the last segment to remove padding\n",
    "        sr = 16000\n",
    "        audio_fps = 50\n",
    "        fps = int(fps)\n",
    "        whisper_idx_multiplier = audio_fps / fps\n",
    "        num_frames = math.floor((librosa_length / sr) * fps)\n",
    "        actual_length = math.floor((librosa_length / sr) * audio_fps)\n",
    "        whisper_feature = whisper_feature[:,:actual_length,...]\n",
    "\n",
    "        # Calculate padding amount\n",
    "        padding_nums = math.ceil(whisper_idx_multiplier)\n",
    "        # Add padding at start and end\n",
    "        whisper_feature = torch.cat([\n",
    "            torch.zeros_like(whisper_feature[:, :padding_nums * audio_padding_length_left]),\n",
    "            whisper_feature,\n",
    "            # Add extra padding to prevent out of bounds\n",
    "            torch.zeros_like(whisper_feature[:, :padding_nums * 3 * audio_padding_length_right])\n",
    "        ], 1)\n",
    "\n",
    "        audio_prompts = []\n",
    "        for frame_index in range(num_frames):\n",
    "            try:\n",
    "                audio_index = math.floor(frame_index * whisper_idx_multiplier)\n",
    "                audio_clip = whisper_feature[:, audio_index: audio_index + audio_feature_length_per_frame]\n",
    "                assert audio_clip.shape[1] == audio_feature_length_per_frame\n",
    "                audio_prompts.append(audio_clip)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                print(f\"whisper_feature.shape: {whisper_feature.shape}\")\n",
    "                print(f\"audio_clip.shape: {audio_clip.shape}\")\n",
    "                print(f\"num frames: {num_frames}, fps: {fps}, whisper_idx_multiplier: {whisper_idx_multiplier}\")\n",
    "                print(f\"frame_index: {frame_index}, audio_index: {audio_index}-{audio_index + audio_feature_length_per_frame}\")\n",
    "                exit()\n",
    "\n",
    "        audio_prompts = torch.cat(audio_prompts, dim=0)  # T, 10, 5, 384\n",
    "        audio_prompts = rearrange(audio_prompts, 'b c h w -> b (c h) w')\n",
    "        return audio_prompts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a54901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m wav_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/audio/piradoba.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m audio_feature, librosa_feature_length \u001b[38;5;241m=\u001b[39m audio_processor\u001b[38;5;241m.\u001b[39mget_audio_feature(wav_path)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio Feature shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43maudio_feature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa_feature_length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, librosa_feature_length)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "audio_processor = AudioProcessor()\n",
    "wav_path = './data/audio/piradoba.wav'\n",
    "audio_feature, librosa_feature_length = audio_processor.get_audio_feature(wav_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55d4cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.4280, -0.0149, -0.1384,  ..., -0.7431, -0.7431, -0.7431],\n",
       "          [-0.5044, -0.1854, -0.2437,  ..., -0.7431, -0.7431, -0.7431],\n",
       "          [-0.4600, -0.2401, -0.3250,  ..., -0.7431, -0.7431, -0.7431],\n",
       "          ...,\n",
       "          [-0.7431, -0.7431, -0.7431,  ..., -0.7431, -0.7431, -0.7431],\n",
       "          [-0.7431, -0.7431, -0.7431,  ..., -0.7431, -0.7431, -0.7431],\n",
       "          [-0.7431, -0.7431, -0.7431,  ..., -0.7431, -0.7431, -0.7431]]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77305e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Feature shape: torch.Size([1, 80, 3000])\n",
      "librosa_feature_length: 106496\n"
     ]
    }
   ],
   "source": [
    "print(\"Audio Feature shape:\", audio_feature[0].shape)\n",
    "print(\"librosa_feature_length:\", librosa_feature_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7758fe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from musetalk.models.vae import VAE\n",
    "from musetalk.models.unet import UNet,PositionalEncoding\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class MuseTalkGenerator(nn.Module):\n",
    "    \"\"\"Unified generator that wraps positional encoding, UNet, and VAE decoding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        whisper_encoder: nn.Module,\n",
    "        vae: nn.Module,\n",
    "        unet: nn.Module,\n",
    "        pe: nn.Module,\n",
    "        audio_padding_length_left: int = 2,\n",
    "        audio_padding_length_right: int = 2,\n",
    "        video_fps: int = 25,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.whisper_encoder = whisper_encoder\n",
    "        self.whisper_encoder_device = next(self.whisper_encoder.parameters()).device\n",
    "\n",
    "        self.vae = vae\n",
    "        self.vae_device = next(self.vae.parameters()).device\n",
    "        self.scaling_factor = self.vae.config.scaling_factor\n",
    "        self.unet = unet\n",
    "        self.unet_device = next(self.unet.parameters()).device\n",
    "        self.timesteps = torch.tensor([0], device=self.unet_device)\n",
    "        self.pe = pe\n",
    "\n",
    "        self.audio_padding_length_left = audio_padding_length_left\n",
    "        self.audio_padding_length_right = audio_padding_length_right\n",
    "        self.video_fps = video_fps\n",
    "\n",
    "    def decode_latents(self, latents):\n",
    "        \"\"\"\n",
    "        Decode latent variables back into an image.\n",
    "        :param latents: The latent variables to decode.\n",
    "        :return: A NumPy array representing the decoded image.\n",
    "        \"\"\"\n",
    "        latents = (1/  self.scaling_factor) * latents\n",
    "        image = self.vae.decode(latents.to(self.vae.dtype)).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = image[...,::-1] # RGB to BGR\n",
    "        return image\n",
    "\n",
    "    def get_whisper_chunk(\n",
    "        self,\n",
    "        whisper_input_features,\n",
    "        device,\n",
    "        weight_dtype,\n",
    "        librosa_length,\n",
    "        fps=25,\n",
    "    ):\n",
    "        audio_feature_length_per_frame = 2 * (self.audio_padding_length_left + self.audio_padding_length_right + 1)\n",
    "        whisper_feature = []\n",
    "        # Process multiple 30s mel input features\n",
    "        for input_feature in whisper_input_features:\n",
    "            input_feature = input_feature.to(device).to(weight_dtype)\n",
    "            audio_feats = self.whisper_encoder(input_feature, output_hidden_states=True).hidden_states\n",
    "            audio_feats = torch.stack(audio_feats, dim=2)\n",
    "            whisper_feature.append(audio_feats)\n",
    "\n",
    "        whisper_feature = torch.cat(whisper_feature, dim=1)\n",
    "        # Trim the last segment to remove padding\n",
    "        sr = 16000\n",
    "        audio_fps = 50\n",
    "        fps = int(fps)\n",
    "        whisper_idx_multiplier = audio_fps / fps\n",
    "        num_frames = math.floor((librosa_length / sr) * fps)\n",
    "        actual_length = math.floor((librosa_length / sr) * audio_fps)\n",
    "        whisper_feature = whisper_feature[:,:actual_length,...]\n",
    "\n",
    "        # Calculate padding amount\n",
    "        padding_nums = math.ceil(whisper_idx_multiplier)\n",
    "        # Add padding at start and end\n",
    "        whisper_feature = torch.cat([\n",
    "            torch.zeros_like(whisper_feature[:, :padding_nums * self.audio_padding_length_left]),\n",
    "            whisper_feature,\n",
    "            # Add extra padding to prevent out of bounds\n",
    "            torch.zeros_like(whisper_feature[:, :padding_nums * 3 * self.audio_padding_length_right])\n",
    "        ], 1)\n",
    "\n",
    "        audio_prompts = []\n",
    "        for frame_index in range(num_frames):\n",
    "            audio_index = math.floor(frame_index * whisper_idx_multiplier)\n",
    "            audio_clip = whisper_feature[:, audio_index: audio_index + audio_feature_length_per_frame]\n",
    "            audio_prompts.append(audio_clip)\n",
    "\n",
    "        audio_prompts = torch.cat(audio_prompts, dim=0)  # T, 10, 5, 384\n",
    "        audio_prompts = rearrange(audio_prompts, 'b c h w -> b (c h) w')\n",
    "        return audio_prompts\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        whisper_input_features: List[torch.Tensor],\n",
    "        latent_inputs: torch.Tensor,\n",
    "        frame_idx: int,\n",
    "        librosa_length: int,\n",
    "    ):\n",
    "        whisper_chunks = self.get_whisper_chunk(\n",
    "            whisper_input_features=whisper_input_features,\n",
    "            device=self.whisper_encoder_device,\n",
    "            weight_dtype=self.whisper_encoder.dtype,\n",
    "            librosa_length=librosa_length,\n",
    "            fps=self.video_fps,\n",
    "        )\n",
    "\n",
    "        batch_size = latent_inputs.shape[0]\n",
    "        context_whisper_chunk = whisper_chunks[frame_idx: frame_idx+batch_size]  # shape [batch_size, 50, 384]\n",
    "        \n",
    "        audio_feature_batch = self.pe(context_whisper_chunk.to(self.unet_device))\n",
    "        latent_batch = latent_inputs.to(device=self.unet_device, dtype=self.unet.dtype)\n",
    "\n",
    "        pred_latents = self.unet(\n",
    "            latent_batch,\n",
    "            self.timesteps,\n",
    "            encoder_hidden_states=audio_feature_batch,\n",
    "        ).sample\n",
    "\n",
    "        pred_latents = pred_latents.to(device=self.vae_device, dtype=self.vae.dtype)\n",
    "        recon = self.decode_latents(pred_latents)\n",
    "        return recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed5d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a12105d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch models/sd-vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/sd-vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load unet model from ./models/musetalk/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from musetalk.utils.utils import load_all_model\n",
    "\n",
    "\n",
    "vae, unet, pe = load_all_model(\n",
    "    unet_model_path=\"./models/musetalk/pytorch_model.bin\",\n",
    "    vae_type=\"sd-vae\",\n",
    "    unet_config=\"./models/musetalk/musetalk.json\",\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc6c0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEncoding()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c0d9c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperModel(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 384)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperSdpaAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): WhisperDecoder(\n",
       "    (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "    (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x WhisperDecoderLayer(\n",
       "        (self_attn): WhisperSdpaAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): WhisperSdpaAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperModel\n",
    "whisper = WhisperModel.from_pretrained('./models/whisper')\n",
    "whisper = whisper.to(device='cuda', dtype=unet.model.dtype).eval()\n",
    "whisper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71c5fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MuseTalkGenerator(\n",
    "    whisper_encoder=whisper.encoder,\n",
    "    vae=vae.vae,\n",
    "    unet=unet.model,\n",
    "    pe=pe,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcd16b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MuseTalkGenerator(\n",
       "  (whisper_encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 384)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperSdpaAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vae): AutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (up_blocks): ModuleList(\n",
       "        (0-1): 2 x UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (unet): UNet2DConditionModel(\n",
       "    (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_proj): Timesteps()\n",
       "    (time_embedding): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=384, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=384, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2DCrossAttn(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=384, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (pe): PositionalEncoding()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d78d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 11.59 GiB of which 108.56 MiB is free. Including non-PyTorch memory, this process has 10.66 GiB memory in use. Of the allocated memory 10.11 GiB is allocated by PyTorch, and 368.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhisper_input_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43mlatent_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43mframe_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43mlibrosa_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m106_496\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[29], line 133\u001b[0m, in \u001b[0;36mMuseTalkGenerator.forward\u001b[0;34m(self, whisper_input_features, latent_inputs, frame_idx, librosa_length)\u001b[0m\n\u001b[1;32m    126\u001b[0m pred_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(\n\u001b[1;32m    127\u001b[0m     latent_batch,\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps,\n\u001b[1;32m    129\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39maudio_feature_batch,\n\u001b[1;32m    130\u001b[0m )\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    132\u001b[0m pred_latents \u001b[38;5;241m=\u001b[39m pred_latents\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_device, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 133\u001b[0m recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_latents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recon\n",
      "Cell \u001b[0;32mIn[29], line 49\u001b[0m, in \u001b[0;36mMuseTalkGenerator.decode_latents\u001b[0;34m(self, latents)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mDecode latent variables back into an image.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m:param latents: The latent variables to decode.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m:return: A NumPy array representing the decoded image.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m latents \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor) \u001b[38;5;241m*\u001b[39m latents\n\u001b[0;32m---> 49\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     50\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:321\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:292\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 292\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:337\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[0;32m--> 337\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2746\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, temb: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets:\n\u001b[0;32m-> 2746\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2749\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[1;32m    327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(hidden_states)\n\u001b[0;32m--> 328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonlinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:473\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/functional.py:2371\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 11.59 GiB of which 108.56 MiB is free. Including non-PyTorch memory, this process has 10.66 GiB memory in use. Of the allocated memory 10.11 GiB is allocated by PyTorch, and 368.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model(whisper_input_features = [torch.randn(1, 80, 3000)],\n",
    "latent_inputs = torch.randn(8, 8, 32, 32),\n",
    "frame_idx = torch.tensor(0, dtype=torch.long),\n",
    "librosa_length = torch.tensor(106_496, dtype=torch.long),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bbdf1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MuseTalkGenerator([...]` with `torch.export.export(..., strict=False)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[384, 80, 3]\", arg1_1: \"f32[384]\", arg2_1: \"f32[384, 384, 3]\", arg3_1: \"f32[384]\", arg4_1: \"f32[1500, 384]\", arg5_1: \"f32[384, 384]\", arg6_1: \"f32[384, 384]\", arg7_1: \"f32[384]\", arg8_1: \"f32[384, 384]\", arg9_1: \"f32[384]\", arg10_1: \"f32[384, 384]\", arg11_1: \"f32[384]\", arg12_1: \"f32[384]\", arg13_1: \"f32[384]\", arg14_1: \"f32[1536, 384]\", arg15_1: \"f32[1536]\", arg16_1: \"f32[384, 1536]\", arg17_1: \"f32[384]\", arg18_1: \"f32[384]\", arg19_1: \"f32[384]\", arg20_1: \"f32[384, 384]\", arg21_1: \"f32[384, 384]\", arg22_1: \"f32[384]\", arg23_1: \"f32[384, 384]\", arg24_1: \"f32[384]\", arg25_1: \"f32[384, 384]\", arg26_1: \"f32[384]\", arg27_1: \"f32[384]\", arg28_1: \"f32[384]\", arg29_1: \"f32[1536, 384]\", arg30_1: \"f32[1536]\", arg31_1: \"f32[384, 1536]\", arg32_1: \"f32[384]\", arg33_1: \"f32[384]\", arg34_1: \"f32[384]\", arg35_1: \"f32[384, 384]\", arg36_1: \"f32[384, 384]\", arg37_1: \"f32[384]\", arg38_1: \"f32[384, 384]\", arg39_1: \"f32[384]\", arg40_1: \"f32[384, 384]\", arg41_1: \"f32[384]\", arg42_1: \"f32[384]\", arg43_1: \"f32[384]\", arg44_1: \"f32[1536, 384]\", arg45_1: \"f32[1536]\", arg46_1: \"f32[384, 1536]\", arg47_1: \"f32[384]\", arg48_1: \"f32[384]\", arg49_1: \"f32[384]\", arg50_1: \"f32[384, 384]\", arg51_1: \"f32[384, 384]\", arg52_1: \"f32[384]\", arg53_1: \"f32[384, 384]\", arg54_1: \"f32[384]\", arg55_1: \"f32[384, 384]\", arg56_1: \"f32[384]\", arg57_1: \"f32[384]\", arg58_1: \"f32[384]\", arg59_1: \"f32[1536, 384]\", arg60_1: \"f32[1536]\", arg61_1: \"f32[384, 1536]\", arg62_1: \"f32[384]\", arg63_1: \"f32[384]\", arg64_1: \"f32[384]\", arg65_1: \"f32[384]\", arg66_1: \"f32[384]\", arg67_1: \"f32[128, 3, 3, 3]\", arg68_1: \"f32[128]\", arg69_1: \"f32[128]\", arg70_1: \"f32[128]\", arg71_1: \"f32[128, 128, 3, 3]\", arg72_1: \"f32[128]\", arg73_1: \"f32[128]\", arg74_1: \"f32[128]\", arg75_1: \"f32[128, 128, 3, 3]\", arg76_1: \"f32[128]\", arg77_1: \"f32[128]\", arg78_1: \"f32[128]\", arg79_1: \"f32[128, 128, 3, 3]\", arg80_1: \"f32[128]\", arg81_1: \"f32[128]\", arg82_1: \"f32[128]\", arg83_1: \"f32[128, 128, 3, 3]\", arg84_1: \"f32[128]\", arg85_1: \"f32[128, 128, 3, 3]\", arg86_1: \"f32[128]\", arg87_1: \"f32[128]\", arg88_1: \"f32[128]\", arg89_1: \"f32[256, 128, 3, 3]\", arg90_1: \"f32[256]\", arg91_1: \"f32[256]\", arg92_1: \"f32[256]\", arg93_1: \"f32[256, 256, 3, 3]\", arg94_1: \"f32[256]\", arg95_1: \"f32[256, 128, 1, 1]\", arg96_1: \"f32[256]\", arg97_1: \"f32[256]\", arg98_1: \"f32[256]\", arg99_1: \"f32[256, 256, 3, 3]\", arg100_1: \"f32[256]\", arg101_1: \"f32[256]\", arg102_1: \"f32[256]\", arg103_1: \"f32[256, 256, 3, 3]\", arg104_1: \"f32[256]\", arg105_1: \"f32[256, 256, 3, 3]\", arg106_1: \"f32[256]\", arg107_1: \"f32[256]\", arg108_1: \"f32[256]\", arg109_1: \"f32[512, 256, 3, 3]\", arg110_1: \"f32[512]\", arg111_1: \"f32[512]\", arg112_1: \"f32[512]\", arg113_1: \"f32[512, 512, 3, 3]\", arg114_1: \"f32[512]\", arg115_1: \"f32[512, 256, 1, 1]\", arg116_1: \"f32[512]\", arg117_1: \"f32[512]\", arg118_1: \"f32[512]\", arg119_1: \"f32[512, 512, 3, 3]\", arg120_1: \"f32[512]\", arg121_1: \"f32[512]\", arg122_1: \"f32[512]\", arg123_1: \"f32[512, 512, 3, 3]\", arg124_1: \"f32[512]\", arg125_1: \"f32[512, 512, 3, 3]\", arg126_1: \"f32[512]\", arg127_1: \"f32[512]\", arg128_1: \"f32[512]\", arg129_1: \"f32[512, 512, 3, 3]\", arg130_1: \"f32[512]\", arg131_1: \"f32[512]\", arg132_1: \"f32[512]\", arg133_1: \"f32[512, 512, 3, 3]\", arg134_1: \"f32[512]\", arg135_1: \"f32[512]\", arg136_1: \"f32[512]\", arg137_1: \"f32[512, 512, 3, 3]\", arg138_1: \"f32[512]\", arg139_1: \"f32[512]\", arg140_1: \"f32[512]\", arg141_1: \"f32[512, 512, 3, 3]\", arg142_1: \"f32[512]\", arg143_1: \"f32[512]\", arg144_1: \"f32[512]\", arg145_1: \"f32[512, 512]\", arg146_1: \"f32[512]\", arg147_1: \"f32[512, 512]\", arg148_1: \"f32[512]\", arg149_1: \"f32[512, 512]\", arg150_1: \"f32[512]\", arg151_1: \"f32[512, 512]\", arg152_1: \"f32[512]\", arg153_1: \"f32[512]\", arg154_1: \"f32[512]\", arg155_1: \"f32[512, 512, 3, 3]\", arg156_1: \"f32[512]\", arg157_1: \"f32[512]\", arg158_1: \"f32[512]\", arg159_1: \"f32[512, 512, 3, 3]\", arg160_1: \"f32[512]\", arg161_1: \"f32[512]\", arg162_1: \"f32[512]\", arg163_1: \"f32[512, 512, 3, 3]\", arg164_1: \"f32[512]\", arg165_1: \"f32[512]\", arg166_1: \"f32[512]\", arg167_1: \"f32[512, 512, 3, 3]\", arg168_1: \"f32[512]\", arg169_1: \"f32[512]\", arg170_1: \"f32[512]\", arg171_1: \"f32[8, 512, 3, 3]\", arg172_1: \"f32[8]\", arg173_1: \"f32[512, 4, 3, 3]\", arg174_1: \"f32[512]\", arg175_1: \"f32[512]\", arg176_1: \"f32[512]\", arg177_1: \"f32[512, 512, 3, 3]\", arg178_1: \"f32[512]\", arg179_1: \"f32[512]\", arg180_1: \"f32[512]\", arg181_1: \"f32[512, 512, 3, 3]\", arg182_1: \"f32[512]\", arg183_1: \"f32[512]\", arg184_1: \"f32[512]\", arg185_1: \"f32[512, 512, 3, 3]\", arg186_1: \"f32[512]\", arg187_1: \"f32[512]\", arg188_1: \"f32[512]\", arg189_1: \"f32[512, 512, 3, 3]\", arg190_1: \"f32[512]\", arg191_1: \"f32[512]\", arg192_1: \"f32[512]\", arg193_1: \"f32[512, 512, 3, 3]\", arg194_1: \"f32[512]\", arg195_1: \"f32[512]\", arg196_1: \"f32[512]\", arg197_1: \"f32[512, 512, 3, 3]\", arg198_1: \"f32[512]\", arg199_1: \"f32[512, 512, 3, 3]\", arg200_1: \"f32[512]\", arg201_1: \"f32[512]\", arg202_1: \"f32[512]\", arg203_1: \"f32[512, 512, 3, 3]\", arg204_1: \"f32[512]\", arg205_1: \"f32[512]\", arg206_1: \"f32[512]\", arg207_1: \"f32[512, 512, 3, 3]\", arg208_1: \"f32[512]\", arg209_1: \"f32[512]\", arg210_1: \"f32[512]\", arg211_1: \"f32[512, 512, 3, 3]\", arg212_1: \"f32[512]\", arg213_1: \"f32[512]\", arg214_1: \"f32[512]\", arg215_1: \"f32[512, 512, 3, 3]\", arg216_1: \"f32[512]\", arg217_1: \"f32[512]\", arg218_1: \"f32[512]\", arg219_1: \"f32[512, 512, 3, 3]\", arg220_1: \"f32[512]\", arg221_1: \"f32[512]\", arg222_1: \"f32[512]\", arg223_1: \"f32[512, 512, 3, 3]\", arg224_1: \"f32[512]\", arg225_1: \"f32[512, 512, 3, 3]\", arg226_1: \"f32[512]\", arg227_1: \"f32[512]\", arg228_1: \"f32[512]\", arg229_1: \"f32[256, 512, 3, 3]\", arg230_1: \"f32[256]\", arg231_1: \"f32[256]\", arg232_1: \"f32[256]\", arg233_1: \"f32[256, 256, 3, 3]\", arg234_1: \"f32[256]\", arg235_1: \"f32[256, 512, 1, 1]\", arg236_1: \"f32[256]\", arg237_1: \"f32[256]\", arg238_1: \"f32[256]\", arg239_1: \"f32[256, 256, 3, 3]\", arg240_1: \"f32[256]\", arg241_1: \"f32[256]\", arg242_1: \"f32[256]\", arg243_1: \"f32[256, 256, 3, 3]\", arg244_1: \"f32[256]\", arg245_1: \"f32[256]\", arg246_1: \"f32[256]\", arg247_1: \"f32[256, 256, 3, 3]\", arg248_1: \"f32[256]\", arg249_1: \"f32[256]\", arg250_1: \"f32[256]\", arg251_1: \"f32[256, 256, 3, 3]\", arg252_1: \"f32[256]\", arg253_1: \"f32[256, 256, 3, 3]\", arg254_1: \"f32[256]\", arg255_1: \"f32[256]\", arg256_1: \"f32[256]\", arg257_1: \"f32[128, 256, 3, 3]\", arg258_1: \"f32[128]\", arg259_1: \"f32[128]\", arg260_1: \"f32[128]\", arg261_1: \"f32[128, 128, 3, 3]\", arg262_1: \"f32[128]\", arg263_1: \"f32[128, 256, 1, 1]\", arg264_1: \"f32[128]\", arg265_1: \"f32[128]\", arg266_1: \"f32[128]\", arg267_1: \"f32[128, 128, 3, 3]\", arg268_1: \"f32[128]\", arg269_1: \"f32[128]\", arg270_1: \"f32[128]\", arg271_1: \"f32[128, 128, 3, 3]\", arg272_1: \"f32[128]\", arg273_1: \"f32[128]\", arg274_1: \"f32[128]\", arg275_1: \"f32[128, 128, 3, 3]\", arg276_1: \"f32[128]\", arg277_1: \"f32[128]\", arg278_1: \"f32[128]\", arg279_1: \"f32[128, 128, 3, 3]\", arg280_1: \"f32[128]\", arg281_1: \"f32[512]\", arg282_1: \"f32[512]\", arg283_1: \"f32[512, 512]\", arg284_1: \"f32[512]\", arg285_1: \"f32[512, 512]\", arg286_1: \"f32[512]\", arg287_1: \"f32[512, 512]\", arg288_1: \"f32[512]\", arg289_1: \"f32[512, 512]\", arg290_1: \"f32[512]\", arg291_1: \"f32[512]\", arg292_1: \"f32[512]\", arg293_1: \"f32[512, 512, 3, 3]\", arg294_1: \"f32[512]\", arg295_1: \"f32[512]\", arg296_1: \"f32[512]\", arg297_1: \"f32[512, 512, 3, 3]\", arg298_1: \"f32[512]\", arg299_1: \"f32[512]\", arg300_1: \"f32[512]\", arg301_1: \"f32[512, 512, 3, 3]\", arg302_1: \"f32[512]\", arg303_1: \"f32[512]\", arg304_1: \"f32[512]\", arg305_1: \"f32[512, 512, 3, 3]\", arg306_1: \"f32[512]\", arg307_1: \"f32[128]\", arg308_1: \"f32[128]\", arg309_1: \"f32[3, 128, 3, 3]\", arg310_1: \"f32[3]\", arg311_1: \"f32[8, 8, 1, 1]\", arg312_1: \"f32[8]\", arg313_1: \"f32[4, 4, 1, 1]\", arg314_1: \"f32[4]\", arg315_1: \"f32[320, 8, 3, 3]\", arg316_1: \"f32[320]\", arg317_1: \"f32[1280, 320]\", arg318_1: \"f32[1280]\", arg319_1: \"f32[1280, 1280]\", arg320_1: \"f32[1280]\", arg321_1: \"f32[320]\", arg322_1: \"f32[320]\", arg323_1: \"f32[320, 320, 1, 1]\", arg324_1: \"f32[320]\", arg325_1: \"f32[320]\", arg326_1: \"f32[320]\", arg327_1: \"f32[320, 320]\", arg328_1: \"f32[320, 320]\", arg329_1: \"f32[320, 320]\", arg330_1: \"f32[320, 320]\", arg331_1: \"f32[320]\", arg332_1: \"f32[320]\", arg333_1: \"f32[320]\", arg334_1: \"f32[320, 320]\", arg335_1: \"f32[320, 384]\", arg336_1: \"f32[320, 384]\", arg337_1: \"f32[320, 320]\", arg338_1: \"f32[320]\", arg339_1: \"f32[320]\", arg340_1: \"f32[320]\", arg341_1: \"f32[2560, 320]\", arg342_1: \"f32[2560]\", arg343_1: \"f32[320, 1280]\", arg344_1: \"f32[320]\", arg345_1: \"f32[320, 320, 1, 1]\", arg346_1: \"f32[320]\", arg347_1: \"f32[320]\", arg348_1: \"f32[320]\", arg349_1: \"f32[320, 320, 1, 1]\", arg350_1: \"f32[320]\", arg351_1: \"f32[320]\", arg352_1: \"f32[320]\", arg353_1: \"f32[320, 320]\", arg354_1: \"f32[320, 320]\", arg355_1: \"f32[320, 320]\", arg356_1: \"f32[320, 320]\", arg357_1: \"f32[320]\", arg358_1: \"f32[320]\", arg359_1: \"f32[320]\", arg360_1: \"f32[320, 320]\", arg361_1: \"f32[320, 384]\", arg362_1: \"f32[320, 384]\", arg363_1: \"f32[320, 320]\", arg364_1: \"f32[320]\", arg365_1: \"f32[320]\", arg366_1: \"f32[320]\", arg367_1: \"f32[2560, 320]\", arg368_1: \"f32[2560]\", arg369_1: \"f32[320, 1280]\", arg370_1: \"f32[320]\", arg371_1: \"f32[320, 320, 1, 1]\", arg372_1: \"f32[320]\", arg373_1: \"f32[320]\", arg374_1: \"f32[320]\", arg375_1: \"f32[320, 320, 3, 3]\", arg376_1: \"f32[320]\", arg377_1: \"f32[320, 1280]\", arg378_1: \"f32[320]\", arg379_1: \"f32[320]\", arg380_1: \"f32[320]\", arg381_1: \"f32[320, 320, 3, 3]\", arg382_1: \"f32[320]\", arg383_1: \"f32[320]\", arg384_1: \"f32[320]\", arg385_1: \"f32[320, 320, 3, 3]\", arg386_1: \"f32[320]\", arg387_1: \"f32[320, 1280]\", arg388_1: \"f32[320]\", arg389_1: \"f32[320]\", arg390_1: \"f32[320]\", arg391_1: \"f32[320, 320, 3, 3]\", arg392_1: \"f32[320]\", arg393_1: \"f32[320, 320, 3, 3]\", arg394_1: \"f32[320]\", arg395_1: \"f32[640]\", arg396_1: \"f32[640]\", arg397_1: \"f32[640, 640, 1, 1]\", arg398_1: \"f32[640]\", arg399_1: \"f32[640]\", arg400_1: \"f32[640]\", arg401_1: \"f32[640, 640]\", arg402_1: \"f32[640, 640]\", arg403_1: \"f32[640, 640]\", arg404_1: \"f32[640, 640]\", arg405_1: \"f32[640]\", arg406_1: \"f32[640]\", arg407_1: \"f32[640]\", arg408_1: \"f32[640, 640]\", arg409_1: \"f32[640, 384]\", arg410_1: \"f32[640, 384]\", arg411_1: \"f32[640, 640]\", arg412_1: \"f32[640]\", arg413_1: \"f32[640]\", arg414_1: \"f32[640]\", arg415_1: \"f32[5120, 640]\", arg416_1: \"f32[5120]\", arg417_1: \"f32[640, 2560]\", arg418_1: \"f32[640]\", arg419_1: \"f32[640, 640, 1, 1]\", arg420_1: \"f32[640]\", arg421_1: \"f32[640]\", arg422_1: \"f32[640]\", arg423_1: \"f32[640, 640, 1, 1]\", arg424_1: \"f32[640]\", arg425_1: \"f32[640]\", arg426_1: \"f32[640]\", arg427_1: \"f32[640, 640]\", arg428_1: \"f32[640, 640]\", arg429_1: \"f32[640, 640]\", arg430_1: \"f32[640, 640]\", arg431_1: \"f32[640]\", arg432_1: \"f32[640]\", arg433_1: \"f32[640]\", arg434_1: \"f32[640, 640]\", arg435_1: \"f32[640, 384]\", arg436_1: \"f32[640, 384]\", arg437_1: \"f32[640, 640]\", arg438_1: \"f32[640]\", arg439_1: \"f32[640]\", arg440_1: \"f32[640]\", arg441_1: \"f32[5120, 640]\", arg442_1: \"f32[5120]\", arg443_1: \"f32[640, 2560]\", arg444_1: \"f32[640]\", arg445_1: \"f32[640, 640, 1, 1]\", arg446_1: \"f32[640]\", arg447_1: \"f32[320]\", arg448_1: \"f32[320]\", arg449_1: \"f32[640, 320, 3, 3]\", arg450_1: \"f32[640]\", arg451_1: \"f32[640, 1280]\", arg452_1: \"f32[640]\", arg453_1: \"f32[640]\", arg454_1: \"f32[640]\", arg455_1: \"f32[640, 640, 3, 3]\", arg456_1: \"f32[640]\", arg457_1: \"f32[640, 320, 1, 1]\", arg458_1: \"f32[640]\", arg459_1: \"f32[640]\", arg460_1: \"f32[640]\", arg461_1: \"f32[640, 640, 3, 3]\", arg462_1: \"f32[640]\", arg463_1: \"f32[640, 1280]\", arg464_1: \"f32[640]\", arg465_1: \"f32[640]\", arg466_1: \"f32[640]\", arg467_1: \"f32[640, 640, 3, 3]\", arg468_1: \"f32[640]\", arg469_1: \"f32[640, 640, 3, 3]\", arg470_1: \"f32[640]\", arg471_1: \"f32[1280]\", arg472_1: \"f32[1280]\", arg473_1: \"f32[1280, 1280, 1, 1]\", arg474_1: \"f32[1280]\", arg475_1: \"f32[1280]\", arg476_1: \"f32[1280]\", arg477_1: \"f32[1280, 1280]\", arg478_1: \"f32[1280, 1280]\", arg479_1: \"f32[1280, 1280]\", arg480_1: \"f32[1280, 1280]\", arg481_1: \"f32[1280]\", arg482_1: \"f32[1280]\", arg483_1: \"f32[1280]\", arg484_1: \"f32[1280, 1280]\", arg485_1: \"f32[1280, 384]\", arg486_1: \"f32[1280, 384]\", arg487_1: \"f32[1280, 1280]\", arg488_1: \"f32[1280]\", arg489_1: \"f32[1280]\", arg490_1: \"f32[1280]\", arg491_1: \"f32[10240, 1280]\", arg492_1: \"f32[10240]\", arg493_1: \"f32[1280, 5120]\", arg494_1: \"f32[1280]\", arg495_1: \"f32[1280, 1280, 1, 1]\", arg496_1: \"f32[1280]\", arg497_1: \"f32[1280]\", arg498_1: \"f32[1280]\", arg499_1: \"f32[1280, 1280, 1, 1]\", arg500_1: \"f32[1280]\", arg501_1: \"f32[1280]\", arg502_1: \"f32[1280]\", arg503_1: \"f32[1280, 1280]\", arg504_1: \"f32[1280, 1280]\", arg505_1: \"f32[1280, 1280]\", arg506_1: \"f32[1280, 1280]\", arg507_1: \"f32[1280]\", arg508_1: \"f32[1280]\", arg509_1: \"f32[1280]\", arg510_1: \"f32[1280, 1280]\", arg511_1: \"f32[1280, 384]\", arg512_1: \"f32[1280, 384]\", arg513_1: \"f32[1280, 1280]\", arg514_1: \"f32[1280]\", arg515_1: \"f32[1280]\", arg516_1: \"f32[1280]\", arg517_1: \"f32[10240, 1280]\", arg518_1: \"f32[10240]\", arg519_1: \"f32[1280, 5120]\", arg520_1: \"f32[1280]\", arg521_1: \"f32[1280, 1280, 1, 1]\", arg522_1: \"f32[1280]\", arg523_1: \"f32[640]\", arg524_1: \"f32[640]\", arg525_1: \"f32[1280, 640, 3, 3]\", arg526_1: \"f32[1280]\", arg527_1: \"f32[1280, 1280]\", arg528_1: \"f32[1280]\", arg529_1: \"f32[1280]\", arg530_1: \"f32[1280]\", arg531_1: \"f32[1280, 1280, 3, 3]\", arg532_1: \"f32[1280]\", arg533_1: \"f32[1280, 640, 1, 1]\", arg534_1: \"f32[1280]\", arg535_1: \"f32[1280]\", arg536_1: \"f32[1280]\", arg537_1: \"f32[1280, 1280, 3, 3]\", arg538_1: \"f32[1280]\", arg539_1: \"f32[1280, 1280]\", arg540_1: \"f32[1280]\", arg541_1: \"f32[1280]\", arg542_1: \"f32[1280]\", arg543_1: \"f32[1280, 1280, 3, 3]\", arg544_1: \"f32[1280]\", arg545_1: \"f32[1280, 1280, 3, 3]\", arg546_1: \"f32[1280]\", arg547_1: \"f32[1280]\", arg548_1: \"f32[1280]\", arg549_1: \"f32[1280, 1280, 3, 3]\", arg550_1: \"f32[1280]\", arg551_1: \"f32[1280, 1280]\", arg552_1: \"f32[1280]\", arg553_1: \"f32[1280]\", arg554_1: \"f32[1280]\", arg555_1: \"f32[1280, 1280, 3, 3]\", arg556_1: \"f32[1280]\", arg557_1: \"f32[1280]\", arg558_1: \"f32[1280]\", arg559_1: \"f32[1280, 1280, 3, 3]\", arg560_1: \"f32[1280]\", arg561_1: \"f32[1280, 1280]\", arg562_1: \"f32[1280]\", arg563_1: \"f32[1280]\", arg564_1: \"f32[1280]\", arg565_1: \"f32[1280, 1280, 3, 3]\", arg566_1: \"f32[1280]\", arg567_1: \"f32[2560]\", arg568_1: \"f32[2560]\", arg569_1: \"f32[1280, 2560, 3, 3]\", arg570_1: \"f32[1280]\", arg571_1: \"f32[1280, 1280]\", arg572_1: \"f32[1280]\", arg573_1: \"f32[1280]\", arg574_1: \"f32[1280]\", arg575_1: \"f32[1280, 1280, 3, 3]\", arg576_1: \"f32[1280]\", arg577_1: \"f32[1280, 2560, 1, 1]\", arg578_1: \"f32[1280]\", arg579_1: \"f32[2560]\", arg580_1: \"f32[2560]\", arg581_1: \"f32[1280, 2560, 3, 3]\", arg582_1: \"f32[1280]\", arg583_1: \"f32[1280, 1280]\", arg584_1: \"f32[1280]\", arg585_1: \"f32[1280]\", arg586_1: \"f32[1280]\", arg587_1: \"f32[1280, 1280, 3, 3]\", arg588_1: \"f32[1280]\", arg589_1: \"f32[1280, 2560, 1, 1]\", arg590_1: \"f32[1280]\", arg591_1: \"f32[2560]\", arg592_1: \"f32[2560]\", arg593_1: \"f32[1280, 2560, 3, 3]\", arg594_1: \"f32[1280]\", arg595_1: \"f32[1280, 1280]\", arg596_1: \"f32[1280]\", arg597_1: \"f32[1280]\", arg598_1: \"f32[1280]\", arg599_1: \"f32[1280, 1280, 3, 3]\", arg600_1: \"f32[1280]\", arg601_1: \"f32[1280, 2560, 1, 1]\", arg602_1: \"f32[1280]\", arg603_1: \"f32[1280, 1280, 3, 3]\", arg604_1: \"f32[1280]\", arg605_1: \"f32[1280]\", arg606_1: \"f32[1280]\", arg607_1: \"f32[1280, 1280, 1, 1]\", arg608_1: \"f32[1280]\", arg609_1: \"f32[1280]\", arg610_1: \"f32[1280]\", arg611_1: \"f32[1280, 1280]\", arg612_1: \"f32[1280, 1280]\", arg613_1: \"f32[1280, 1280]\", arg614_1: \"f32[1280, 1280]\", arg615_1: \"f32[1280]\", arg616_1: \"f32[1280]\", arg617_1: \"f32[1280]\", arg618_1: \"f32[1280, 1280]\", arg619_1: \"f32[1280, 384]\", arg620_1: \"f32[1280, 384]\", arg621_1: \"f32[1280, 1280]\", arg622_1: \"f32[1280]\", arg623_1: \"f32[1280]\", arg624_1: \"f32[1280]\", arg625_1: \"f32[10240, 1280]\", arg626_1: \"f32[10240]\", arg627_1: \"f32[1280, 5120]\", arg628_1: \"f32[1280]\", arg629_1: \"f32[1280, 1280, 1, 1]\", arg630_1: \"f32[1280]\", arg631_1: \"f32[1280]\", arg632_1: \"f32[1280]\", arg633_1: \"f32[1280, 1280, 1, 1]\", arg634_1: \"f32[1280]\", arg635_1: \"f32[1280]\", arg636_1: \"f32[1280]\", arg637_1: \"f32[1280, 1280]\", arg638_1: \"f32[1280, 1280]\", arg639_1: \"f32[1280, 1280]\", arg640_1: \"f32[1280, 1280]\", arg641_1: \"f32[1280]\", arg642_1: \"f32[1280]\", arg643_1: \"f32[1280]\", arg644_1: \"f32[1280, 1280]\", arg645_1: \"f32[1280, 384]\", arg646_1: \"f32[1280, 384]\", arg647_1: \"f32[1280, 1280]\", arg648_1: \"f32[1280]\", arg649_1: \"f32[1280]\", arg650_1: \"f32[1280]\", arg651_1: \"f32[10240, 1280]\", arg652_1: \"f32[10240]\", arg653_1: \"f32[1280, 5120]\", arg654_1: \"f32[1280]\", arg655_1: \"f32[1280, 1280, 1, 1]\", arg656_1: \"f32[1280]\", arg657_1: \"f32[1280]\", arg658_1: \"f32[1280]\", arg659_1: \"f32[1280, 1280, 1, 1]\", arg660_1: \"f32[1280]\", arg661_1: \"f32[1280]\", arg662_1: \"f32[1280]\", arg663_1: \"f32[1280, 1280]\", arg664_1: \"f32[1280, 1280]\", arg665_1: \"f32[1280, 1280]\", arg666_1: \"f32[1280, 1280]\", arg667_1: \"f32[1280]\", arg668_1: \"f32[1280]\", arg669_1: \"f32[1280]\", arg670_1: \"f32[1280, 1280]\", arg671_1: \"f32[1280, 384]\", arg672_1: \"f32[1280, 384]\", arg673_1: \"f32[1280, 1280]\", arg674_1: \"f32[1280]\", arg675_1: \"f32[1280]\", arg676_1: \"f32[1280]\", arg677_1: \"f32[10240, 1280]\", arg678_1: \"f32[10240]\", arg679_1: \"f32[1280, 5120]\", arg680_1: \"f32[1280]\", arg681_1: \"f32[1280, 1280, 1, 1]\", arg682_1: \"f32[1280]\", arg683_1: \"f32[2560]\", arg684_1: \"f32[2560]\", arg685_1: \"f32[1280, 2560, 3, 3]\", arg686_1: \"f32[1280]\", arg687_1: \"f32[1280, 1280]\", arg688_1: \"f32[1280]\", arg689_1: \"f32[1280]\", arg690_1: \"f32[1280]\", arg691_1: \"f32[1280, 1280, 3, 3]\", arg692_1: \"f32[1280]\", arg693_1: \"f32[1280, 2560, 1, 1]\", arg694_1: \"f32[1280]\", arg695_1: \"f32[2560]\", arg696_1: \"f32[2560]\", arg697_1: \"f32[1280, 2560, 3, 3]\", arg698_1: \"f32[1280]\", arg699_1: \"f32[1280, 1280]\", arg700_1: \"f32[1280]\", arg701_1: \"f32[1280]\", arg702_1: \"f32[1280]\", arg703_1: \"f32[1280, 1280, 3, 3]\", arg704_1: \"f32[1280]\", arg705_1: \"f32[1280, 2560, 1, 1]\", arg706_1: \"f32[1280]\", arg707_1: \"f32[1920]\", arg708_1: \"f32[1920]\", arg709_1: \"f32[1280, 1920, 3, 3]\", arg710_1: \"f32[1280]\", arg711_1: \"f32[1280, 1280]\", arg712_1: \"f32[1280]\", arg713_1: \"f32[1280]\", arg714_1: \"f32[1280]\", arg715_1: \"f32[1280, 1280, 3, 3]\", arg716_1: \"f32[1280]\", arg717_1: \"f32[1280, 1920, 1, 1]\", arg718_1: \"f32[1280]\", arg719_1: \"f32[1280, 1280, 3, 3]\", arg720_1: \"f32[1280]\", arg721_1: \"f32[640]\", arg722_1: \"f32[640]\", arg723_1: \"f32[640, 640, 1, 1]\", arg724_1: \"f32[640]\", arg725_1: \"f32[640]\", arg726_1: \"f32[640]\", arg727_1: \"f32[640, 640]\", arg728_1: \"f32[640, 640]\", arg729_1: \"f32[640, 640]\", arg730_1: \"f32[640, 640]\", arg731_1: \"f32[640]\", arg732_1: \"f32[640]\", arg733_1: \"f32[640]\", arg734_1: \"f32[640, 640]\", arg735_1: \"f32[640, 384]\", arg736_1: \"f32[640, 384]\", arg737_1: \"f32[640, 640]\", arg738_1: \"f32[640]\", arg739_1: \"f32[640]\", arg740_1: \"f32[640]\", arg741_1: \"f32[5120, 640]\", arg742_1: \"f32[5120]\", arg743_1: \"f32[640, 2560]\", arg744_1: \"f32[640]\", arg745_1: \"f32[640, 640, 1, 1]\", arg746_1: \"f32[640]\", arg747_1: \"f32[640]\", arg748_1: \"f32[640]\", arg749_1: \"f32[640, 640, 1, 1]\", arg750_1: \"f32[640]\", arg751_1: \"f32[640]\", arg752_1: \"f32[640]\", arg753_1: \"f32[640, 640]\", arg754_1: \"f32[640, 640]\", arg755_1: \"f32[640, 640]\", arg756_1: \"f32[640, 640]\", arg757_1: \"f32[640]\", arg758_1: \"f32[640]\", arg759_1: \"f32[640]\", arg760_1: \"f32[640, 640]\", arg761_1: \"f32[640, 384]\", arg762_1: \"f32[640, 384]\", arg763_1: \"f32[640, 640]\", arg764_1: \"f32[640]\", arg765_1: \"f32[640]\", arg766_1: \"f32[640]\", arg767_1: \"f32[5120, 640]\", arg768_1: \"f32[5120]\", arg769_1: \"f32[640, 2560]\", arg770_1: \"f32[640]\", arg771_1: \"f32[640, 640, 1, 1]\", arg772_1: \"f32[640]\", arg773_1: \"f32[640]\", arg774_1: \"f32[640]\", arg775_1: \"f32[640, 640, 1, 1]\", arg776_1: \"f32[640]\", arg777_1: \"f32[640]\", arg778_1: \"f32[640]\", arg779_1: \"f32[640, 640]\", arg780_1: \"f32[640, 640]\", arg781_1: \"f32[640, 640]\", arg782_1: \"f32[640, 640]\", arg783_1: \"f32[640]\", arg784_1: \"f32[640]\", arg785_1: \"f32[640]\", arg786_1: \"f32[640, 640]\", arg787_1: \"f32[640, 384]\", arg788_1: \"f32[640, 384]\", arg789_1: \"f32[640, 640]\", arg790_1: \"f32[640]\", arg791_1: \"f32[640]\", arg792_1: \"f32[640]\", arg793_1: \"f32[5120, 640]\", arg794_1: \"f32[5120]\", arg795_1: \"f32[640, 2560]\", arg796_1: \"f32[640]\", arg797_1: \"f32[640, 640, 1, 1]\", arg798_1: \"f32[640]\", arg799_1: \"f32[1920]\", arg800_1: \"f32[1920]\", arg801_1: \"f32[640, 1920, 3, 3]\", arg802_1: \"f32[640]\", arg803_1: \"f32[640, 1280]\", arg804_1: \"f32[640]\", arg805_1: \"f32[640]\", arg806_1: \"f32[640]\", arg807_1: \"f32[640, 640, 3, 3]\", arg808_1: \"f32[640]\", arg809_1: \"f32[640, 1920, 1, 1]\", arg810_1: \"f32[640]\", arg811_1: \"f32[1280]\", arg812_1: \"f32[1280]\", arg813_1: \"f32[640, 1280, 3, 3]\", arg814_1: \"f32[640]\", arg815_1: \"f32[640, 1280]\", arg816_1: \"f32[640]\", arg817_1: \"f32[640]\", arg818_1: \"f32[640]\", arg819_1: \"f32[640, 640, 3, 3]\", arg820_1: \"f32[640]\", arg821_1: \"f32[640, 1280, 1, 1]\", arg822_1: \"f32[640]\", arg823_1: \"f32[960]\", arg824_1: \"f32[960]\", arg825_1: \"f32[640, 960, 3, 3]\", arg826_1: \"f32[640]\", arg827_1: \"f32[640, 1280]\", arg828_1: \"f32[640]\", arg829_1: \"f32[640]\", arg830_1: \"f32[640]\", arg831_1: \"f32[640, 640, 3, 3]\", arg832_1: \"f32[640]\", arg833_1: \"f32[640, 960, 1, 1]\", arg834_1: \"f32[640]\", arg835_1: \"f32[640, 640, 3, 3]\", arg836_1: \"f32[640]\", arg837_1: \"f32[320]\", arg838_1: \"f32[320]\", arg839_1: \"f32[320, 320, 1, 1]\", arg840_1: \"f32[320]\", arg841_1: \"f32[320]\", arg842_1: \"f32[320]\", arg843_1: \"f32[320, 320]\", arg844_1: \"f32[320, 320]\", arg845_1: \"f32[320, 320]\", arg846_1: \"f32[320, 320]\", arg847_1: \"f32[320]\", arg848_1: \"f32[320]\", arg849_1: \"f32[320]\", arg850_1: \"f32[320, 320]\", arg851_1: \"f32[320, 384]\", arg852_1: \"f32[320, 384]\", arg853_1: \"f32[320, 320]\", arg854_1: \"f32[320]\", arg855_1: \"f32[320]\", arg856_1: \"f32[320]\", arg857_1: \"f32[2560, 320]\", arg858_1: \"f32[2560]\", arg859_1: \"f32[320, 1280]\", arg860_1: \"f32[320]\", arg861_1: \"f32[320, 320, 1, 1]\", arg862_1: \"f32[320]\", arg863_1: \"f32[320]\", arg864_1: \"f32[320]\", arg865_1: \"f32[320, 320, 1, 1]\", arg866_1: \"f32[320]\", arg867_1: \"f32[320]\", arg868_1: \"f32[320]\", arg869_1: \"f32[320, 320]\", arg870_1: \"f32[320, 320]\", arg871_1: \"f32[320, 320]\", arg872_1: \"f32[320, 320]\", arg873_1: \"f32[320]\", arg874_1: \"f32[320]\", arg875_1: \"f32[320]\", arg876_1: \"f32[320, 320]\", arg877_1: \"f32[320, 384]\", arg878_1: \"f32[320, 384]\", arg879_1: \"f32[320, 320]\", arg880_1: \"f32[320]\", arg881_1: \"f32[320]\", arg882_1: \"f32[320]\", arg883_1: \"f32[2560, 320]\", arg884_1: \"f32[2560]\", arg885_1: \"f32[320, 1280]\", arg886_1: \"f32[320]\", arg887_1: \"f32[320, 320, 1, 1]\", arg888_1: \"f32[320]\", arg889_1: \"f32[320]\", arg890_1: \"f32[320]\", arg891_1: \"f32[320, 320, 1, 1]\", arg892_1: \"f32[320]\", arg893_1: \"f32[320]\", arg894_1: \"f32[320]\", arg895_1: \"f32[320, 320]\", arg896_1: \"f32[320, 320]\", arg897_1: \"f32[320, 320]\", arg898_1: \"f32[320, 320]\", arg899_1: \"f32[320]\", arg900_1: \"f32[320]\", arg901_1: \"f32[320]\", arg902_1: \"f32[320, 320]\", arg903_1: \"f32[320, 384]\", arg904_1: \"f32[320, 384]\", arg905_1: \"f32[320, 320]\", arg906_1: \"f32[320]\", arg907_1: \"f32[320]\", arg908_1: \"f32[320]\", arg909_1: \"f32[2560, 320]\", arg910_1: \"f32[2560]\", arg911_1: \"f32[320, 1280]\", arg912_1: \"f32[320]\", arg913_1: \"f32[320, 320, 1, 1]\", arg914_1: \"f32[320]\", arg915_1: \"f32[960]\", arg916_1: \"f32[960]\", arg917_1: \"f32[320, 960, 3, 3]\", arg918_1: \"f32[320]\", arg919_1: \"f32[320, 1280]\", arg920_1: \"f32[320]\", arg921_1: \"f32[320]\", arg922_1: \"f32[320]\", arg923_1: \"f32[320, 320, 3, 3]\", arg924_1: \"f32[320]\", arg925_1: \"f32[320, 960, 1, 1]\", arg926_1: \"f32[320]\", arg927_1: \"f32[640]\", arg928_1: \"f32[640]\", arg929_1: \"f32[320, 640, 3, 3]\", arg930_1: \"f32[320]\", arg931_1: \"f32[320, 1280]\", arg932_1: \"f32[320]\", arg933_1: \"f32[320]\", arg934_1: \"f32[320]\", arg935_1: \"f32[320, 320, 3, 3]\", arg936_1: \"f32[320]\", arg937_1: \"f32[320, 640, 1, 1]\", arg938_1: \"f32[320]\", arg939_1: \"f32[640]\", arg940_1: \"f32[640]\", arg941_1: \"f32[320, 640, 3, 3]\", arg942_1: \"f32[320]\", arg943_1: \"f32[320, 1280]\", arg944_1: \"f32[320]\", arg945_1: \"f32[320]\", arg946_1: \"f32[320]\", arg947_1: \"f32[320, 320, 3, 3]\", arg948_1: \"f32[320]\", arg949_1: \"f32[320, 640, 1, 1]\", arg950_1: \"f32[320]\", arg951_1: \"f32[1280]\", arg952_1: \"f32[1280]\", arg953_1: \"f32[1280, 1280, 1, 1]\", arg954_1: \"f32[1280]\", arg955_1: \"f32[1280]\", arg956_1: \"f32[1280]\", arg957_1: \"f32[1280, 1280]\", arg958_1: \"f32[1280, 1280]\", arg959_1: \"f32[1280, 1280]\", arg960_1: \"f32[1280, 1280]\", arg961_1: \"f32[1280]\", arg962_1: \"f32[1280]\", arg963_1: \"f32[1280]\", arg964_1: \"f32[1280, 1280]\", arg965_1: \"f32[1280, 384]\", arg966_1: \"f32[1280, 384]\", arg967_1: \"f32[1280, 1280]\", arg968_1: \"f32[1280]\", arg969_1: \"f32[1280]\", arg970_1: \"f32[1280]\", arg971_1: \"f32[10240, 1280]\", arg972_1: \"f32[10240]\", arg973_1: \"f32[1280, 5120]\", arg974_1: \"f32[1280]\", arg975_1: \"f32[1280, 1280, 1, 1]\", arg976_1: \"f32[1280]\", arg977_1: \"f32[1280]\", arg978_1: \"f32[1280]\", arg979_1: \"f32[1280, 1280, 3, 3]\", arg980_1: \"f32[1280]\", arg981_1: \"f32[1280, 1280]\", arg982_1: \"f32[1280]\", arg983_1: \"f32[1280]\", arg984_1: \"f32[1280]\", arg985_1: \"f32[1280, 1280, 3, 3]\", arg986_1: \"f32[1280]\", arg987_1: \"f32[1280]\", arg988_1: \"f32[1280]\", arg989_1: \"f32[1280, 1280, 3, 3]\", arg990_1: \"f32[1280]\", arg991_1: \"f32[1280, 1280]\", arg992_1: \"f32[1280]\", arg993_1: \"f32[1280]\", arg994_1: \"f32[1280]\", arg995_1: \"f32[1280, 1280, 3, 3]\", arg996_1: \"f32[1280]\", arg997_1: \"f32[320]\", arg998_1: \"f32[320]\", arg999_1: \"f32[4, 320, 3, 3]\", arg1000_1: \"f32[4]\", arg1001_1: \"f32[1, 5000, 384]\", arg1002_1: \"f32[1, 80, 3000]\", arg1003_1: \"f32[8, 8, 32, 32]\", arg1004_1: \"i64[]\", arg1005_1: \"i64[]\"):\n",
      "     # File: /tmp/ipykernel_67451/113760652.py:110 in forward, code: frame_idx = frame_idx.item()\n",
      "    item: \"Sym(u0)\" = torch.ops.aten.item.default(arg1004_1);  arg1004_1 = item = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:111 in forward, code: librosa_length = librosa_length.item()\n",
      "    item_1: \"Sym(u1)\" = torch.ops.aten.item.default(arg1005_1);  arg1005_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:112 in forward, code: whisper_chunks = self.get_whisper_chunk(\n",
      "    to: \"f32[1, 80, 3000]\" = torch.ops.aten.to.dtype_layout(arg1002_1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  arg1002_1 = None\n",
      "    to_1: \"f32[1, 80, 3000]\" = torch.ops.aten.to.dtype(to, torch.float32);  to = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv1d: \"f32[1, 384, 3000]\" = torch.ops.aten.conv1d.default(to_1, arg0_1, arg1_1, [1], [1]);  to_1 = arg0_1 = arg1_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1175 in forward, code: inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
      "    gelu: \"f32[1, 384, 3000]\" = torch.ops.aten.gelu.default(conv1d);  conv1d = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv1d_1: \"f32[1, 384, 1500]\" = torch.ops.aten.conv1d.default(gelu, arg2_1, arg3_1, [2], [1]);  gelu = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1176 in forward, code: inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
      "    gelu_1: \"f32[1, 384, 1500]\" = torch.ops.aten.gelu.default(conv1d_1);  conv1d_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1178 in forward, code: inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
      "    permute: \"f32[1, 1500, 384]\" = torch.ops.aten.permute.default(gelu_1, [0, 2, 1]);  gelu_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1181 in forward, code: hidden_states = inputs_embeds + embed_pos\n",
      "    add: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(permute, arg4_1);  permute = arg4_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1182 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(dropout, [384], arg12_1, arg13_1);  arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg8_1, arg9_1);  arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg5_1);  arg5_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_1, [1, -1, 6, 64]);  linear_1 = None\n",
      "    transpose: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
      "    contiguous: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose);  transpose = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_1: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_2, [1, -1, 6, 64]);  linear_2 = None\n",
      "    transpose_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
      "    contiguous_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_1);  transpose_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_2: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear, [1, 1500, 6, 64]);  linear = None\n",
      "    transpose_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
      "    contiguous_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_2);  transpose_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_2, contiguous, contiguous_1);  contiguous_2 = contiguous = contiguous_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_3, [1, 1500, 384]);  transpose_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape, arg10_1, arg11_1);  reshape = arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_1: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_1: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_1: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_1, [384], arg18_1, arg19_1);  arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_1, arg14_1, arg15_1);  layer_norm_1 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_2, arg16_1, arg17_1);  dropout_2 = arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_3: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_2: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_2: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_2, [384], arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg20_1);  arg20_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_7, [1, -1, 6, 64]);  linear_7 = None\n",
      "    transpose_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None\n",
      "    contiguous_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_4);  transpose_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg21_1, arg22_1);  layer_norm_2 = arg21_1 = arg22_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_4: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_8, [1, -1, 6, 64]);  linear_8 = None\n",
      "    transpose_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
      "    contiguous_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_5);  transpose_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_5: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_6, [1, 1500, 6, 64]);  linear_6 = None\n",
      "    transpose_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
      "    contiguous_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_5, contiguous_3, contiguous_4);  contiguous_5 = contiguous_3 = contiguous_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_1: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_7, [1, 1500, 384]);  transpose_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_1, arg25_1, arg26_1);  reshape_1 = arg25_1 = arg26_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_4: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_3: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  dropout_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_3: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_3, [384], arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_3, arg29_1, arg30_1);  layer_norm_3 = arg29_1 = arg30_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_3: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_5, arg31_1, arg32_1);  dropout_5 = arg31_1 = arg32_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_6: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_4: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_4: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_4, [384], arg42_1, arg43_1);  arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg38_1, arg39_1);  arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg35_1);  arg35_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_6: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_13, [1, -1, 6, 64]);  linear_13 = None\n",
      "    transpose_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
      "    contiguous_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_8);  transpose_8 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg36_1, arg37_1);  layer_norm_4 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_14, [1, -1, 6, 64]);  linear_14 = None\n",
      "    transpose_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_7, 1, 2);  view_7 = None\n",
      "    contiguous_7: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_9);  transpose_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_8: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_12, [1, 1500, 6, 64]);  linear_12 = None\n",
      "    transpose_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
      "    contiguous_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_10);  transpose_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_8, contiguous_6, contiguous_7);  contiguous_8 = contiguous_6 = contiguous_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_2: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_11, [1, 1500, 384]);  transpose_11 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_2, arg40_1, arg41_1);  reshape_2 = arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_7: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_5: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  dropout_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_5: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_5, [384], arg48_1, arg49_1);  arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_5, arg44_1, arg45_1);  layer_norm_5 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_8: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_8, arg46_1, arg47_1);  dropout_8 = arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_9: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_6: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_6: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg50_1);  arg50_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_9: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_19, [1, -1, 6, 64]);  linear_19 = None\n",
      "    transpose_12: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
      "    contiguous_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_12);  transpose_12 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg51_1, arg52_1);  layer_norm_6 = arg51_1 = arg52_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_10: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_20, [1, -1, 6, 64]);  linear_20 = None\n",
      "    transpose_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
      "    contiguous_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_13);  transpose_13 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_18, [1, 1500, 6, 64]);  linear_18 = None\n",
      "    transpose_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_11, 1, 2);  view_11 = None\n",
      "    contiguous_11: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_14);  transpose_14 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_11, contiguous_9, contiguous_10);  contiguous_11 = contiguous_9 = contiguous_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_15: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_3: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_15, [1, 1500, 384]);  transpose_15 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_3, arg55_1, arg56_1);  reshape_3 = arg55_1 = arg56_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_10: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_7: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  dropout_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_7: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_7, [384], arg63_1, arg64_1);  arg63_1 = arg64_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_7, arg59_1, arg60_1);  layer_norm_7 = arg59_1 = arg60_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_11: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_11, arg61_1, arg62_1);  dropout_11 = arg61_1 = arg62_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_12: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_8: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_8: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_8, [384], arg65_1, arg66_1);  add_8 = arg65_1 = arg66_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:112 in forward, code: whisper_chunks = self.get_whisper_chunk(\n",
      "    stack: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.stack.default([dropout, add_2, add_4, add_6, layer_norm_8], 2);  dropout = add_2 = add_4 = add_6 = layer_norm_8 = None\n",
      "    cat: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.cat.default([stack], 1);  stack = None\n",
      "    truediv: \"Sym(IntTrueDiv(u1, 16000))\" = item_1 / 16000\n",
      "    mul: \"Sym(25.0*(IntTrueDiv(u1, 16000)))\" = truediv * 25.0;  truediv = None\n",
      "    floor: \"Sym(FloorToInt(25.0*(IntTrueDiv(u1, 16000))))\" = math_floor(mul);  mul = floor = None\n",
      "    truediv_1: \"Sym(IntTrueDiv(u1, 16000))\" = item_1 / 16000;  item_1 = None\n",
      "    mul_1: \"Sym(50.0*(IntTrueDiv(u1, 16000)))\" = truediv_1 * 50.0;  truediv_1 = None\n",
      "    floor_1: \"Sym(FloorToInt(50.0*(IntTrueDiv(u1, 16000))))\" = math_floor(mul_1);  mul_1 = None\n",
      "    slice_1 = torch.ops.aten.slice.Tensor(cat, 1, None, floor_1);  cat = floor_1 = slice_1 = None\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[384, 80, 3]\", arg1_1: \"f32[384]\", arg2_1: \"f32[384, 384, 3]\", arg3_1: \"f32[384]\", arg4_1: \"f32[1500, 384]\", arg5_1: \"f32[384, 384]\", arg6_1: \"f32[384, 384]\", arg7_1: \"f32[384]\", arg8_1: \"f32[384, 384]\", arg9_1: \"f32[384]\", arg10_1: \"f32[384, 384]\", arg11_1: \"f32[384]\", arg12_1: \"f32[384]\", arg13_1: \"f32[384]\", arg14_1: \"f32[1536, 384]\", arg15_1: \"f32[1536]\", arg16_1: \"f32[384, 1536]\", arg17_1: \"f32[384]\", arg18_1: \"f32[384]\", arg19_1: \"f32[384]\", arg20_1: \"f32[384, 384]\", arg21_1: \"f32[384, 384]\", arg22_1: \"f32[384]\", arg23_1: \"f32[384, 384]\", arg24_1: \"f32[384]\", arg25_1: \"f32[384, 384]\", arg26_1: \"f32[384]\", arg27_1: \"f32[384]\", arg28_1: \"f32[384]\", arg29_1: \"f32[1536, 384]\", arg30_1: \"f32[1536]\", arg31_1: \"f32[384, 1536]\", arg32_1: \"f32[384]\", arg33_1: \"f32[384]\", arg34_1: \"f32[384]\", arg35_1: \"f32[384, 384]\", arg36_1: \"f32[384, 384]\", arg37_1: \"f32[384]\", arg38_1: \"f32[384, 384]\", arg39_1: \"f32[384]\", arg40_1: \"f32[384, 384]\", arg41_1: \"f32[384]\", arg42_1: \"f32[384]\", arg43_1: \"f32[384]\", arg44_1: \"f32[1536, 384]\", arg45_1: \"f32[1536]\", arg46_1: \"f32[384, 1536]\", arg47_1: \"f32[384]\", arg48_1: \"f32[384]\", arg49_1: \"f32[384]\", arg50_1: \"f32[384, 384]\", arg51_1: \"f32[384, 384]\", arg52_1: \"f32[384]\", arg53_1: \"f32[384, 384]\", arg54_1: \"f32[384]\", arg55_1: \"f32[384, 384]\", arg56_1: \"f32[384]\", arg57_1: \"f32[384]\", arg58_1: \"f32[384]\", arg59_1: \"f32[1536, 384]\", arg60_1: \"f32[1536]\", arg61_1: \"f32[384, 1536]\", arg62_1: \"f32[384]\", arg63_1: \"f32[384]\", arg64_1: \"f32[384]\", arg65_1: \"f32[384]\", arg66_1: \"f32[384]\", arg67_1: \"f32[128, 3, 3, 3]\", arg68_1: \"f32[128]\", arg69_1: \"f32[128]\", arg70_1: \"f32[128]\", arg71_1: \"f32[128, 128, 3, 3]\", arg72_1: \"f32[128]\", arg73_1: \"f32[128]\", arg74_1: \"f32[128]\", arg75_1: \"f32[128, 128, 3, 3]\", arg76_1: \"f32[128]\", arg77_1: \"f32[128]\", arg78_1: \"f32[128]\", arg79_1: \"f32[128, 128, 3, 3]\", arg80_1: \"f32[128]\", arg81_1: \"f32[128]\", arg82_1: \"f32[128]\", arg83_1: \"f32[128, 128, 3, 3]\", arg84_1: \"f32[128]\", arg85_1: \"f32[128, 128, 3, 3]\", arg86_1: \"f32[128]\", arg87_1: \"f32[128]\", arg88_1: \"f32[128]\", arg89_1: \"f32[256, 128, 3, 3]\", arg90_1: \"f32[256]\", arg91_1: \"f32[256]\", arg92_1: \"f32[256]\", arg93_1: \"f32[256, 256, 3, 3]\", arg94_1: \"f32[256]\", arg95_1: \"f32[256, 128, 1, 1]\", arg96_1: \"f32[256]\", arg97_1: \"f32[256]\", arg98_1: \"f32[256]\", arg99_1: \"f32[256, 256, 3, 3]\", arg100_1: \"f32[256]\", arg101_1: \"f32[256]\", arg102_1: \"f32[256]\", arg103_1: \"f32[256, 256, 3, 3]\", arg104_1: \"f32[256]\", arg105_1: \"f32[256, 256, 3, 3]\", arg106_1: \"f32[256]\", arg107_1: \"f32[256]\", arg108_1: \"f32[256]\", arg109_1: \"f32[512, 256, 3, 3]\", arg110_1: \"f32[512]\", arg111_1: \"f32[512]\", arg112_1: \"f32[512]\", arg113_1: \"f32[512, 512, 3, 3]\", arg114_1: \"f32[512]\", arg115_1: \"f32[512, 256, 1, 1]\", arg116_1: \"f32[512]\", arg117_1: \"f32[512]\", arg118_1: \"f32[512]\", arg119_1: \"f32[512, 512, 3, 3]\", arg120_1: \"f32[512]\", arg121_1: \"f32[512]\", arg122_1: \"f32[512]\", arg123_1: \"f32[512, 512, 3, 3]\", arg124_1: \"f32[512]\", arg125_1: \"f32[512, 512, 3, 3]\", arg126_1: \"f32[512]\", arg127_1: \"f32[512]\", arg128_1: \"f32[512]\", arg129_1: \"f32[512, 512, 3, 3]\", arg130_1: \"f32[512]\", arg131_1: \"f32[512]\", arg132_1: \"f32[512]\", arg133_1: \"f32[512, 512, 3, 3]\", arg134_1: \"f32[512]\", arg135_1: \"f32[512]\", arg136_1: \"f32[512]\", arg137_1: \"f32[512, 512, 3, 3]\", arg138_1: \"f32[512]\", arg139_1: \"f32[512]\", arg140_1: \"f32[512]\", arg141_1: \"f32[512, 512, 3, 3]\", arg142_1: \"f32[512]\", arg143_1: \"f32[512]\", arg144_1: \"f32[512]\", arg145_1: \"f32[512, 512]\", arg146_1: \"f32[512]\", arg147_1: \"f32[512, 512]\", arg148_1: \"f32[512]\", arg149_1: \"f32[512, 512]\", arg150_1: \"f32[512]\", arg151_1: \"f32[512, 512]\", arg152_1: \"f32[512]\", arg153_1: \"f32[512]\", arg154_1: \"f32[512]\", arg155_1: \"f32[512, 512, 3, 3]\", arg156_1: \"f32[512]\", arg157_1: \"f32[512]\", arg158_1: \"f32[512]\", arg159_1: \"f32[512, 512, 3, 3]\", arg160_1: \"f32[512]\", arg161_1: \"f32[512]\", arg162_1: \"f32[512]\", arg163_1: \"f32[512, 512, 3, 3]\", arg164_1: \"f32[512]\", arg165_1: \"f32[512]\", arg166_1: \"f32[512]\", arg167_1: \"f32[512, 512, 3, 3]\", arg168_1: \"f32[512]\", arg169_1: \"f32[512]\", arg170_1: \"f32[512]\", arg171_1: \"f32[8, 512, 3, 3]\", arg172_1: \"f32[8]\", arg173_1: \"f32[512, 4, 3, 3]\", arg174_1: \"f32[512]\", arg175_1: \"f32[512]\", arg176_1: \"f32[512]\", arg177_1: \"f32[512, 512, 3, 3]\", arg178_1: \"f32[512]\", arg179_1: \"f32[512]\", arg180_1: \"f32[512]\", arg181_1: \"f32[512, 512, 3, 3]\", arg182_1: \"f32[512]\", arg183_1: \"f32[512]\", arg184_1: \"f32[512]\", arg185_1: \"f32[512, 512, 3, 3]\", arg186_1: \"f32[512]\", arg187_1: \"f32[512]\", arg188_1: \"f32[512]\", arg189_1: \"f32[512, 512, 3, 3]\", arg190_1: \"f32[512]\", arg191_1: \"f32[512]\", arg192_1: \"f32[512]\", arg193_1: \"f32[512, 512, 3, 3]\", arg194_1: \"f32[512]\", arg195_1: \"f32[512]\", arg196_1: \"f32[512]\", arg197_1: \"f32[512, 512, 3, 3]\", arg198_1: \"f32[512]\", arg199_1: \"f32[512, 512, 3, 3]\", arg200_1: \"f32[512]\", arg201_1: \"f32[512]\", arg202_1: \"f32[512]\", arg203_1: \"f32[512, 512, 3, 3]\", arg204_1: \"f32[512]\", arg205_1: \"f32[512]\", arg206_1: \"f32[512]\", arg207_1: \"f32[512, 512, 3, 3]\", arg208_1: \"f32[512]\", arg209_1: \"f32[512]\", arg210_1: \"f32[512]\", arg211_1: \"f32[512, 512, 3, 3]\", arg212_1: \"f32[512]\", arg213_1: \"f32[512]\", arg214_1: \"f32[512]\", arg215_1: \"f32[512, 512, 3, 3]\", arg216_1: \"f32[512]\", arg217_1: \"f32[512]\", arg218_1: \"f32[512]\", arg219_1: \"f32[512, 512, 3, 3]\", arg220_1: \"f32[512]\", arg221_1: \"f32[512]\", arg222_1: \"f32[512]\", arg223_1: \"f32[512, 512, 3, 3]\", arg224_1: \"f32[512]\", arg225_1: \"f32[512, 512, 3, 3]\", arg226_1: \"f32[512]\", arg227_1: \"f32[512]\", arg228_1: \"f32[512]\", arg229_1: \"f32[256, 512, 3, 3]\", arg230_1: \"f32[256]\", arg231_1: \"f32[256]\", arg232_1: \"f32[256]\", arg233_1: \"f32[256, 256, 3, 3]\", arg234_1: \"f32[256]\", arg235_1: \"f32[256, 512, 1, 1]\", arg236_1: \"f32[256]\", arg237_1: \"f32[256]\", arg238_1: \"f32[256]\", arg239_1: \"f32[256, 256, 3, 3]\", arg240_1: \"f32[256]\", arg241_1: \"f32[256]\", arg242_1: \"f32[256]\", arg243_1: \"f32[256, 256, 3, 3]\", arg244_1: \"f32[256]\", arg245_1: \"f32[256]\", arg246_1: \"f32[256]\", arg247_1: \"f32[256, 256, 3, 3]\", arg248_1: \"f32[256]\", arg249_1: \"f32[256]\", arg250_1: \"f32[256]\", arg251_1: \"f32[256, 256, 3, 3]\", arg252_1: \"f32[256]\", arg253_1: \"f32[256, 256, 3, 3]\", arg254_1: \"f32[256]\", arg255_1: \"f32[256]\", arg256_1: \"f32[256]\", arg257_1: \"f32[128, 256, 3, 3]\", arg258_1: \"f32[128]\", arg259_1: \"f32[128]\", arg260_1: \"f32[128]\", arg261_1: \"f32[128, 128, 3, 3]\", arg262_1: \"f32[128]\", arg263_1: \"f32[128, 256, 1, 1]\", arg264_1: \"f32[128]\", arg265_1: \"f32[128]\", arg266_1: \"f32[128]\", arg267_1: \"f32[128, 128, 3, 3]\", arg268_1: \"f32[128]\", arg269_1: \"f32[128]\", arg270_1: \"f32[128]\", arg271_1: \"f32[128, 128, 3, 3]\", arg272_1: \"f32[128]\", arg273_1: \"f32[128]\", arg274_1: \"f32[128]\", arg275_1: \"f32[128, 128, 3, 3]\", arg276_1: \"f32[128]\", arg277_1: \"f32[128]\", arg278_1: \"f32[128]\", arg279_1: \"f32[128, 128, 3, 3]\", arg280_1: \"f32[128]\", arg281_1: \"f32[512]\", arg282_1: \"f32[512]\", arg283_1: \"f32[512, 512]\", arg284_1: \"f32[512]\", arg285_1: \"f32[512, 512]\", arg286_1: \"f32[512]\", arg287_1: \"f32[512, 512]\", arg288_1: \"f32[512]\", arg289_1: \"f32[512, 512]\", arg290_1: \"f32[512]\", arg291_1: \"f32[512]\", arg292_1: \"f32[512]\", arg293_1: \"f32[512, 512, 3, 3]\", arg294_1: \"f32[512]\", arg295_1: \"f32[512]\", arg296_1: \"f32[512]\", arg297_1: \"f32[512, 512, 3, 3]\", arg298_1: \"f32[512]\", arg299_1: \"f32[512]\", arg300_1: \"f32[512]\", arg301_1: \"f32[512, 512, 3, 3]\", arg302_1: \"f32[512]\", arg303_1: \"f32[512]\", arg304_1: \"f32[512]\", arg305_1: \"f32[512, 512, 3, 3]\", arg306_1: \"f32[512]\", arg307_1: \"f32[128]\", arg308_1: \"f32[128]\", arg309_1: \"f32[3, 128, 3, 3]\", arg310_1: \"f32[3]\", arg311_1: \"f32[8, 8, 1, 1]\", arg312_1: \"f32[8]\", arg313_1: \"f32[4, 4, 1, 1]\", arg314_1: \"f32[4]\", arg315_1: \"f32[320, 8, 3, 3]\", arg316_1: \"f32[320]\", arg317_1: \"f32[1280, 320]\", arg318_1: \"f32[1280]\", arg319_1: \"f32[1280, 1280]\", arg320_1: \"f32[1280]\", arg321_1: \"f32[320]\", arg322_1: \"f32[320]\", arg323_1: \"f32[320, 320, 1, 1]\", arg324_1: \"f32[320]\", arg325_1: \"f32[320]\", arg326_1: \"f32[320]\", arg327_1: \"f32[320, 320]\", arg328_1: \"f32[320, 320]\", arg329_1: \"f32[320, 320]\", arg330_1: \"f32[320, 320]\", arg331_1: \"f32[320]\", arg332_1: \"f32[320]\", arg333_1: \"f32[320]\", arg334_1: \"f32[320, 320]\", arg335_1: \"f32[320, 384]\", arg336_1: \"f32[320, 384]\", arg337_1: \"f32[320, 320]\", arg338_1: \"f32[320]\", arg339_1: \"f32[320]\", arg340_1: \"f32[320]\", arg341_1: \"f32[2560, 320]\", arg342_1: \"f32[2560]\", arg343_1: \"f32[320, 1280]\", arg344_1: \"f32[320]\", arg345_1: \"f32[320, 320, 1, 1]\", arg346_1: \"f32[320]\", arg347_1: \"f32[320]\", arg348_1: \"f32[320]\", arg349_1: \"f32[320, 320, 1, 1]\", arg350_1: \"f32[320]\", arg351_1: \"f32[320]\", arg352_1: \"f32[320]\", arg353_1: \"f32[320, 320]\", arg354_1: \"f32[320, 320]\", arg355_1: \"f32[320, 320]\", arg356_1: \"f32[320, 320]\", arg357_1: \"f32[320]\", arg358_1: \"f32[320]\", arg359_1: \"f32[320]\", arg360_1: \"f32[320, 320]\", arg361_1: \"f32[320, 384]\", arg362_1: \"f32[320, 384]\", arg363_1: \"f32[320, 320]\", arg364_1: \"f32[320]\", arg365_1: \"f32[320]\", arg366_1: \"f32[320]\", arg367_1: \"f32[2560, 320]\", arg368_1: \"f32[2560]\", arg369_1: \"f32[320, 1280]\", arg370_1: \"f32[320]\", arg371_1: \"f32[320, 320, 1, 1]\", arg372_1: \"f32[320]\", arg373_1: \"f32[320]\", arg374_1: \"f32[320]\", arg375_1: \"f32[320, 320, 3, 3]\", arg376_1: \"f32[320]\", arg377_1: \"f32[320, 1280]\", arg378_1: \"f32[320]\", arg379_1: \"f32[320]\", arg380_1: \"f32[320]\", arg381_1: \"f32[320, 320, 3, 3]\", arg382_1: \"f32[320]\", arg383_1: \"f32[320]\", arg384_1: \"f32[320]\", arg385_1: \"f32[320, 320, 3, 3]\", arg386_1: \"f32[320]\", arg387_1: \"f32[320, 1280]\", arg388_1: \"f32[320]\", arg389_1: \"f32[320]\", arg390_1: \"f32[320]\", arg391_1: \"f32[320, 320, 3, 3]\", arg392_1: \"f32[320]\", arg393_1: \"f32[320, 320, 3, 3]\", arg394_1: \"f32[320]\", arg395_1: \"f32[640]\", arg396_1: \"f32[640]\", arg397_1: \"f32[640, 640, 1, 1]\", arg398_1: \"f32[640]\", arg399_1: \"f32[640]\", arg400_1: \"f32[640]\", arg401_1: \"f32[640, 640]\", arg402_1: \"f32[640, 640]\", arg403_1: \"f32[640, 640]\", arg404_1: \"f32[640, 640]\", arg405_1: \"f32[640]\", arg406_1: \"f32[640]\", arg407_1: \"f32[640]\", arg408_1: \"f32[640, 640]\", arg409_1: \"f32[640, 384]\", arg410_1: \"f32[640, 384]\", arg411_1: \"f32[640, 640]\", arg412_1: \"f32[640]\", arg413_1: \"f32[640]\", arg414_1: \"f32[640]\", arg415_1: \"f32[5120, 640]\", arg416_1: \"f32[5120]\", arg417_1: \"f32[640, 2560]\", arg418_1: \"f32[640]\", arg419_1: \"f32[640, 640, 1, 1]\", arg420_1: \"f32[640]\", arg421_1: \"f32[640]\", arg422_1: \"f32[640]\", arg423_1: \"f32[640, 640, 1, 1]\", arg424_1: \"f32[640]\", arg425_1: \"f32[640]\", arg426_1: \"f32[640]\", arg427_1: \"f32[640, 640]\", arg428_1: \"f32[640, 640]\", arg429_1: \"f32[640, 640]\", arg430_1: \"f32[640, 640]\", arg431_1: \"f32[640]\", arg432_1: \"f32[640]\", arg433_1: \"f32[640]\", arg434_1: \"f32[640, 640]\", arg435_1: \"f32[640, 384]\", arg436_1: \"f32[640, 384]\", arg437_1: \"f32[640, 640]\", arg438_1: \"f32[640]\", arg439_1: \"f32[640]\", arg440_1: \"f32[640]\", arg441_1: \"f32[5120, 640]\", arg442_1: \"f32[5120]\", arg443_1: \"f32[640, 2560]\", arg444_1: \"f32[640]\", arg445_1: \"f32[640, 640, 1, 1]\", arg446_1: \"f32[640]\", arg447_1: \"f32[320]\", arg448_1: \"f32[320]\", arg449_1: \"f32[640, 320, 3, 3]\", arg450_1: \"f32[640]\", arg451_1: \"f32[640, 1280]\", arg452_1: \"f32[640]\", arg453_1: \"f32[640]\", arg454_1: \"f32[640]\", arg455_1: \"f32[640, 640, 3, 3]\", arg456_1: \"f32[640]\", arg457_1: \"f32[640, 320, 1, 1]\", arg458_1: \"f32[640]\", arg459_1: \"f32[640]\", arg460_1: \"f32[640]\", arg461_1: \"f32[640, 640, 3, 3]\", arg462_1: \"f32[640]\", arg463_1: \"f32[640, 1280]\", arg464_1: \"f32[640]\", arg465_1: \"f32[640]\", arg466_1: \"f32[640]\", arg467_1: \"f32[640, 640, 3, 3]\", arg468_1: \"f32[640]\", arg469_1: \"f32[640, 640, 3, 3]\", arg470_1: \"f32[640]\", arg471_1: \"f32[1280]\", arg472_1: \"f32[1280]\", arg473_1: \"f32[1280, 1280, 1, 1]\", arg474_1: \"f32[1280]\", arg475_1: \"f32[1280]\", arg476_1: \"f32[1280]\", arg477_1: \"f32[1280, 1280]\", arg478_1: \"f32[1280, 1280]\", arg479_1: \"f32[1280, 1280]\", arg480_1: \"f32[1280, 1280]\", arg481_1: \"f32[1280]\", arg482_1: \"f32[1280]\", arg483_1: \"f32[1280]\", arg484_1: \"f32[1280, 1280]\", arg485_1: \"f32[1280, 384]\", arg486_1: \"f32[1280, 384]\", arg487_1: \"f32[1280, 1280]\", arg488_1: \"f32[1280]\", arg489_1: \"f32[1280]\", arg490_1: \"f32[1280]\", arg491_1: \"f32[10240, 1280]\", arg492_1: \"f32[10240]\", arg493_1: \"f32[1280, 5120]\", arg494_1: \"f32[1280]\", arg495_1: \"f32[1280, 1280, 1, 1]\", arg496_1: \"f32[1280]\", arg497_1: \"f32[1280]\", arg498_1: \"f32[1280]\", arg499_1: \"f32[1280, 1280, 1, 1]\", arg500_1: \"f32[1280]\", arg501_1: \"f32[1280]\", arg502_1: \"f32[1280]\", arg503_1: \"f32[1280, 1280]\", arg504_1: \"f32[1280, 1280]\", arg505_1: \"f32[1280, 1280]\", arg506_1: \"f32[1280, 1280]\", arg507_1: \"f32[1280]\", arg508_1: \"f32[1280]\", arg509_1: \"f32[1280]\", arg510_1: \"f32[1280, 1280]\", arg511_1: \"f32[1280, 384]\", arg512_1: \"f32[1280, 384]\", arg513_1: \"f32[1280, 1280]\", arg514_1: \"f32[1280]\", arg515_1: \"f32[1280]\", arg516_1: \"f32[1280]\", arg517_1: \"f32[10240, 1280]\", arg518_1: \"f32[10240]\", arg519_1: \"f32[1280, 5120]\", arg520_1: \"f32[1280]\", arg521_1: \"f32[1280, 1280, 1, 1]\", arg522_1: \"f32[1280]\", arg523_1: \"f32[640]\", arg524_1: \"f32[640]\", arg525_1: \"f32[1280, 640, 3, 3]\", arg526_1: \"f32[1280]\", arg527_1: \"f32[1280, 1280]\", arg528_1: \"f32[1280]\", arg529_1: \"f32[1280]\", arg530_1: \"f32[1280]\", arg531_1: \"f32[1280, 1280, 3, 3]\", arg532_1: \"f32[1280]\", arg533_1: \"f32[1280, 640, 1, 1]\", arg534_1: \"f32[1280]\", arg535_1: \"f32[1280]\", arg536_1: \"f32[1280]\", arg537_1: \"f32[1280, 1280, 3, 3]\", arg538_1: \"f32[1280]\", arg539_1: \"f32[1280, 1280]\", arg540_1: \"f32[1280]\", arg541_1: \"f32[1280]\", arg542_1: \"f32[1280]\", arg543_1: \"f32[1280, 1280, 3, 3]\", arg544_1: \"f32[1280]\", arg545_1: \"f32[1280, 1280, 3, 3]\", arg546_1: \"f32[1280]\", arg547_1: \"f32[1280]\", arg548_1: \"f32[1280]\", arg549_1: \"f32[1280, 1280, 3, 3]\", arg550_1: \"f32[1280]\", arg551_1: \"f32[1280, 1280]\", arg552_1: \"f32[1280]\", arg553_1: \"f32[1280]\", arg554_1: \"f32[1280]\", arg555_1: \"f32[1280, 1280, 3, 3]\", arg556_1: \"f32[1280]\", arg557_1: \"f32[1280]\", arg558_1: \"f32[1280]\", arg559_1: \"f32[1280, 1280, 3, 3]\", arg560_1: \"f32[1280]\", arg561_1: \"f32[1280, 1280]\", arg562_1: \"f32[1280]\", arg563_1: \"f32[1280]\", arg564_1: \"f32[1280]\", arg565_1: \"f32[1280, 1280, 3, 3]\", arg566_1: \"f32[1280]\", arg567_1: \"f32[2560]\", arg568_1: \"f32[2560]\", arg569_1: \"f32[1280, 2560, 3, 3]\", arg570_1: \"f32[1280]\", arg571_1: \"f32[1280, 1280]\", arg572_1: \"f32[1280]\", arg573_1: \"f32[1280]\", arg574_1: \"f32[1280]\", arg575_1: \"f32[1280, 1280, 3, 3]\", arg576_1: \"f32[1280]\", arg577_1: \"f32[1280, 2560, 1, 1]\", arg578_1: \"f32[1280]\", arg579_1: \"f32[2560]\", arg580_1: \"f32[2560]\", arg581_1: \"f32[1280, 2560, 3, 3]\", arg582_1: \"f32[1280]\", arg583_1: \"f32[1280, 1280]\", arg584_1: \"f32[1280]\", arg585_1: \"f32[1280]\", arg586_1: \"f32[1280]\", arg587_1: \"f32[1280, 1280, 3, 3]\", arg588_1: \"f32[1280]\", arg589_1: \"f32[1280, 2560, 1, 1]\", arg590_1: \"f32[1280]\", arg591_1: \"f32[2560]\", arg592_1: \"f32[2560]\", arg593_1: \"f32[1280, 2560, 3, 3]\", arg594_1: \"f32[1280]\", arg595_1: \"f32[1280, 1280]\", arg596_1: \"f32[1280]\", arg597_1: \"f32[1280]\", arg598_1: \"f32[1280]\", arg599_1: \"f32[1280, 1280, 3, 3]\", arg600_1: \"f32[1280]\", arg601_1: \"f32[1280, 2560, 1, 1]\", arg602_1: \"f32[1280]\", arg603_1: \"f32[1280, 1280, 3, 3]\", arg604_1: \"f32[1280]\", arg605_1: \"f32[1280]\", arg606_1: \"f32[1280]\", arg607_1: \"f32[1280, 1280, 1, 1]\", arg608_1: \"f32[1280]\", arg609_1: \"f32[1280]\", arg610_1: \"f32[1280]\", arg611_1: \"f32[1280, 1280]\", arg612_1: \"f32[1280, 1280]\", arg613_1: \"f32[1280, 1280]\", arg614_1: \"f32[1280, 1280]\", arg615_1: \"f32[1280]\", arg616_1: \"f32[1280]\", arg617_1: \"f32[1280]\", arg618_1: \"f32[1280, 1280]\", arg619_1: \"f32[1280, 384]\", arg620_1: \"f32[1280, 384]\", arg621_1: \"f32[1280, 1280]\", arg622_1: \"f32[1280]\", arg623_1: \"f32[1280]\", arg624_1: \"f32[1280]\", arg625_1: \"f32[10240, 1280]\", arg626_1: \"f32[10240]\", arg627_1: \"f32[1280, 5120]\", arg628_1: \"f32[1280]\", arg629_1: \"f32[1280, 1280, 1, 1]\", arg630_1: \"f32[1280]\", arg631_1: \"f32[1280]\", arg632_1: \"f32[1280]\", arg633_1: \"f32[1280, 1280, 1, 1]\", arg634_1: \"f32[1280]\", arg635_1: \"f32[1280]\", arg636_1: \"f32[1280]\", arg637_1: \"f32[1280, 1280]\", arg638_1: \"f32[1280, 1280]\", arg639_1: \"f32[1280, 1280]\", arg640_1: \"f32[1280, 1280]\", arg641_1: \"f32[1280]\", arg642_1: \"f32[1280]\", arg643_1: \"f32[1280]\", arg644_1: \"f32[1280, 1280]\", arg645_1: \"f32[1280, 384]\", arg646_1: \"f32[1280, 384]\", arg647_1: \"f32[1280, 1280]\", arg648_1: \"f32[1280]\", arg649_1: \"f32[1280]\", arg650_1: \"f32[1280]\", arg651_1: \"f32[10240, 1280]\", arg652_1: \"f32[10240]\", arg653_1: \"f32[1280, 5120]\", arg654_1: \"f32[1280]\", arg655_1: \"f32[1280, 1280, 1, 1]\", arg656_1: \"f32[1280]\", arg657_1: \"f32[1280]\", arg658_1: \"f32[1280]\", arg659_1: \"f32[1280, 1280, 1, 1]\", arg660_1: \"f32[1280]\", arg661_1: \"f32[1280]\", arg662_1: \"f32[1280]\", arg663_1: \"f32[1280, 1280]\", arg664_1: \"f32[1280, 1280]\", arg665_1: \"f32[1280, 1280]\", arg666_1: \"f32[1280, 1280]\", arg667_1: \"f32[1280]\", arg668_1: \"f32[1280]\", arg669_1: \"f32[1280]\", arg670_1: \"f32[1280, 1280]\", arg671_1: \"f32[1280, 384]\", arg672_1: \"f32[1280, 384]\", arg673_1: \"f32[1280, 1280]\", arg674_1: \"f32[1280]\", arg675_1: \"f32[1280]\", arg676_1: \"f32[1280]\", arg677_1: \"f32[10240, 1280]\", arg678_1: \"f32[10240]\", arg679_1: \"f32[1280, 5120]\", arg680_1: \"f32[1280]\", arg681_1: \"f32[1280, 1280, 1, 1]\", arg682_1: \"f32[1280]\", arg683_1: \"f32[2560]\", arg684_1: \"f32[2560]\", arg685_1: \"f32[1280, 2560, 3, 3]\", arg686_1: \"f32[1280]\", arg687_1: \"f32[1280, 1280]\", arg688_1: \"f32[1280]\", arg689_1: \"f32[1280]\", arg690_1: \"f32[1280]\", arg691_1: \"f32[1280, 1280, 3, 3]\", arg692_1: \"f32[1280]\", arg693_1: \"f32[1280, 2560, 1, 1]\", arg694_1: \"f32[1280]\", arg695_1: \"f32[2560]\", arg696_1: \"f32[2560]\", arg697_1: \"f32[1280, 2560, 3, 3]\", arg698_1: \"f32[1280]\", arg699_1: \"f32[1280, 1280]\", arg700_1: \"f32[1280]\", arg701_1: \"f32[1280]\", arg702_1: \"f32[1280]\", arg703_1: \"f32[1280, 1280, 3, 3]\", arg704_1: \"f32[1280]\", arg705_1: \"f32[1280, 2560, 1, 1]\", arg706_1: \"f32[1280]\", arg707_1: \"f32[1920]\", arg708_1: \"f32[1920]\", arg709_1: \"f32[1280, 1920, 3, 3]\", arg710_1: \"f32[1280]\", arg711_1: \"f32[1280, 1280]\", arg712_1: \"f32[1280]\", arg713_1: \"f32[1280]\", arg714_1: \"f32[1280]\", arg715_1: \"f32[1280, 1280, 3, 3]\", arg716_1: \"f32[1280]\", arg717_1: \"f32[1280, 1920, 1, 1]\", arg718_1: \"f32[1280]\", arg719_1: \"f32[1280, 1280, 3, 3]\", arg720_1: \"f32[1280]\", arg721_1: \"f32[640]\", arg722_1: \"f32[640]\", arg723_1: \"f32[640, 640, 1, 1]\", arg724_1: \"f32[640]\", arg725_1: \"f32[640]\", arg726_1: \"f32[640]\", arg727_1: \"f32[640, 640]\", arg728_1: \"f32[640, 640]\", arg729_1: \"f32[640, 640]\", arg730_1: \"f32[640, 640]\", arg731_1: \"f32[640]\", arg732_1: \"f32[640]\", arg733_1: \"f32[640]\", arg734_1: \"f32[640, 640]\", arg735_1: \"f32[640, 384]\", arg736_1: \"f32[640, 384]\", arg737_1: \"f32[640, 640]\", arg738_1: \"f32[640]\", arg739_1: \"f32[640]\", arg740_1: \"f32[640]\", arg741_1: \"f32[5120, 640]\", arg742_1: \"f32[5120]\", arg743_1: \"f32[640, 2560]\", arg744_1: \"f32[640]\", arg745_1: \"f32[640, 640, 1, 1]\", arg746_1: \"f32[640]\", arg747_1: \"f32[640]\", arg748_1: \"f32[640]\", arg749_1: \"f32[640, 640, 1, 1]\", arg750_1: \"f32[640]\", arg751_1: \"f32[640]\", arg752_1: \"f32[640]\", arg753_1: \"f32[640, 640]\", arg754_1: \"f32[640, 640]\", arg755_1: \"f32[640, 640]\", arg756_1: \"f32[640, 640]\", arg757_1: \"f32[640]\", arg758_1: \"f32[640]\", arg759_1: \"f32[640]\", arg760_1: \"f32[640, 640]\", arg761_1: \"f32[640, 384]\", arg762_1: \"f32[640, 384]\", arg763_1: \"f32[640, 640]\", arg764_1: \"f32[640]\", arg765_1: \"f32[640]\", arg766_1: \"f32[640]\", arg767_1: \"f32[5120, 640]\", arg768_1: \"f32[5120]\", arg769_1: \"f32[640, 2560]\", arg770_1: \"f32[640]\", arg771_1: \"f32[640, 640, 1, 1]\", arg772_1: \"f32[640]\", arg773_1: \"f32[640]\", arg774_1: \"f32[640]\", arg775_1: \"f32[640, 640, 1, 1]\", arg776_1: \"f32[640]\", arg777_1: \"f32[640]\", arg778_1: \"f32[640]\", arg779_1: \"f32[640, 640]\", arg780_1: \"f32[640, 640]\", arg781_1: \"f32[640, 640]\", arg782_1: \"f32[640, 640]\", arg783_1: \"f32[640]\", arg784_1: \"f32[640]\", arg785_1: \"f32[640]\", arg786_1: \"f32[640, 640]\", arg787_1: \"f32[640, 384]\", arg788_1: \"f32[640, 384]\", arg789_1: \"f32[640, 640]\", arg790_1: \"f32[640]\", arg791_1: \"f32[640]\", arg792_1: \"f32[640]\", arg793_1: \"f32[5120, 640]\", arg794_1: \"f32[5120]\", arg795_1: \"f32[640, 2560]\", arg796_1: \"f32[640]\", arg797_1: \"f32[640, 640, 1, 1]\", arg798_1: \"f32[640]\", arg799_1: \"f32[1920]\", arg800_1: \"f32[1920]\", arg801_1: \"f32[640, 1920, 3, 3]\", arg802_1: \"f32[640]\", arg803_1: \"f32[640, 1280]\", arg804_1: \"f32[640]\", arg805_1: \"f32[640]\", arg806_1: \"f32[640]\", arg807_1: \"f32[640, 640, 3, 3]\", arg808_1: \"f32[640]\", arg809_1: \"f32[640, 1920, 1, 1]\", arg810_1: \"f32[640]\", arg811_1: \"f32[1280]\", arg812_1: \"f32[1280]\", arg813_1: \"f32[640, 1280, 3, 3]\", arg814_1: \"f32[640]\", arg815_1: \"f32[640, 1280]\", arg816_1: \"f32[640]\", arg817_1: \"f32[640]\", arg818_1: \"f32[640]\", arg819_1: \"f32[640, 640, 3, 3]\", arg820_1: \"f32[640]\", arg821_1: \"f32[640, 1280, 1, 1]\", arg822_1: \"f32[640]\", arg823_1: \"f32[960]\", arg824_1: \"f32[960]\", arg825_1: \"f32[640, 960, 3, 3]\", arg826_1: \"f32[640]\", arg827_1: \"f32[640, 1280]\", arg828_1: \"f32[640]\", arg829_1: \"f32[640]\", arg830_1: \"f32[640]\", arg831_1: \"f32[640, 640, 3, 3]\", arg832_1: \"f32[640]\", arg833_1: \"f32[640, 960, 1, 1]\", arg834_1: \"f32[640]\", arg835_1: \"f32[640, 640, 3, 3]\", arg836_1: \"f32[640]\", arg837_1: \"f32[320]\", arg838_1: \"f32[320]\", arg839_1: \"f32[320, 320, 1, 1]\", arg840_1: \"f32[320]\", arg841_1: \"f32[320]\", arg842_1: \"f32[320]\", arg843_1: \"f32[320, 320]\", arg844_1: \"f32[320, 320]\", arg845_1: \"f32[320, 320]\", arg846_1: \"f32[320, 320]\", arg847_1: \"f32[320]\", arg848_1: \"f32[320]\", arg849_1: \"f32[320]\", arg850_1: \"f32[320, 320]\", arg851_1: \"f32[320, 384]\", arg852_1: \"f32[320, 384]\", arg853_1: \"f32[320, 320]\", arg854_1: \"f32[320]\", arg855_1: \"f32[320]\", arg856_1: \"f32[320]\", arg857_1: \"f32[2560, 320]\", arg858_1: \"f32[2560]\", arg859_1: \"f32[320, 1280]\", arg860_1: \"f32[320]\", arg861_1: \"f32[320, 320, 1, 1]\", arg862_1: \"f32[320]\", arg863_1: \"f32[320]\", arg864_1: \"f32[320]\", arg865_1: \"f32[320, 320, 1, 1]\", arg866_1: \"f32[320]\", arg867_1: \"f32[320]\", arg868_1: \"f32[320]\", arg869_1: \"f32[320, 320]\", arg870_1: \"f32[320, 320]\", arg871_1: \"f32[320, 320]\", arg872_1: \"f32[320, 320]\", arg873_1: \"f32[320]\", arg874_1: \"f32[320]\", arg875_1: \"f32[320]\", arg876_1: \"f32[320, 320]\", arg877_1: \"f32[320, 384]\", arg878_1: \"f32[320, 384]\", arg879_1: \"f32[320, 320]\", arg880_1: \"f32[320]\", arg881_1: \"f32[320]\", arg882_1: \"f32[320]\", arg883_1: \"f32[2560, 320]\", arg884_1: \"f32[2560]\", arg885_1: \"f32[320, 1280]\", arg886_1: \"f32[320]\", arg887_1: \"f32[320, 320, 1, 1]\", arg888_1: \"f32[320]\", arg889_1: \"f32[320]\", arg890_1: \"f32[320]\", arg891_1: \"f32[320, 320, 1, 1]\", arg892_1: \"f32[320]\", arg893_1: \"f32[320]\", arg894_1: \"f32[320]\", arg895_1: \"f32[320, 320]\", arg896_1: \"f32[320, 320]\", arg897_1: \"f32[320, 320]\", arg898_1: \"f32[320, 320]\", arg899_1: \"f32[320]\", arg900_1: \"f32[320]\", arg901_1: \"f32[320]\", arg902_1: \"f32[320, 320]\", arg903_1: \"f32[320, 384]\", arg904_1: \"f32[320, 384]\", arg905_1: \"f32[320, 320]\", arg906_1: \"f32[320]\", arg907_1: \"f32[320]\", arg908_1: \"f32[320]\", arg909_1: \"f32[2560, 320]\", arg910_1: \"f32[2560]\", arg911_1: \"f32[320, 1280]\", arg912_1: \"f32[320]\", arg913_1: \"f32[320, 320, 1, 1]\", arg914_1: \"f32[320]\", arg915_1: \"f32[960]\", arg916_1: \"f32[960]\", arg917_1: \"f32[320, 960, 3, 3]\", arg918_1: \"f32[320]\", arg919_1: \"f32[320, 1280]\", arg920_1: \"f32[320]\", arg921_1: \"f32[320]\", arg922_1: \"f32[320]\", arg923_1: \"f32[320, 320, 3, 3]\", arg924_1: \"f32[320]\", arg925_1: \"f32[320, 960, 1, 1]\", arg926_1: \"f32[320]\", arg927_1: \"f32[640]\", arg928_1: \"f32[640]\", arg929_1: \"f32[320, 640, 3, 3]\", arg930_1: \"f32[320]\", arg931_1: \"f32[320, 1280]\", arg932_1: \"f32[320]\", arg933_1: \"f32[320]\", arg934_1: \"f32[320]\", arg935_1: \"f32[320, 320, 3, 3]\", arg936_1: \"f32[320]\", arg937_1: \"f32[320, 640, 1, 1]\", arg938_1: \"f32[320]\", arg939_1: \"f32[640]\", arg940_1: \"f32[640]\", arg941_1: \"f32[320, 640, 3, 3]\", arg942_1: \"f32[320]\", arg943_1: \"f32[320, 1280]\", arg944_1: \"f32[320]\", arg945_1: \"f32[320]\", arg946_1: \"f32[320]\", arg947_1: \"f32[320, 320, 3, 3]\", arg948_1: \"f32[320]\", arg949_1: \"f32[320, 640, 1, 1]\", arg950_1: \"f32[320]\", arg951_1: \"f32[1280]\", arg952_1: \"f32[1280]\", arg953_1: \"f32[1280, 1280, 1, 1]\", arg954_1: \"f32[1280]\", arg955_1: \"f32[1280]\", arg956_1: \"f32[1280]\", arg957_1: \"f32[1280, 1280]\", arg958_1: \"f32[1280, 1280]\", arg959_1: \"f32[1280, 1280]\", arg960_1: \"f32[1280, 1280]\", arg961_1: \"f32[1280]\", arg962_1: \"f32[1280]\", arg963_1: \"f32[1280]\", arg964_1: \"f32[1280, 1280]\", arg965_1: \"f32[1280, 384]\", arg966_1: \"f32[1280, 384]\", arg967_1: \"f32[1280, 1280]\", arg968_1: \"f32[1280]\", arg969_1: \"f32[1280]\", arg970_1: \"f32[1280]\", arg971_1: \"f32[10240, 1280]\", arg972_1: \"f32[10240]\", arg973_1: \"f32[1280, 5120]\", arg974_1: \"f32[1280]\", arg975_1: \"f32[1280, 1280, 1, 1]\", arg976_1: \"f32[1280]\", arg977_1: \"f32[1280]\", arg978_1: \"f32[1280]\", arg979_1: \"f32[1280, 1280, 3, 3]\", arg980_1: \"f32[1280]\", arg981_1: \"f32[1280, 1280]\", arg982_1: \"f32[1280]\", arg983_1: \"f32[1280]\", arg984_1: \"f32[1280]\", arg985_1: \"f32[1280, 1280, 3, 3]\", arg986_1: \"f32[1280]\", arg987_1: \"f32[1280]\", arg988_1: \"f32[1280]\", arg989_1: \"f32[1280, 1280, 3, 3]\", arg990_1: \"f32[1280]\", arg991_1: \"f32[1280, 1280]\", arg992_1: \"f32[1280]\", arg993_1: \"f32[1280]\", arg994_1: \"f32[1280]\", arg995_1: \"f32[1280, 1280, 3, 3]\", arg996_1: \"f32[1280]\", arg997_1: \"f32[320]\", arg998_1: \"f32[320]\", arg999_1: \"f32[4, 320, 3, 3]\", arg1000_1: \"f32[4]\", arg1001_1: \"f32[1, 5000, 384]\", arg1002_1: \"f32[1, 80, 3000]\", arg1003_1: \"f32[8, 8, 32, 32]\", arg1004_1: \"i64[]\", arg1005_1: \"i64[]\"):\n",
      "     # File: /tmp/ipykernel_67451/113760652.py:110 in forward, code: frame_idx = frame_idx.item()\n",
      "    item: \"Sym(u0)\" = torch.ops.aten.item.default(arg1004_1);  arg1004_1 = item = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:111 in forward, code: librosa_length = librosa_length.item()\n",
      "    item_1: \"Sym(u1)\" = torch.ops.aten.item.default(arg1005_1);  arg1005_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:112 in forward, code: whisper_chunks = self.get_whisper_chunk(\n",
      "    to: \"f32[1, 80, 3000]\" = torch.ops.aten.to.dtype_layout(arg1002_1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  arg1002_1 = None\n",
      "    to_1: \"f32[1, 80, 3000]\" = torch.ops.aten.to.dtype(to, torch.float32);  to = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv1d: \"f32[1, 384, 3000]\" = torch.ops.aten.conv1d.default(to_1, arg0_1, arg1_1, [1], [1]);  to_1 = arg0_1 = arg1_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1175 in forward, code: inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
      "    gelu: \"f32[1, 384, 3000]\" = torch.ops.aten.gelu.default(conv1d);  conv1d = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv1d_1: \"f32[1, 384, 1500]\" = torch.ops.aten.conv1d.default(gelu, arg2_1, arg3_1, [2], [1]);  gelu = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1176 in forward, code: inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
      "    gelu_1: \"f32[1, 384, 1500]\" = torch.ops.aten.gelu.default(conv1d_1);  conv1d_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1178 in forward, code: inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
      "    permute: \"f32[1, 1500, 384]\" = torch.ops.aten.permute.default(gelu_1, [0, 2, 1]);  gelu_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1181 in forward, code: hidden_states = inputs_embeds + embed_pos\n",
      "    add: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(permute, arg4_1);  permute = arg4_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1182 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(dropout, [384], arg12_1, arg13_1);  arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg8_1, arg9_1);  arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg5_1);  arg5_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_1, [1, -1, 6, 64]);  linear_1 = None\n",
      "    transpose: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
      "    contiguous: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose);  transpose = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_1: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_2, [1, -1, 6, 64]);  linear_2 = None\n",
      "    transpose_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
      "    contiguous_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_1);  transpose_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_2: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear, [1, 1500, 6, 64]);  linear = None\n",
      "    transpose_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
      "    contiguous_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_2);  transpose_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_2, contiguous, contiguous_1);  contiguous_2 = contiguous = contiguous_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_3, [1, 1500, 384]);  transpose_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape, arg10_1, arg11_1);  reshape = arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_1: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_1: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_1: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_1, [384], arg18_1, arg19_1);  arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_1, arg14_1, arg15_1);  layer_norm_1 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_2, arg16_1, arg17_1);  dropout_2 = arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_3: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_2: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_2: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_2, [384], arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg20_1);  arg20_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_7, [1, -1, 6, 64]);  linear_7 = None\n",
      "    transpose_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None\n",
      "    contiguous_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_4);  transpose_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, arg21_1, arg22_1);  layer_norm_2 = arg21_1 = arg22_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_4: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_8, [1, -1, 6, 64]);  linear_8 = None\n",
      "    transpose_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
      "    contiguous_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_5);  transpose_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_5: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_6, [1, 1500, 6, 64]);  linear_6 = None\n",
      "    transpose_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
      "    contiguous_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_5, contiguous_3, contiguous_4);  contiguous_5 = contiguous_3 = contiguous_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_1: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_7, [1, 1500, 384]);  transpose_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_1, arg25_1, arg26_1);  reshape_1 = arg25_1 = arg26_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_4: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_3: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  dropout_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_3: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_3, [384], arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_3, arg29_1, arg30_1);  layer_norm_3 = arg29_1 = arg30_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_3: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_5, arg31_1, arg32_1);  dropout_5 = arg31_1 = arg32_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_6: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_4: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_4: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_4, [384], arg42_1, arg43_1);  arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg38_1, arg39_1);  arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg35_1);  arg35_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_6: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_13, [1, -1, 6, 64]);  linear_13 = None\n",
      "    transpose_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
      "    contiguous_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_8);  transpose_8 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, arg36_1, arg37_1);  layer_norm_4 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_14, [1, -1, 6, 64]);  linear_14 = None\n",
      "    transpose_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_7, 1, 2);  view_7 = None\n",
      "    contiguous_7: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_9);  transpose_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_8: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_12, [1, 1500, 6, 64]);  linear_12 = None\n",
      "    transpose_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
      "    contiguous_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_10);  transpose_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_8, contiguous_6, contiguous_7);  contiguous_8 = contiguous_6 = contiguous_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_2: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_11, [1, 1500, 384]);  transpose_11 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_2, arg40_1, arg41_1);  reshape_2 = arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_7: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_5: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  dropout_7 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_5: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_5, [384], arg48_1, arg49_1);  arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_5, arg44_1, arg45_1);  layer_norm_5 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_8: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_8, arg46_1, arg47_1);  dropout_8 = arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_9: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_6: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_6: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg50_1);  arg50_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "    view_9: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_19, [1, -1, 6, 64]);  linear_19 = None\n",
      "    transpose_12: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
      "    contiguous_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_12);  transpose_12 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, arg51_1, arg52_1);  layer_norm_6 = arg51_1 = arg52_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "    view_10: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_20, [1, -1, 6, 64]);  linear_20 = None\n",
      "    transpose_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
      "    contiguous_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_13);  transpose_13 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:693 in forward, code: query_states = self._shape(query_states, tgt_len, bsz)\n",
      "    view_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_18, [1, 1500, 6, 64]);  linear_18 = None\n",
      "    transpose_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_11, 1, 2);  view_11 = None\n",
      "    contiguous_11: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.contiguous.default(transpose_14);  transpose_14 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(contiguous_11, contiguous_9, contiguous_10);  contiguous_11 = contiguous_9 = contiguous_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
      "    transpose_15: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
      "    reshape_3: \"f32[1, 1500, 384]\" = torch.ops.aten.reshape.default(transpose_15, [1, 1500, 384]);  transpose_15 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(reshape_3, arg55_1, arg56_1);  reshape_3 = arg55_1 = arg56_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_10: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_7: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  dropout_10 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_7: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_7, [384], arg63_1, arg64_1);  arg63_1 = arg64_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_7, arg59_1, arg60_1);  layer_norm_7 = arg59_1 = arg60_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
      "    gelu_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
      "    dropout_11: \"f32[1, 1500, 1536]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(dropout_11, arg61_1, arg62_1);  dropout_11 = arg61_1 = arg62_1 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
      "    dropout_12: \"f32[1, 1500, 384]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
      "    add_8: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
      "    layer_norm_8: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_8, [384], arg65_1, arg66_1);  add_8 = arg65_1 = arg66_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_67451/113760652.py:112 in forward, code: whisper_chunks = self.get_whisper_chunk(\n",
      "    stack: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.stack.default([dropout, add_2, add_4, add_6, layer_norm_8], 2);  dropout = add_2 = add_4 = add_6 = layer_norm_8 = None\n",
      "    cat: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.cat.default([stack], 1);  stack = None\n",
      "    truediv: \"Sym(IntTrueDiv(u1, 16000))\" = item_1 / 16000\n",
      "    mul: \"Sym(25.0*(IntTrueDiv(u1, 16000)))\" = truediv * 25.0;  truediv = None\n",
      "    floor: \"Sym(FloorToInt(25.0*(IntTrueDiv(u1, 16000))))\" = math_floor(mul);  mul = floor = None\n",
      "    truediv_1: \"Sym(IntTrueDiv(u1, 16000))\" = item_1 / 16000;  item_1 = None\n",
      "    mul_1: \"Sym(50.0*(IntTrueDiv(u1, 16000)))\" = truediv_1 * 50.0;  truediv_1 = None\n",
      "    floor_1: \"Sym(FloorToInt(50.0*(IntTrueDiv(u1, 16000))))\" = math_floor(mul_1);  mul_1 = None\n",
      "    slice_1 = torch.ops.aten.slice.Tensor(cat, 1, None, floor_1);  cat = floor_1 = slice_1 = None\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MuseTalkGenerator([...]` with `torch.export.export(..., strict=False)`... \n",
      "[torch.onnx] Obtain model graph for `MuseTalkGenerator([...]` with `torch.export.export(..., strict=True)`...\n",
      "[torch.onnx] Obtain model graph for `MuseTalkGenerator([...]` with `torch.export.export(..., strict=True)`... \n"
     ]
    },
    {
     "ename": "TorchExportError",
     "evalue": "Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n- Debug `torch.export.export` and submit a PR to PyTorch.\n- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n\n## Exception summary\n\n<class 'torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode'>: Could not guard on data-dependent expression FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 (unhinted: FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0).  (Size-like symbols: none)\n\nconsider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_decomp/decompositions.py:745 in slice_forward)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u1\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File \"/tmp/ipykernel_67451/113760652.py\", line 81, in get_whisper_chunk\n    whisper_feature = whisper_feature[:,:actual_length,...]\n\nTo fix the error, insert one of the following checks before this call:\n  1. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) < 0)\n  2. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) >= 0)\n\n(These suggested fixes were derived by replacing `u1` with librosa_length in FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 and its negation.)\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n\n(Refer to the full stack trace above for more information.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGuardOnDataDependentSymNode\u001b[0m               Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_capture_strategies.py:118\u001b[0m, in \u001b[0;36mCaptureStrategy.__call__\u001b[0;34m(self, model, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     exported_program \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_capture_strategies.py:210\u001b[0m, in \u001b[0;36mTorchExportNonStrictStrategy._capture\u001b[0;34m(self, model, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mexc\u001b[38;5;241m.\u001b[39mUserError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Refine the dynamic shapes based on the suggested fixes.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/__init__.py:311\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m    310\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (new_msg,)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/__init__.py:277\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1163\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1159\u001b[0m             e\u001b[38;5;241m.\u001b[39mpartial_fx_graph,\n\u001b[1;32m   1160\u001b[0m             file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1161\u001b[0m         )\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1129\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1129\u001b[0m ep \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/exported_program.py:124\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:2255\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;129;01mand\u001b[39;00m export_training_ir_rollout_check():\n\u001b[0;32m-> 2255\u001b[0m     ep \u001b[38;5;241m=\u001b[39m \u001b[43m_export_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2264\u001b[0m     dtrace_structured(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexported_program\u001b[39m\u001b[38;5;124m\"\u001b[39m, payload_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mstr\u001b[39m(ep))\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1163\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1159\u001b[0m             e\u001b[38;5;241m.\u001b[39mpartial_fx_graph,\n\u001b[1;32m   1160\u001b[0m             file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1161\u001b[0m         )\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1129\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1129\u001b[0m ep \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/exported_program.py:124\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:2071\u001b[0m, in \u001b[0;36m_export_for_training\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     alive_fake_input_ids_before_export \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2066\u001b[0m         \u001b[38;5;28mid\u001b[39m(i)\n\u001b[1;32m   2067\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m gc\u001b[38;5;241m.\u001b[39mget_objects()\n\u001b[1;32m   2068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, torch\u001b[38;5;241m.\u001b[39m_subclasses\u001b[38;5;241m.\u001b[39mfake_tensor\u001b[38;5;241m.\u001b[39mFakeTensor)\n\u001b[1;32m   2069\u001b[0m     ]\n\u001b[0;32m-> 2071\u001b[0m export_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_to_aten_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_export_to_aten_ir_make_fx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m export_graph_signature \u001b[38;5;241m=\u001b[39m export_artifact\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39msig\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:2002\u001b[0m, in \u001b[0;36m_non_strict_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, orig_in_spec, prefer_deferred_runtime_asserts_over_guards, _to_aten_func)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   1992\u001b[0m     _fakify_script_objects(mod, fake_args, fake_kwargs, fake_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m   1993\u001b[0m         patched_mod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     _override_builtin_ops(),\n\u001b[1;32m   2001\u001b[0m ):\n\u001b[0;32m-> 2002\u001b[0m     aten_export_artifact \u001b[38;5;241m=\u001b[39m \u001b[43m_to_aten_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[1;32m   2003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatched_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_params_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fake_constant_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduce_guards_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_produce_guards_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tuplify_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;66;03m# aten_export_artifact.constants contains only fake script objects, we need to map them back\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1793\u001b[0m, in \u001b[0;36m_export_to_aten_ir_make_fx\u001b[0;34m(mod, fake_args, fake_kwargs, fake_params_buffers, constant_attrs, produce_guards_callback, transform)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   1783\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mstateless\u001b[38;5;241m.\u001b[39m_reparametrize_module(\n\u001b[1;32m   1784\u001b[0m         mod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     _compiling_state_context(),\n\u001b[1;32m   1792\u001b[0m ):\n\u001b[0;32m-> 1793\u001b[0m     gm, graph_signature \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_make_fx_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrace_joint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;66;03m# [NOTE] In training IR, we don't run\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;66;03m# any DCE as a result we preserve constant\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# nodes in the graph. make_fx invariant is that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# graph, the node.meta here actually doesn't matter. But\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;66;03m# we do this to make spec verifier happy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1922\u001b[0m, in \u001b[0;36m_non_strict_export.<locals>._tuplify_outputs.<locals>._aot_export_non_strict\u001b[0;34m(mod, args, kwargs, **flags)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m-> 1922\u001b[0m     gm, sig \u001b[38;5;241m=\u001b[39m \u001b[43maot_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExported program from AOTAutograd:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, gm)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1706\u001b[0m, in \u001b[0;36m_export_to_aten_ir_make_fx.<locals>._make_fx_helper\u001b[0;34m(mod, args, kwargs, **flags)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   1702\u001b[0m     ctx,\n\u001b[1;32m   1703\u001b[0m     override_getattribute_for_subclasses(flat_args),\n\u001b[1;32m   1704\u001b[0m     _maybe_restore_grad_state(),\n\u001b[1;32m   1705\u001b[0m ):\n\u001b[0;32m-> 1706\u001b[0m     gm \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m non_strict_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2429\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 2429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2356\u001b[0m, in \u001b[0;36m_MakefxTracer.trace\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_modes_from_inputs(f, args):\n\u001b[0;32m-> 2356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2318\u001b[0m, in \u001b[0;36m_MakefxTracer._trace_inner\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2318\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1303\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdispatch_trace\u001b[39m(\n\u001b[1;32m   1299\u001b[0m     root: Union[Module, Callable],\n\u001b[1;32m   1300\u001b[0m     tracer: Tracer,\n\u001b[1;32m   1301\u001b[0m     concrete_args: Optional[\u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 1303\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# NB: be careful not to DCE .item() calls\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1908\u001b[0m, in \u001b[0;36m_ModuleStackTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrace\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;28mself\u001b[39m, root: Union[Module, Callable], concrete_args: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]\n\u001b[1;32m   1907\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m fx\u001b[38;5;241m.\u001b[39mGraph:\n\u001b[0;32m-> 1908\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;66;03m# NOTE [export non-strict fake tensor leak detection]\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;66;03m# In non-strict export, we don't have dynamo's side effect\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;66;03m# tracking logic which makes some cases hard to detect.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m#      (2) Associated with gm.meta\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m#  (6) Do ID match with the proxies\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:868\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    862\u001b[0m         _autowrap_check(\n\u001b[1;32m    863\u001b[0m             patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 868\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    869\u001b[0m         {},\n\u001b[1;32m    870\u001b[0m         type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1361\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies, **_unused)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mproxy)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1361\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type:ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(Tensor, get_tensor_proxy_slot, out)\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11, arg12, arg13, arg14, arg15, arg16, arg17, arg18, arg19, arg20, arg21, arg22, arg23, arg24, arg25, arg26, arg27, arg28, arg29, arg30, arg31, arg32, arg33, arg34, arg35, arg36, arg37, arg38, arg39, arg40, arg41, arg42, arg43, arg44, arg45, arg46, arg47, arg48, arg49, arg50, arg51, arg52, arg53, arg54, arg55, arg56, arg57, arg58, arg59, arg60, arg61, arg62, arg63, arg64, arg65, arg66, arg67, arg68, arg69, arg70, arg71, arg72, arg73, arg74, arg75, arg76, arg77, arg78, arg79, arg80, arg81, arg82, arg83, arg84, arg85, arg86, arg87, arg88, arg89, arg90, arg91, arg92, arg93, arg94, arg95, arg96, arg97, arg98, arg99, arg100, arg101, arg102, arg103, arg104, arg105, arg106, arg107, arg108, arg109, arg110, arg111, arg112, arg113, arg114, arg115, arg116, arg117, arg118, arg119, arg120, arg121, arg122, arg123, arg124, arg125, arg126, arg127, arg128, arg129, arg130, arg131, arg132, arg133, arg134, arg135, arg136, arg137, arg138, arg139, arg140, arg141, arg142, arg143, arg144, arg145, arg146, arg147, arg148, arg149, arg150, arg151, arg152, arg153, arg154, arg155, arg156, arg157, arg158, arg159, arg160, arg161, arg162, arg163, arg164, arg165, arg166, arg167, arg168, arg169, arg170, arg171, arg172, arg173, arg174, arg175, arg176, arg177, arg178, arg179, arg180, arg181, arg182, arg183, arg184, arg185, arg186, arg187, arg188, arg189, arg190, arg191, arg192, arg193, arg194, arg195, arg196, arg197, arg198, arg199, arg200, arg201, arg202, arg203, arg204, arg205, arg206, arg207, arg208, arg209, arg210, arg211, arg212, arg213, arg214, arg215, arg216, arg217, arg218, arg219, arg220, arg221, arg222, arg223, arg224, arg225, arg226, arg227, arg228, arg229, arg230, arg231, arg232, arg233, arg234, arg235, arg236, arg237, arg238, arg239, arg240, arg241, arg242, arg243, arg244, arg245, arg246, arg247, arg248, arg249, arg250, arg251, arg252, arg253, arg254, arg255, arg256, arg257, arg258, arg259, arg260, arg261, arg262, arg263, arg264, arg265, arg266, arg267, arg268, arg269, arg270, arg271, arg272, arg273, arg274, arg275, arg276, arg277, arg278, arg279, arg280, arg281, arg282, arg283, arg284, arg285, arg286, arg287, arg288, arg289, arg290, arg291, arg292, arg293, arg294, arg295, arg296, arg297, arg298, arg299, arg300, arg301, arg302, arg303, arg304, arg305, arg306, arg307, arg308, arg309, arg310, arg311, arg312, arg313, arg314, arg315, arg316, arg317, arg318, arg319, arg320, arg321, arg322, arg323, arg324, arg325, arg326, arg327, arg328, arg329, arg330, arg331, arg332, arg333, arg334, arg335, arg336, arg337, arg338, arg339, arg340, arg341, arg342, arg343, arg344, arg345, arg346, arg347, arg348, arg349, arg350, arg351, arg352, arg353, arg354, arg355, arg356, arg357, arg358, arg359, arg360, arg361, arg362, arg363, arg364, arg365, arg366, arg367, arg368, arg369, arg370, arg371, arg372, arg373, arg374, arg375, arg376, arg377, arg378, arg379, arg380, arg381, arg382, arg383, arg384, arg385, arg386, arg387, arg388, arg389, arg390, arg391, arg392, arg393, arg394, arg395, arg396, arg397, arg398, arg399, arg400, arg401, arg402, arg403, arg404, arg405, arg406, arg407, arg408, arg409, arg410, arg411, arg412, arg413, arg414, arg415, arg416, arg417, arg418, arg419, arg420, arg421, arg422, arg423, arg424, arg425, arg426, arg427, arg428, arg429, arg430, arg431, arg432, arg433, arg434, arg435, arg436, arg437, arg438, arg439, arg440, arg441, arg442, arg443, arg444, arg445, arg446, arg447, arg448, arg449, arg450, arg451, arg452, arg453, arg454, arg455, arg456, arg457, arg458, arg459, arg460, arg461, arg462, arg463, arg464, arg465, arg466, arg467, arg468, arg469, arg470, arg471, arg472, arg473, arg474, arg475, arg476, arg477, arg478, arg479, arg480, arg481, arg482, arg483, arg484, arg485, arg486, arg487, arg488, arg489, arg490, arg491, arg492, arg493, arg494, arg495, arg496, arg497, arg498, arg499, arg500, arg501, arg502, arg503, arg504, arg505, arg506, arg507, arg508, arg509, arg510, arg511, arg512, arg513, arg514, arg515, arg516, arg517, arg518, arg519, arg520, arg521, arg522, arg523, arg524, arg525, arg526, arg527, arg528, arg529, arg530, arg531, arg532, arg533, arg534, arg535, arg536, arg537, arg538, arg539, arg540, arg541, arg542, arg543, arg544, arg545, arg546, arg547, arg548, arg549, arg550, arg551, arg552, arg553, arg554, arg555, arg556, arg557, arg558, arg559, arg560, arg561, arg562, arg563, arg564, arg565, arg566, arg567, arg568, arg569, arg570, arg571, arg572, arg573, arg574, arg575, arg576, arg577, arg578, arg579, arg580, arg581, arg582, arg583, arg584, arg585, arg586, arg587, arg588, arg589, arg590, arg591, arg592, arg593, arg594, arg595, arg596, arg597, arg598, arg599, arg600, arg601, arg602, arg603, arg604, arg605, arg606, arg607, arg608, arg609, arg610, arg611, arg612, arg613, arg614, arg615, arg616, arg617, arg618, arg619, arg620, arg621, arg622, arg623, arg624, arg625, arg626, arg627, arg628, arg629, arg630, arg631, arg632, arg633, arg634, arg635, arg636, arg637, arg638, arg639, arg640, arg641, arg642, arg643, arg644, arg645, arg646, arg647, arg648, arg649, arg650, arg651, arg652, arg653, arg654, arg655, arg656, arg657, arg658, arg659, arg660, arg661, arg662, arg663, arg664, arg665, arg666, arg667, arg668, arg669, arg670, arg671, arg672, arg673, arg674, arg675, arg676, arg677, arg678, arg679, arg680, arg681, arg682, arg683, arg684, arg685, arg686, arg687, arg688, arg689, arg690, arg691, arg692, arg693, arg694, arg695, arg696, arg697, arg698, arg699, arg700, arg701, arg702, arg703, arg704, arg705, arg706, arg707, arg708, arg709, arg710, arg711, arg712, arg713, arg714, arg715, arg716, arg717, arg718, arg719, arg720, arg721, arg722, arg723, arg724, arg725, arg726, arg727, arg728, arg729, arg730, arg731, arg732, arg733, arg734, arg735, arg736, arg737, arg738, arg739, arg740, arg741, arg742, arg743, arg744, arg745, arg746, arg747, arg748, arg749, arg750, arg751, arg752, arg753, arg754, arg755, arg756, arg757, arg758, arg759, arg760, arg761, arg762, arg763, arg764, arg765, arg766, arg767, arg768, arg769, arg770, arg771, arg772, arg773, arg774, arg775, arg776, arg777, arg778, arg779, arg780, arg781, arg782, arg783, arg784, arg785, arg786, arg787, arg788, arg789, arg790, arg791, arg792, arg793, arg794, arg795, arg796, arg797, arg798, arg799, arg800, arg801, arg802, arg803, arg804, arg805, arg806, arg807, arg808, arg809, arg810, arg811, arg812, arg813, arg814, arg815, arg816, arg817, arg818, arg819, arg820, arg821, arg822, arg823, arg824, arg825, arg826, arg827, arg828, arg829, arg830, arg831, arg832, arg833, arg834, arg835, arg836, arg837, arg838, arg839, arg840, arg841, arg842, arg843, arg844, arg845, arg846, arg847, arg848, arg849, arg850, arg851, arg852, arg853, arg854, arg855, arg856, arg857, arg858, arg859, arg860, arg861, arg862, arg863, arg864, arg865, arg866, arg867, arg868, arg869, arg870, arg871, arg872, arg873, arg874, arg875, arg876, arg877, arg878, arg879, arg880, arg881, arg882, arg883, arg884, arg885, arg886, arg887, arg888, arg889, arg890, arg891, arg892, arg893, arg894, arg895, arg896, arg897, arg898, arg899, arg900, arg901, arg902, arg903, arg904, arg905, arg906, arg907, arg908, arg909, arg910, arg911, arg912, arg913, arg914, arg915, arg916, arg917, arg918, arg919, arg920, arg921, arg922, arg923, arg924, arg925, arg926, arg927, arg928, arg929, arg930, arg931, arg932, arg933, arg934, arg935, arg936, arg937, arg938, arg939, arg940, arg941, arg942, arg943, arg944, arg945, arg946, arg947, arg948, arg949, arg950, arg951, arg952, arg953, arg954, arg955, arg956, arg957, arg958, arg959, arg960, arg961, arg962, arg963, arg964, arg965, arg966, arg967, arg968, arg969, arg970, arg971, arg972, arg973, arg974, arg975, arg976, arg977, arg978, arg979, arg980, arg981, arg982, arg983, arg984, arg985, arg986, arg987, arg988, arg989, arg990, arg991, arg992, arg993, arg994, arg995, arg996, arg997, arg998, arg999, arg1000, arg1001, arg1002, arg1003, arg1004, arg1005)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1593\u001b[0m, in \u001b[0;36m_export_to_aten_ir_make_fx.<locals>._make_fx_helper.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(flat_fn)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mflat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:187\u001b[0m, in \u001b[0;36mcreate_tree_flattened_fn.<locals>.flat_fn\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    186\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, tensor_args_spec)\n\u001b[0;32m--> 187\u001b[0m tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m flat_out, spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py:1354\u001b[0m, in \u001b[0;36mcreate_functional_call.<locals>.functional_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict_out_tuple \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m _autowrap_check(\n\u001b[1;32m    839\u001b[0m     patcher,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    842\u001b[0m )\n\u001b[0;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997\u001b[0m, in \u001b[0;36m_ModuleStackTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _ModuleNotInstalledAsSubmoduleError:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 560\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/export/_trace.py:1906\u001b[0m, in \u001b[0;36m_non_strict_export.<locals>._tuplify_outputs.<locals>._aot_export_non_strict.<locals>.Wrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1906\u001b[0m     tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m flat_outs, out_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m _autowrap_check(\n\u001b[1;32m    839\u001b[0m     patcher,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    842\u001b[0m )\n\u001b[0;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997\u001b[0m, in \u001b[0;36m_ModuleStackTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _ModuleNotInstalledAsSubmoduleError:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 560\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 112\u001b[0m, in \u001b[0;36mMuseTalkGenerator.forward\u001b[0;34m(self, whisper_input_features, latent_inputs, frame_idx, librosa_length)\u001b[0m\n\u001b[1;32m    111\u001b[0m librosa_length \u001b[38;5;241m=\u001b[39m librosa_length\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 112\u001b[0m whisper_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_whisper_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhisper_input_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhisper_input_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhisper_encoder_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhisper_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrosa_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrosa_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_fps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m latent_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[29], line 81\u001b[0m, in \u001b[0;36mMuseTalkGenerator.get_whisper_chunk\u001b[0;34m(self, whisper_input_features, device, weight_dtype, librosa_length, fps)\u001b[0m\n\u001b[1;32m     80\u001b[0m actual_length \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor((librosa_length \u001b[38;5;241m/\u001b[39m sr) \u001b[38;5;241m*\u001b[39m audio_fps)\n\u001b[0;32m---> 81\u001b[0m whisper_feature \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_feature\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mactual_length\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Calculate padding amount\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1409\u001b[0m, in \u001b[0;36mTorchFunctionMetadataMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts[func] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts\u001b[38;5;241m.\u001b[39mget(func, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1479\u001b[0m, in \u001b[0;36mPreDispatchTorchFunctionMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_export/non_strict_utils.py:1066\u001b[0m, in \u001b[0;36m_NonStrictTorchFunctionHandler.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GuardOnDataDependentSymNode \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_export/non_strict_utils.py:1041\u001b[0m, in \u001b[0;36m_NonStrictTorchFunctionHandler._override.<locals>.run\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _method, _args \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[0;32m-> 1041\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43m_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_ops.py:1255\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_ops.py:962\u001b[0m, in \u001b[0;36mOpOverload._get_dispatch.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _temporarily_pop_modes_from_pre_dispatch() \u001b[38;5;28;01mas\u001b[39;00m curr_mode:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_dispatch_mode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurr_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_library/utils.py:286\u001b[0m, in \u001b[0;36mhandle_dispatch_mode\u001b[0;34m(curr_mode, op_overload, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# TODO: check that I got these args correct (in C++, we pass in \"0000\"??)\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurr_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_dispatch__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_overload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverload_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/utils/_stats.py:28\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1534\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:994\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, pre_dispatch, args, kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _enable_thunkify(proxy_mode\u001b[38;5;241m.\u001b[39mtracer):\n\u001b[0;32m--> 994\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# live on CPU, but we could.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_ops.py:841\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/utils/_stats.py:28\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1376\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2096\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 2096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1511\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# We don't have a cache entry.\u001b[39;00m\n\u001b[0;32m-> 1511\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2639\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2637\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   2638\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m maybe_propagate_real_tensors(\n\u001b[0;32m-> 2639\u001b[0m             \u001b[43mdecomposition_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2640\u001b[0m         )\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   2643\u001b[0m     \u001b[38;5;66;03m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py:745\u001b[0m, in \u001b[0;36mslice_forward\u001b[0;34m(self, dim, start, end, step)\u001b[0m\n\u001b[1;32m    743\u001b[0m     start_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sizes[dim]\n\u001b[0;32m--> 745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mguard_size_oblivious\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    746\u001b[0m     end_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sizes[dim]\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:471\u001b[0m, in \u001b[0;36mguard_size_oblivious\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, torch\u001b[38;5;241m.\u001b[39mSymBool):\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguard_size_oblivious\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:596\u001b[0m, in \u001b[0;36mSymNode.guard_size_oblivious\u001b[0;34m(self, file, line)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# TODO: use the file/line for some useful diagnostic on why a\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;66;03m# guard occurred\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:512\u001b[0m, in \u001b[0;36mSymNode.evaluate\u001b[0;34m(self, size_oblivious)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, size_oblivious\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_sym_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7233\u001b[0m, in \u001b[0;36mShapeEnv.evaluate_sym_node\u001b[0;34m(self, sym_node, size_oblivious, fallback_value)\u001b[0m\n\u001b[1;32m   7232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expr_sym_node_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(sym_node)\n\u001b[0;32m-> 7233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_expr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7234\u001b[0m \u001b[43m    \u001b[49m\u001b[43msym_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7235\u001b[0m \u001b[43m    \u001b[49m\u001b[43msym_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7236\u001b[0m \u001b[43m    \u001b[49m\u001b[43msym_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7237\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7333\u001b[0m, in \u001b[0;36mShapeEnv.evaluate_expr\u001b[0;34m(self, orig_expr, hint, fx_node, size_oblivious, fallback_value, forcing_spec)\u001b[0m\n\u001b[1;32m   7332\u001b[0m suppress_guards_tls \u001b[38;5;241m=\u001b[39m ShapeEnv\u001b[38;5;241m.\u001b[39m_suppress_guards_tls()\n\u001b[0;32m-> 7333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_evaluate_expr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7334\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_expr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfx_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7337\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforcing_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7339\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_guards_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/recording.py:272\u001b[0m, in \u001b[0;36mrecord_shapeenv_event.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_env\u001b[38;5;241m.\u001b[39mshould_record_events \u001b[38;5;129;01mor\u001b[39;00m shape_env\u001b[38;5;241m.\u001b[39mis_recording:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# If ShapeEnv is already recording an event, call the wrapped\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# function directly.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# NB: here, we skip the check of whether all ShapeEnv instances\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# are equal, in favor of a faster dispatch.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retlog(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Retrieve an instance of ShapeEnv.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Assumption: the collection of args and kwargs may not reference\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# different ShapeEnv instances.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7356\u001b[0m, in \u001b[0;36mShapeEnv._inner_evaluate_expr\u001b[0;34m(self, orig_expr, hint, fx_node, size_oblivious, forcing_spec, _suppress_guards_tls, fallback_value)\u001b[0m\n\u001b[1;32m   7355\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 7356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_expr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7357\u001b[0m \u001b[43m        \u001b[49m\u001b[43morig_expr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfx_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfallback_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforcing_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcing_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7574\u001b[0m, in \u001b[0;36mShapeEnv._evaluate_expr\u001b[0;34m(self, orig_expr, hint, fx_node, size_oblivious, fallback_value, forcing_spec)\u001b[0m\n\u001b[1;32m   7573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m-> 7574\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_data_dependent_error(\n\u001b[1;32m   7575\u001b[0m             expr\u001b[38;5;241m.\u001b[39mxreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_to_val),\n\u001b[1;32m   7576\u001b[0m             expr,\n\u001b[1;32m   7577\u001b[0m             expr_sym_node_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expr_sym_node_id,\n\u001b[1;32m   7578\u001b[0m         )\n\u001b[1;32m   7579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mGuardOnDataDependentSymNode\u001b[0m: Could not guard on data-dependent expression FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 (unhinted: FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0).  (Size-like symbols: none)\n\nconsider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_decomp/decompositions.py:745 in slice_forward)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u1\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File \"/tmp/ipykernel_67451/113760652.py\", line 81, in get_whisper_chunk\n    whisper_feature = whisper_feature[:,:actual_length,...]\n\nTo fix the error, insert one of the following checks before this call:\n  1. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) < 0)\n  2. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) >= 0)\n\n(These suggested fixes were derived by replacing `u1` with librosa_length in FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 and its negation.)\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTorchExportError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m frame_idx_example \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      7\u001b[0m librosa_length_example \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m106_496\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhisper_input_features_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_inputs_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_idx_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrosa_length_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmusetalk.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhisper_input_features_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent_inputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibrosa_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/__init__.py:296\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, verbose, input_names, output_names, opset_version, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, export_params, keep_initializers_as_inputs, dynamic_axes, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Prepare legacy export parameters for potential fallback\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     legacy_export_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: training,\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperator_export_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: operator_export_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautograd_inlining\u001b[39m\u001b[38;5;124m\"\u001b[39m: autograd_inlining,\n\u001b[1;32m    294\u001b[0m     }\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_compat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_translation_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_translation_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexternal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdump_exported_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdump_exported_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegacy_export_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy_export_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_compat.py:143\u001b[0m, in \u001b[0;36mexport_compat\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, custom_translation_table, dynamic_axes, dynamic_shapes, keep_initializers_as_inputs, external_data, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, legacy_export_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m             registry\u001b[38;5;241m.\u001b[39mregister_op(torch_op, op, is_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     onnx_program \u001b[38;5;241m=\u001b[39m \u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes_with_export_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdump_exported_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdump_exported_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fallback:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_flags.py:23\u001b[0m, in \u001b[0;36mset_onnx_exporting_flag.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m _is_onnx_exporting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Ensure it resets even if an exception occurs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     _is_onnx_exporting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_core.py:1385\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, kwargs, registry, dynamic_shapes, input_names, output_names, report, verify, profile, dump_exported_program, artifacts_dir, verbose)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;66;03m# NOTE: We only throw the torch.export (first) exception because we want to\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m         \u001b[38;5;66;03m# focus on the torch.export.export error. Errors from other strategies like\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m         \u001b[38;5;66;03m# torch.jit.trace is due to the fallback and can be confusing to users.\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m         \u001b[38;5;66;03m# We save all errors in the error report.\u001b[39;00m\n\u001b[0;32m-> 1385\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _errors\u001b[38;5;241m.\u001b[39mTorchExportError(\n\u001b[1;32m   1386\u001b[0m             _STEP_ONE_ERROR_MESSAGE\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError report has been saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m report\n\u001b[1;32m   1390\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1391\u001b[0m             )\n\u001b[1;32m   1392\u001b[0m             \u001b[38;5;241m+\u001b[39m _summarize_exception_stack(first_error)\n\u001b[1;32m   1393\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfirst_error\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m program \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dump_exported_program:\n",
      "\u001b[0;31mTorchExportError\u001b[0m: Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n- Debug `torch.export.export` and submit a PR to PyTorch.\n- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n\n## Exception summary\n\n<class 'torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode'>: Could not guard on data-dependent expression FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 (unhinted: FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0).  (Size-like symbols: none)\n\nconsider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_decomp/decompositions.py:745 in slice_forward)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u1\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File \"/tmp/ipykernel_67451/113760652.py\", line 81, in get_whisper_chunk\n    whisper_feature = whisper_feature[:,:actual_length,...]\n\nTo fix the error, insert one of the following checks before this call:\n  1. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) < 0)\n  2. torch._check(math.floor(50.000000000000000*(librosa_length / 16000)) >= 0)\n\n(These suggested fixes were derived by replacing `u1` with librosa_length in FloorToInt(50.0*(IntTrueDiv(u1, 16000))) < 0 and its negation.)\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n\n(Refer to the full stack trace above for more information.)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "whisper_input_features_example = [torch.randn(1, 80, 3000)]\n",
    "latent_inputs_example = torch.randn(8, 8, 32, 32)\n",
    "\n",
    "frame_idx_example = torch.tensor(0, dtype=torch.long)\n",
    "librosa_length_example = torch.tensor(106_496, dtype=torch.long)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (\n",
    "        whisper_input_features_example,\n",
    "        latent_inputs_example,\n",
    "        frame_idx_example,\n",
    "        librosa_length_example,\n",
    "    ),\n",
    "    \"musetalk.onnx\",\n",
    "    input_names=[\n",
    "        \"whisper_input_features_0\",\n",
    "        \"latent_inputs\",\n",
    "        \"frame_idx\",\n",
    "        \"librosa_length\",\n",
    "    ],\n",
    "    output_names=[\"alias\"],\n",
    "    dynamo=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46022de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0+cu130',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"whisper_input_features_0\"<FLOAT,[1,80,3000]>,\n",
       "                %\"latent_inputs\"<FLOAT,[8,8,32,32]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"alias\"<UINT8,[8,256,256,3]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"whisper_encoder.embed_positions.weight\"<FLOAT,[1500,384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv1.weight\"<FLOAT,[384,80,3]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv2.weight\"<FLOAT,[384,384,3]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_1.weight\"<FLOAT,[1280,320]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_2.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.conv_in.weight\"<FLOAT,[320,8,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.conv_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv1.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv1.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.downsamplers.0.conv.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.downsamplers.0.conv.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv1.weight\"<FLOAT,[640,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv_shortcut.weight\"<FLOAT,[640,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv1.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.downsamplers.0.conv.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.downsamplers.0.conv.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv1.weight\"<FLOAT,[1280,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.downsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.downsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.upsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.upsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv1.weight\"<FLOAT,[1280,1920,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv_shortcut.weight\"<FLOAT,[1280,1920,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.upsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.upsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv1.weight\"<FLOAT,[640,1920,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[640,1920,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv1.weight\"<FLOAT,[640,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv_shortcut.weight\"<FLOAT,[640,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv1.weight\"<FLOAT,[640,960,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv_shortcut.weight\"<FLOAT,[640,960,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.upsamplers.0.conv.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.upsamplers.0.conv.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv1.weight\"<FLOAT,[320,960,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv_shortcut.weight\"<FLOAT,[320,960,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv1.weight\"<FLOAT,[320,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv_shortcut.weight\"<FLOAT,[320,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv1.weight\"<FLOAT,[320,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv_shortcut.weight\"<FLOAT,[320,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.conv_out.weight\"<FLOAT,[4,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.conv_out.bias\"<FLOAT,[4]>{TorchTensor<FLOAT,[4]>(Parameter containing: tensor([-0.0061, -0.0004,  0.0061, -0.0182], device='cuda:0', requires_grad=True), name='unet.conv_out.bias')},\n",
       "                %\"vae.post_quant_conv.weight\"<FLOAT,[4,4,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.post_quant_conv.bias\"<FLOAT,[4]>{TorchTensor<FLOAT,[4]>(Parameter containing: tensor([ 0.0321, -0.0843, -0.2432,  0.1315], device='cuda:0', requires_grad=True), name='vae.post_quant_conv.bias')},\n",
       "                %\"vae.decoder.conv_in.weight\"<FLOAT,[512,4,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_in.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_q.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_k.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_v.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_out.0.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.upsamplers.0.conv.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.upsamplers.0.conv.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.upsamplers.0.conv.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.upsamplers.0.conv.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv1.weight\"<FLOAT,[256,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[256,512,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv1.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv1.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.upsamplers.0.conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.upsamplers.0.conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv1.weight\"<FLOAT,[128,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight\"<FLOAT,[128,256,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv1.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv1.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_out.weight\"<FLOAT,[3,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_out.bias\"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(Parameter containing: tensor([ 0.0158, -0.0203, -0.0464], device='cuda:0', requires_grad=True), name='vae.decoder.conv_out.bias')},\n",
       "                %\"pe.pe\"<FLOAT,[1,5000,384]>{TorchTensor(...)},\n",
       "                %\"val_2\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_4\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_10\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, -1,  6, 64]), name='val_10')},\n",
       "                %\"val_11\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_24\"<INT64,[4]>{Tensor<INT64,[4]>(array([   1, 1500,    6,   64]), name='val_24')},\n",
       "                %\"val_43\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1500,   64]), name='val_43')},\n",
       "                %\"val_46\"<INT64,[4]>{Tensor<INT64,[4]>(array([   1,    6,   64, 1500]), name='val_46')},\n",
       "                %\"val_48\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.35355338], dtype=float32), name='val_48')},\n",
       "                %\"val_59\"<INT64,[3]>{Tensor<INT64,[3]>(array([   1, 1500,  384]), name='val_59')},\n",
       "                %\"val_60\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_64\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_66\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_70\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_72\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_79\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_126\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_130\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_132\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_136\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_138\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_145\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_192\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_196\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_198\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_202\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_204\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_211\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_258\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_262\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_264\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_281\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_281')},\n",
       "                %\"val_285\"<INT64,[1]>{Tensor<INT64,[1]>(array([332]), name='val_285')},\n",
       "                %\"val_289\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_289')},\n",
       "                %\"zeros_like\"<FLOAT,[1,4,5,384]>{Tensor(...)},\n",
       "                %\"zeros_like_1\"<FLOAT,[1,12,5,384]>{Tensor(...)},\n",
       "                %\"val_323\"<INT64,[1]>{Tensor<INT64,[1]>(array([10]), name='val_323')},\n",
       "                %\"val_331\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_331')},\n",
       "                %\"val_334\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_334')},\n",
       "                %\"val_341\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_341')},\n",
       "                %\"val_345\"<INT64,[1]>{Tensor<INT64,[1]>(array([14]), name='val_345')},\n",
       "                %\"val_353\"<INT64,[1]>{Tensor<INT64,[1]>(array([6]), name='val_353')},\n",
       "                %\"val_357\"<INT64,[1]>{Tensor<INT64,[1]>(array([16]), name='val_357')},\n",
       "                %\"val_365\"<INT64,[1]>{Tensor<INT64,[1]>(array([8]), name='val_365')},\n",
       "                %\"val_369\"<INT64,[1]>{Tensor<INT64,[1]>(array([18]), name='val_369')},\n",
       "                %\"val_380\"<INT64,[1]>{Tensor<INT64,[1]>(array([20]), name='val_380')},\n",
       "                %\"val_391\"<INT64,[1]>{Tensor<INT64,[1]>(array([22]), name='val_391')},\n",
       "                %\"val_402\"<INT64,[1]>{Tensor<INT64,[1]>(array([24]), name='val_402')},\n",
       "                %\"val_413\"<INT64,[1]>{Tensor<INT64,[1]>(array([26]), name='val_413')},\n",
       "                %\"val_424\"<INT64,[1]>{Tensor<INT64,[1]>(array([28]), name='val_424')},\n",
       "                %\"val_435\"<INT64,[1]>{Tensor<INT64,[1]>(array([30]), name='val_435')},\n",
       "                %\"val_446\"<INT64,[1]>{Tensor<INT64,[1]>(array([32]), name='val_446')},\n",
       "                %\"val_457\"<INT64,[1]>{Tensor<INT64,[1]>(array([34]), name='val_457')},\n",
       "                %\"val_468\"<INT64,[1]>{Tensor<INT64,[1]>(array([36]), name='val_468')},\n",
       "                %\"val_479\"<INT64,[1]>{Tensor<INT64,[1]>(array([38]), name='val_479')},\n",
       "                %\"val_490\"<INT64,[1]>{Tensor<INT64,[1]>(array([40]), name='val_490')},\n",
       "                %\"val_501\"<INT64,[1]>{Tensor<INT64,[1]>(array([42]), name='val_501')},\n",
       "                %\"val_512\"<INT64,[1]>{Tensor<INT64,[1]>(array([44]), name='val_512')},\n",
       "                %\"val_523\"<INT64,[1]>{Tensor<INT64,[1]>(array([46]), name='val_523')},\n",
       "                %\"val_534\"<INT64,[1]>{Tensor<INT64,[1]>(array([48]), name='val_534')},\n",
       "                %\"val_545\"<INT64,[1]>{Tensor<INT64,[1]>(array([50]), name='val_545')},\n",
       "                %\"val_556\"<INT64,[1]>{Tensor<INT64,[1]>(array([52]), name='val_556')},\n",
       "                %\"val_567\"<INT64,[1]>{Tensor<INT64,[1]>(array([54]), name='val_567')},\n",
       "                %\"val_578\"<INT64,[1]>{Tensor<INT64,[1]>(array([56]), name='val_578')},\n",
       "                %\"val_589\"<INT64,[1]>{Tensor<INT64,[1]>(array([58]), name='val_589')},\n",
       "                %\"val_600\"<INT64,[1]>{Tensor<INT64,[1]>(array([60]), name='val_600')},\n",
       "                %\"val_611\"<INT64,[1]>{Tensor<INT64,[1]>(array([62]), name='val_611')},\n",
       "                %\"val_622\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_622')},\n",
       "                %\"val_633\"<INT64,[1]>{Tensor<INT64,[1]>(array([66]), name='val_633')},\n",
       "                %\"val_644\"<INT64,[1]>{Tensor<INT64,[1]>(array([68]), name='val_644')},\n",
       "                %\"val_655\"<INT64,[1]>{Tensor<INT64,[1]>(array([70]), name='val_655')},\n",
       "                %\"val_666\"<INT64,[1]>{Tensor<INT64,[1]>(array([72]), name='val_666')},\n",
       "                %\"val_677\"<INT64,[1]>{Tensor<INT64,[1]>(array([74]), name='val_677')},\n",
       "                %\"val_688\"<INT64,[1]>{Tensor<INT64,[1]>(array([76]), name='val_688')},\n",
       "                %\"val_699\"<INT64,[1]>{Tensor<INT64,[1]>(array([78]), name='val_699')},\n",
       "                %\"val_710\"<INT64,[1]>{Tensor<INT64,[1]>(array([80]), name='val_710')},\n",
       "                %\"val_721\"<INT64,[1]>{Tensor<INT64,[1]>(array([82]), name='val_721')},\n",
       "                %\"val_732\"<INT64,[1]>{Tensor<INT64,[1]>(array([84]), name='val_732')},\n",
       "                %\"val_743\"<INT64,[1]>{Tensor<INT64,[1]>(array([86]), name='val_743')},\n",
       "                %\"val_754\"<INT64,[1]>{Tensor<INT64,[1]>(array([88]), name='val_754')},\n",
       "                %\"val_765\"<INT64,[1]>{Tensor<INT64,[1]>(array([90]), name='val_765')},\n",
       "                %\"val_776\"<INT64,[1]>{Tensor<INT64,[1]>(array([92]), name='val_776')},\n",
       "                %\"val_787\"<INT64,[1]>{Tensor<INT64,[1]>(array([94]), name='val_787')},\n",
       "                %\"val_798\"<INT64,[1]>{Tensor<INT64,[1]>(array([96]), name='val_798')},\n",
       "                %\"val_809\"<INT64,[1]>{Tensor<INT64,[1]>(array([98]), name='val_809')},\n",
       "                %\"val_820\"<INT64,[1]>{Tensor<INT64,[1]>(array([100]), name='val_820')},\n",
       "                %\"val_831\"<INT64,[1]>{Tensor<INT64,[1]>(array([102]), name='val_831')},\n",
       "                %\"val_842\"<INT64,[1]>{Tensor<INT64,[1]>(array([104]), name='val_842')},\n",
       "                %\"val_853\"<INT64,[1]>{Tensor<INT64,[1]>(array([106]), name='val_853')},\n",
       "                %\"val_864\"<INT64,[1]>{Tensor<INT64,[1]>(array([108]), name='val_864')},\n",
       "                %\"val_875\"<INT64,[1]>{Tensor<INT64,[1]>(array([110]), name='val_875')},\n",
       "                %\"val_886\"<INT64,[1]>{Tensor<INT64,[1]>(array([112]), name='val_886')},\n",
       "                %\"val_897\"<INT64,[1]>{Tensor<INT64,[1]>(array([114]), name='val_897')},\n",
       "                %\"val_908\"<INT64,[1]>{Tensor<INT64,[1]>(array([116]), name='val_908')},\n",
       "                %\"val_919\"<INT64,[1]>{Tensor<INT64,[1]>(array([118]), name='val_919')},\n",
       "                %\"val_930\"<INT64,[1]>{Tensor<INT64,[1]>(array([120]), name='val_930')},\n",
       "                %\"val_941\"<INT64,[1]>{Tensor<INT64,[1]>(array([122]), name='val_941')},\n",
       "                %\"val_952\"<INT64,[1]>{Tensor<INT64,[1]>(array([124]), name='val_952')},\n",
       "                %\"val_963\"<INT64,[1]>{Tensor<INT64,[1]>(array([126]), name='val_963')},\n",
       "                %\"val_974\"<INT64,[1]>{Tensor<INT64,[1]>(array([128]), name='val_974')},\n",
       "                %\"val_985\"<INT64,[1]>{Tensor<INT64,[1]>(array([130]), name='val_985')},\n",
       "                %\"val_996\"<INT64,[1]>{Tensor<INT64,[1]>(array([132]), name='val_996')},\n",
       "                %\"val_1007\"<INT64,[1]>{Tensor<INT64,[1]>(array([134]), name='val_1007')},\n",
       "                %\"val_1018\"<INT64,[1]>{Tensor<INT64,[1]>(array([136]), name='val_1018')},\n",
       "                %\"val_1029\"<INT64,[1]>{Tensor<INT64,[1]>(array([138]), name='val_1029')},\n",
       "                %\"val_1040\"<INT64,[1]>{Tensor<INT64,[1]>(array([140]), name='val_1040')},\n",
       "                %\"val_1051\"<INT64,[1]>{Tensor<INT64,[1]>(array([142]), name='val_1051')},\n",
       "                %\"val_1062\"<INT64,[1]>{Tensor<INT64,[1]>(array([144]), name='val_1062')},\n",
       "                %\"val_1073\"<INT64,[1]>{Tensor<INT64,[1]>(array([146]), name='val_1073')},\n",
       "                %\"val_1084\"<INT64,[1]>{Tensor<INT64,[1]>(array([148]), name='val_1084')},\n",
       "                %\"val_1095\"<INT64,[1]>{Tensor<INT64,[1]>(array([150]), name='val_1095')},\n",
       "                %\"val_1106\"<INT64,[1]>{Tensor<INT64,[1]>(array([152]), name='val_1106')},\n",
       "                %\"val_1117\"<INT64,[1]>{Tensor<INT64,[1]>(array([154]), name='val_1117')},\n",
       "                %\"val_1128\"<INT64,[1]>{Tensor<INT64,[1]>(array([156]), name='val_1128')},\n",
       "                %\"val_1139\"<INT64,[1]>{Tensor<INT64,[1]>(array([158]), name='val_1139')},\n",
       "                %\"val_1150\"<INT64,[1]>{Tensor<INT64,[1]>(array([160]), name='val_1150')},\n",
       "                %\"val_1161\"<INT64,[1]>{Tensor<INT64,[1]>(array([162]), name='val_1161')},\n",
       "                %\"val_1172\"<INT64,[1]>{Tensor<INT64,[1]>(array([164]), name='val_1172')},\n",
       "                %\"val_1183\"<INT64,[1]>{Tensor<INT64,[1]>(array([166]), name='val_1183')},\n",
       "                %\"val_1194\"<INT64,[1]>{Tensor<INT64,[1]>(array([168]), name='val_1194')},\n",
       "                %\"val_1205\"<INT64,[1]>{Tensor<INT64,[1]>(array([170]), name='val_1205')},\n",
       "                %\"val_1216\"<INT64,[1]>{Tensor<INT64,[1]>(array([172]), name='val_1216')},\n",
       "                %\"val_1227\"<INT64,[1]>{Tensor<INT64,[1]>(array([174]), name='val_1227')},\n",
       "                %\"val_1238\"<INT64,[1]>{Tensor<INT64,[1]>(array([176]), name='val_1238')},\n",
       "                %\"val_1249\"<INT64,[1]>{Tensor<INT64,[1]>(array([178]), name='val_1249')},\n",
       "                %\"val_1260\"<INT64,[1]>{Tensor<INT64,[1]>(array([180]), name='val_1260')},\n",
       "                %\"val_1271\"<INT64,[1]>{Tensor<INT64,[1]>(array([182]), name='val_1271')},\n",
       "                %\"val_1282\"<INT64,[1]>{Tensor<INT64,[1]>(array([184]), name='val_1282')},\n",
       "                %\"val_1293\"<INT64,[1]>{Tensor<INT64,[1]>(array([186]), name='val_1293')},\n",
       "                %\"val_1304\"<INT64,[1]>{Tensor<INT64,[1]>(array([188]), name='val_1304')},\n",
       "                %\"val_1315\"<INT64,[1]>{Tensor<INT64,[1]>(array([190]), name='val_1315')},\n",
       "                %\"val_1326\"<INT64,[1]>{Tensor<INT64,[1]>(array([192]), name='val_1326')},\n",
       "                %\"val_1337\"<INT64,[1]>{Tensor<INT64,[1]>(array([194]), name='val_1337')},\n",
       "                %\"val_1348\"<INT64,[1]>{Tensor<INT64,[1]>(array([196]), name='val_1348')},\n",
       "                %\"val_1359\"<INT64,[1]>{Tensor<INT64,[1]>(array([198]), name='val_1359')},\n",
       "                %\"val_1370\"<INT64,[1]>{Tensor<INT64,[1]>(array([200]), name='val_1370')},\n",
       "                %\"val_1381\"<INT64,[1]>{Tensor<INT64,[1]>(array([202]), name='val_1381')},\n",
       "                %\"val_1392\"<INT64,[1]>{Tensor<INT64,[1]>(array([204]), name='val_1392')},\n",
       "                %\"val_1403\"<INT64,[1]>{Tensor<INT64,[1]>(array([206]), name='val_1403')},\n",
       "                %\"val_1414\"<INT64,[1]>{Tensor<INT64,[1]>(array([208]), name='val_1414')},\n",
       "                %\"val_1425\"<INT64,[1]>{Tensor<INT64,[1]>(array([210]), name='val_1425')},\n",
       "                %\"val_1436\"<INT64,[1]>{Tensor<INT64,[1]>(array([212]), name='val_1436')},\n",
       "                %\"val_1447\"<INT64,[1]>{Tensor<INT64,[1]>(array([214]), name='val_1447')},\n",
       "                %\"val_1458\"<INT64,[1]>{Tensor<INT64,[1]>(array([216]), name='val_1458')},\n",
       "                %\"val_1469\"<INT64,[1]>{Tensor<INT64,[1]>(array([218]), name='val_1469')},\n",
       "                %\"val_1480\"<INT64,[1]>{Tensor<INT64,[1]>(array([220]), name='val_1480')},\n",
       "                %\"val_1491\"<INT64,[1]>{Tensor<INT64,[1]>(array([222]), name='val_1491')},\n",
       "                %\"val_1502\"<INT64,[1]>{Tensor<INT64,[1]>(array([224]), name='val_1502')},\n",
       "                %\"val_1513\"<INT64,[1]>{Tensor<INT64,[1]>(array([226]), name='val_1513')},\n",
       "                %\"val_1524\"<INT64,[1]>{Tensor<INT64,[1]>(array([228]), name='val_1524')},\n",
       "                %\"val_1535\"<INT64,[1]>{Tensor<INT64,[1]>(array([230]), name='val_1535')},\n",
       "                %\"val_1546\"<INT64,[1]>{Tensor<INT64,[1]>(array([232]), name='val_1546')},\n",
       "                %\"val_1557\"<INT64,[1]>{Tensor<INT64,[1]>(array([234]), name='val_1557')},\n",
       "                %\"val_1568\"<INT64,[1]>{Tensor<INT64,[1]>(array([236]), name='val_1568')},\n",
       "                %\"val_1579\"<INT64,[1]>{Tensor<INT64,[1]>(array([238]), name='val_1579')},\n",
       "                %\"val_1590\"<INT64,[1]>{Tensor<INT64,[1]>(array([240]), name='val_1590')},\n",
       "                %\"val_1601\"<INT64,[1]>{Tensor<INT64,[1]>(array([242]), name='val_1601')},\n",
       "                %\"val_1612\"<INT64,[1]>{Tensor<INT64,[1]>(array([244]), name='val_1612')},\n",
       "                %\"val_1623\"<INT64,[1]>{Tensor<INT64,[1]>(array([246]), name='val_1623')},\n",
       "                %\"val_1634\"<INT64,[1]>{Tensor<INT64,[1]>(array([248]), name='val_1634')},\n",
       "                %\"val_1645\"<INT64,[1]>{Tensor<INT64,[1]>(array([250]), name='val_1645')},\n",
       "                %\"val_1656\"<INT64,[1]>{Tensor<INT64,[1]>(array([252]), name='val_1656')},\n",
       "                %\"val_1667\"<INT64,[1]>{Tensor<INT64,[1]>(array([254]), name='val_1667')},\n",
       "                %\"val_1678\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_1678')},\n",
       "                %\"val_1689\"<INT64,[1]>{Tensor<INT64,[1]>(array([258]), name='val_1689')},\n",
       "                %\"val_1700\"<INT64,[1]>{Tensor<INT64,[1]>(array([260]), name='val_1700')},\n",
       "                %\"val_1711\"<INT64,[1]>{Tensor<INT64,[1]>(array([262]), name='val_1711')},\n",
       "                %\"val_1722\"<INT64,[1]>{Tensor<INT64,[1]>(array([264]), name='val_1722')},\n",
       "                %\"val_1733\"<INT64,[1]>{Tensor<INT64,[1]>(array([266]), name='val_1733')},\n",
       "                %\"val_1744\"<INT64,[1]>{Tensor<INT64,[1]>(array([268]), name='val_1744')},\n",
       "                %\"val_1755\"<INT64,[1]>{Tensor<INT64,[1]>(array([270]), name='val_1755')},\n",
       "                %\"val_1766\"<INT64,[1]>{Tensor<INT64,[1]>(array([272]), name='val_1766')},\n",
       "                %\"val_1777\"<INT64,[1]>{Tensor<INT64,[1]>(array([274]), name='val_1777')},\n",
       "                %\"val_1788\"<INT64,[1]>{Tensor<INT64,[1]>(array([276]), name='val_1788')},\n",
       "                %\"val_1799\"<INT64,[1]>{Tensor<INT64,[1]>(array([278]), name='val_1799')},\n",
       "                %\"val_1810\"<INT64,[1]>{Tensor<INT64,[1]>(array([280]), name='val_1810')},\n",
       "                %\"val_1821\"<INT64,[1]>{Tensor<INT64,[1]>(array([282]), name='val_1821')},\n",
       "                %\"val_1832\"<INT64,[1]>{Tensor<INT64,[1]>(array([284]), name='val_1832')},\n",
       "                %\"val_1843\"<INT64,[1]>{Tensor<INT64,[1]>(array([286]), name='val_1843')},\n",
       "                %\"val_1854\"<INT64,[1]>{Tensor<INT64,[1]>(array([288]), name='val_1854')},\n",
       "                %\"val_1865\"<INT64,[1]>{Tensor<INT64,[1]>(array([290]), name='val_1865')},\n",
       "                %\"val_1876\"<INT64,[1]>{Tensor<INT64,[1]>(array([292]), name='val_1876')},\n",
       "                %\"val_1887\"<INT64,[1]>{Tensor<INT64,[1]>(array([294]), name='val_1887')},\n",
       "                %\"val_1898\"<INT64,[1]>{Tensor<INT64,[1]>(array([296]), name='val_1898')},\n",
       "                %\"val_1909\"<INT64,[1]>{Tensor<INT64,[1]>(array([298]), name='val_1909')},\n",
       "                %\"val_1920\"<INT64,[1]>{Tensor<INT64,[1]>(array([300]), name='val_1920')},\n",
       "                %\"val_1931\"<INT64,[1]>{Tensor<INT64,[1]>(array([302]), name='val_1931')},\n",
       "                %\"val_1942\"<INT64,[1]>{Tensor<INT64,[1]>(array([304]), name='val_1942')},\n",
       "                %\"val_1953\"<INT64,[1]>{Tensor<INT64,[1]>(array([306]), name='val_1953')},\n",
       "                %\"val_1964\"<INT64,[1]>{Tensor<INT64,[1]>(array([308]), name='val_1964')},\n",
       "                %\"val_1975\"<INT64,[1]>{Tensor<INT64,[1]>(array([310]), name='val_1975')},\n",
       "                %\"val_1986\"<INT64,[1]>{Tensor<INT64,[1]>(array([312]), name='val_1986')},\n",
       "                %\"val_1997\"<INT64,[1]>{Tensor<INT64,[1]>(array([314]), name='val_1997')},\n",
       "                %\"val_2008\"<INT64,[1]>{Tensor<INT64,[1]>(array([316]), name='val_2008')},\n",
       "                %\"val_2019\"<INT64,[1]>{Tensor<INT64,[1]>(array([318]), name='val_2019')},\n",
       "                %\"val_2030\"<INT64,[1]>{Tensor<INT64,[1]>(array([320]), name='val_2030')},\n",
       "                %\"val_2041\"<INT64,[1]>{Tensor<INT64,[1]>(array([322]), name='val_2041')},\n",
       "                %\"val_2052\"<INT64,[1]>{Tensor<INT64,[1]>(array([324]), name='val_2052')},\n",
       "                %\"val_2063\"<INT64,[1]>{Tensor<INT64,[1]>(array([326]), name='val_2063')},\n",
       "                %\"val_2074\"<INT64,[1]>{Tensor<INT64,[1]>(array([328]), name='val_2074')},\n",
       "                %\"val_2085\"<INT64,[1]>{Tensor<INT64,[1]>(array([330]), name='val_2085')},\n",
       "                %\"val_2106\"<INT64,[1]>{Tensor<INT64,[1]>(array([334]), name='val_2106')},\n",
       "                %\"val_2117\"<INT64,[1]>{Tensor<INT64,[1]>(array([336]), name='val_2117')},\n",
       "                %\"val_2128\"<INT64,[1]>{Tensor<INT64,[1]>(array([338]), name='val_2128')},\n",
       "                %\"val_2139\"<INT64,[1]>{Tensor<INT64,[1]>(array([340]), name='val_2139')},\n",
       "                %\"val_2148\"<INT64,[3]>{Tensor<INT64,[3]>(array([166,  50, 384]), name='val_2148')},\n",
       "                %\"cat_4\"<FLOAT,[8,320]>{Tensor(...)},\n",
       "                %\"val_2203\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 0, 32, -1]), name='val_2203')},\n",
       "                %\"val_2207\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"val_2210\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"val_2219\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2221\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2245\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2247\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2269\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2271\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2276\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8, 1024,  320]), name='val_2276')},\n",
       "                %\"val_2279\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2280\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2281\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2287\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8, -1,  8, 40]), name='val_2287')},\n",
       "                %\"val_2316\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1024,   40]), name='val_2316')},\n",
       "                %\"val_2319\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    8,   40, 1024]), name='val_2319')},\n",
       "                %\"val_2321\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.39763537], dtype=float32), name='val_2321')},\n",
       "                %\"val_2332\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 320]), name='val_2332')},\n",
       "                %\"val_2333\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2337\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2338\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2339\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2374\"<INT64,[3]>{Tensor<INT64,[3]>(array([-1, 50, 40]), name='val_2374')},\n",
       "                %\"val_2377\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8,  8, 40, 50]), name='val_2377')},\n",
       "                %\"val_2391\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2395\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_2398\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_2405\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  32,  32, 320]), name='val_2405')},\n",
       "                %\"val_2425\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2427\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2449\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2451\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2472\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2474\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2482\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2483\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2484\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2536\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2540\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2541\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2542\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2594\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2598\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_2600\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_2627\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2629\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2651\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2653\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2674\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2676\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2681\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8, 256, 640]), name='val_2681')},\n",
       "                %\"val_2684\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2685\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2686\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2692\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8, -1,  8, 80]), name='val_2692')},\n",
       "                %\"val_2721\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 256,  80]), name='val_2721')},\n",
       "                %\"val_2724\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8,  80, 256]), name='val_2724')},\n",
       "                %\"val_2726\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.33437014], dtype=float32), name='val_2726')},\n",
       "                %\"val_2737\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 640]), name='val_2737')},\n",
       "                %\"val_2738\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2742\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2743\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2744\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2779\"<INT64,[3]>{Tensor<INT64,[3]>(array([-1, 50, 80]), name='val_2779')},\n",
       "                %\"val_2782\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8,  8, 80, 50]), name='val_2782')},\n",
       "                %\"val_2796\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2800\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_2803\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_2810\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  16,  16, 640]), name='val_2810')},\n",
       "                %\"val_2830\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2832\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2854\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2856\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2877\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2879\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2887\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2888\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2889\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2941\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2945\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2946\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2947\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2999\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_3003\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_3005\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_3032\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_3034\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_3056\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3058\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3079\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3081\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3086\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   64, 1280]), name='val_3086')},\n",
       "                %\"val_3089\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3090\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3091\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3097\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  -1,   8, 160]), name='val_3097')},\n",
       "                %\"val_3126\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  64, 160]), name='val_3126')},\n",
       "                %\"val_3129\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  64]), name='val_3129')},\n",
       "                %\"val_3131\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.28117067], dtype=float32), name='val_3131')},\n",
       "                %\"val_3142\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   -1, 1280]), name='val_3142')},\n",
       "                %\"val_3143\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3147\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3148\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3149\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3184\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  50, 160]), name='val_3184')},\n",
       "                %\"val_3187\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  50]), name='val_3187')},\n",
       "                %\"val_3201\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3205\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3208\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3215\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    8,    8, 1280]), name='val_3215')},\n",
       "                %\"val_3235\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3237\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3259\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3261\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3282\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3284\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3292\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3293\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3294\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3346\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3350\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3351\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3352\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3404\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3408\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3410\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3437\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3439\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3461\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3463\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3484\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3486\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3508\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3510\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3531\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3533\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3555\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3557\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3578\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3580\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3585\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   16, 1280]), name='val_3585')},\n",
       "                %\"val_3588\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3589\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3590\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3625\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  16, 160]), name='val_3625')},\n",
       "                %\"val_3628\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  16]), name='val_3628')},\n",
       "                %\"val_3642\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3646\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3647\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3648\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3700\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3704\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3706\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3713\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    4,    4, 1280]), name='val_3713')},\n",
       "                %\"val_3733\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3735\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3757\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3759\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3780\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3782\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3804\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3806\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3827\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3829\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3851\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3853\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3874\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3876\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3898\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3900\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3922\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3924\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3946\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3948\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3969\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3971\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3979\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3980\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3981\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4033\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4037\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4038\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4039\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4091\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4095\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4097\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4124\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_4126\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_4148\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4150\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4171\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4173\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4181\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4182\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4183\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4235\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4239\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4240\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4241\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4293\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4297\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4299\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4326\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4328\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4350\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4352\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4373\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4375\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4383\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4384\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4385\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4437\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4441\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4442\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4443\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4495\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4499\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4501\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4529\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4531\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4553\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4555\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4576\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4578\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4586\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4587\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4588\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4640\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4644\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4645\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4646\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4698\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4702\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_4704\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_4731\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4733\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4755\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4757\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4778\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4780\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4788\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4789\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4790\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4842\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4846\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4847\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4848\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4900\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4904\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_4906\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_4933\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_4935\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_4957\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4959\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4980\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4982\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4990\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4991\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4992\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5044\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5048\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5049\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_5050\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_5102\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5106\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_5108\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_5136\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_5138\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_5160\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5162\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5183\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5185\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5193\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5194\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5195\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5247\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5251\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5252\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5253\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5305\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5309\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5311\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5338\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5340\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5362\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5364\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5385\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5387\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5395\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5396\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5397\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5449\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5453\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5454\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5455\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5507\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5511\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5513\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5540\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5542\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5564\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5566\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5587\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5589\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5597\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5598\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5599\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5651\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5655\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5656\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5657\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5709\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5713\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5715\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5742\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5744\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5766\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5768\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5789\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5791\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5797\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,  512, 1024]), name='val_5797')},\n",
       "                %\"val_5818\"<FLOAT,[512,1]>{Tensor(...)},\n",
       "                %\"val_5820\"<FLOAT,[512,1]>{Tensor(...)},\n",
       "                %\"val_5821\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5823\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5825\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5832\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  -1,   1, 512]), name='val_5832')},\n",
       "                %\"val_5861\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1024,  512]), name='val_5861')},\n",
       "                %\"val_5864\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    1,  512, 1024]), name='val_5864')},\n",
       "                %\"val_5866\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.2102241], dtype=float32), name='val_5866')},\n",
       "                %\"val_5877\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 512]), name='val_5877')},\n",
       "                %\"val_5878\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5885\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512,  32,  32]), name='val_5885')},\n",
       "                %\"val_5905\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5907\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5928\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5930\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5951\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5953\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5974\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5976\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5997\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5999\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6020\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6022\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6043\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6045\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6066\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6068\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6090\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6092\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6113\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6115\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6136\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6138\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6159\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6161\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6182\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6184\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6205\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6207\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6229\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6231\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6252\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6254\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6275\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6277\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6298\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6300\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6321\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6323\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6344\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6346\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6368\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6370\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6391\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6393\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6414\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6416\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6437\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6439\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6460\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6462\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6483\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6485\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6506\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6508\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"scalar_tensor_default_7\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(2., dtype=float32), name='scalar_tensor_default_7')},\n",
       "                %\"clone_126\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(255., dtype=float32), name='clone_126')},\n",
       "                %\"add_133_min\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0., dtype=float32), name=None)},\n",
       "                %\"add_133_max\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)},\n",
       "                %\"val_2212\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 320,  32,  32]), name='val_2212')},\n",
       "                %\"val_6521\"<INT64,[2]>{Tensor<INT64,[2]>(array([2, 3]), name='val_6521')},\n",
       "                %\"val_2620\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 320,  16,  16]), name='val_2620')},\n",
       "                %\"val_2644\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,  16,  16]), name='val_2644')},\n",
       "                %\"val_3025\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,   8,   8]), name='val_3025')},\n",
       "                %\"val_3049\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,    8,    8]), name='val_3049')},\n",
       "                %\"val_3430\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,    4,    4]), name='val_3430')},\n",
       "                %\"val_3773\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 2560,    4,    4]), name='val_3773')},\n",
       "                %\"val_3902\"<FLOAT,[4]>{Tensor<FLOAT,[4]>(array([1., 1., 2., 2.], dtype=float32), name='val_3902')},\n",
       "                %\"val_3915\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 2560,    8,    8]), name='val_3915')},\n",
       "                %\"val_4319\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1920,    8,    8]), name='val_4319')},\n",
       "                %\"val_4522\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1920,   16,   16]), name='val_4522')},\n",
       "                %\"val_4724\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,   16,   16]), name='val_4724')},\n",
       "                %\"val_4926\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 960,  16,  16]), name='val_4926')},\n",
       "                %\"val_5129\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 960,  32,  32]), name='val_5129')},\n",
       "                %\"val_5331\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,  32,  32]), name='val_5331')},\n",
       "                %\"val_5746\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(5.4899807, dtype=float32), name=None)},\n",
       "                %\"val_6083\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512,  64,  64]), name='val_6083')},\n",
       "                %\"val_6222\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512, 128, 128]), name='val_6222')},\n",
       "                %\"val_6245\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 256, 128, 128]), name='val_6245')},\n",
       "                %\"val_6361\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 256, 256, 256]), name='val_6361')},\n",
       "                %\"val_6384\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 128, 256, 256]), name='val_6384')},\n",
       "                %\"val_6510\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)},\n",
       "                %\"val_6518\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_6518')},\n",
       "                %\"val_6519\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_6519')}\n",
       "            ),\n",
       "        ) {\n",
       "               0 |  # node_conv1d\n",
       "                    %\"conv1d\"<FLOAT,[1,384,3000]>  ::Conv(%\"whisper_input_features_0\", %\"whisper_encoder.conv1.weight\"{...}, %\"whisper_encoder.conv1.bias\"{...}) {group=1, pads=(1, 1), auto_pad='NOTSET', strides=(1,), dilations=(1,)}\n",
       "               1 |  # node_gelu\n",
       "                    %\"gelu\"<FLOAT,[1,384,3000]>  ::Gelu(%\"conv1d\") {approximate='none'}\n",
       "               2 |  # node_conv1d_1\n",
       "                    %\"conv1d_1\"<FLOAT,[1,384,1500]>  ::Conv(%\"gelu\", %\"whisper_encoder.conv2.weight\"{...}, %\"whisper_encoder.conv2.bias\"{...}) {group=1, pads=(1, 1), auto_pad='NOTSET', strides=(2,), dilations=(1,)}\n",
       "               3 |  # node_gelu_1\n",
       "                    %\"gelu_1\"<FLOAT,[1,384,1500]>  ::Gelu(%\"conv1d_1\") {approximate='none'}\n",
       "               4 |  # node_permute\n",
       "                    %\"permute\"<FLOAT,[1,1500,384]>  ::Transpose(%\"gelu_1\") {perm=(0, 2, 1)}\n",
       "               5 |  # node_add\n",
       "                    %\"add\"<FLOAT,[1,1500,384]>  ::Add(%\"permute\", %\"whisper_encoder.embed_positions.weight\"{...})\n",
       "               6 |  # node_layer_norm\n",
       "                    %\"layer_norm\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add\", %\"whisper_encoder.layers.0.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.0.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "               7 |  # node_MatMul_1\n",
       "                    %\"val_3\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_2\"{...})\n",
       "               8 |  # node_linear\n",
       "                    %\"linear\"<FLOAT,[1,1500,384]>  ::Add(%\"val_3\", %\"whisper_encoder.layers.0.self_attn.q_proj.bias\"{...})\n",
       "               9 |  # node_linear_1\n",
       "                    %\"linear_1\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_4\"{...})\n",
       "              10 |  # node_view\n",
       "                    %\"view\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_1\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              11 |  # node_transpose\n",
       "                    %\"transpose\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view\") {perm=(0, 2, 1, 3)}\n",
       "              12 |  # node_MatMul_10\n",
       "                    %\"val_12\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_11\"{...})\n",
       "              13 |  # node_linear_2\n",
       "                    %\"linear_2\"<FLOAT,[1,1500,384]>  ::Add(%\"val_12\", %\"whisper_encoder.layers.0.self_attn.v_proj.bias\"{...})\n",
       "              14 |  # node_view_1\n",
       "                    %\"view_1\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_2\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              15 |  # node_transpose_1\n",
       "                    %\"transpose_1\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_1\") {perm=(0, 2, 1, 3)}\n",
       "              16 |  # node_view_2\n",
       "                    %\"view_2\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              17 |  # node_transpose_2\n",
       "                    %\"transpose_2\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
       "              18 |  # node_Reshape_42\n",
       "                    %\"val_44\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              19 |  # node_Transpose_43\n",
       "                    %\"val_45\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_44\") {perm=(0, 2, 1)}\n",
       "              20 |  # node_Reshape_45\n",
       "                    %\"val_47\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_45\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              21 |  # node_Mul_47\n",
       "                    %\"val_49\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_2\", %\"val_48\"{[0.3535533845424652]})\n",
       "              22 |  # node_Mul_50\n",
       "                    %\"val_52\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_47\", %\"val_48\"{[0.3535533845424652]})\n",
       "              23 |  # node_MatMul_51\n",
       "                    %\"val_53\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_49\", %\"val_52\")\n",
       "              24 |  # node_Softmax_52\n",
       "                    %\"val_54\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_53\") {axis=-1}\n",
       "              25 |  # node_scaled_dot_product_attention\n",
       "                    %\"scaled_dot_product_attention\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_54\", %\"transpose_1\")\n",
       "              26 |  # node_transpose_3\n",
       "                    %\"transpose_3\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention\") {perm=(0, 2, 1, 3)}\n",
       "              27 |  # node_view_3\n",
       "                    %\"view_3\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_3\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              28 |  # node_MatMul_59\n",
       "                    %\"val_61\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_3\", %\"val_60\"{...})\n",
       "              29 |  # node_linear_3\n",
       "                    %\"linear_3\"<FLOAT,[1,1500,384]>  ::Add(%\"val_61\", %\"whisper_encoder.layers.0.self_attn.out_proj.bias\"{...})\n",
       "              30 |  # node_add_1\n",
       "                    %\"add_1\"<FLOAT,[1,1500,384]>  ::Add(%\"add\", %\"linear_3\")\n",
       "              31 |  # node_layer_norm_1\n",
       "                    %\"layer_norm_1\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1\", %\"whisper_encoder.layers.0.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.0.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              32 |  # node_MatMul_61\n",
       "                    %\"val_65\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_1\", %\"val_64\"{...})\n",
       "              33 |  # node_linear_4\n",
       "                    %\"linear_4\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_65\", %\"whisper_encoder.layers.0.fc1.bias\"{...})\n",
       "              34 |  # node_gelu_2\n",
       "                    %\"gelu_2\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_4\") {approximate='none'}\n",
       "              35 |  # node_MatMul_63\n",
       "                    %\"val_67\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_2\", %\"val_66\"{...})\n",
       "              36 |  # node_linear_5\n",
       "                    %\"linear_5\"<FLOAT,[1,1500,384]>  ::Add(%\"val_67\", %\"whisper_encoder.layers.0.fc2.bias\"{...})\n",
       "              37 |  # node_add_2\n",
       "                    %\"add_2\"<FLOAT,[1,1500,384]>  ::Add(%\"add_1\", %\"linear_5\")\n",
       "              38 |  # node_layer_norm_2\n",
       "                    %\"layer_norm_2\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2\", %\"whisper_encoder.layers.1.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.1.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              39 |  # node_MatMul_65\n",
       "                    %\"val_71\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_70\"{...})\n",
       "              40 |  # node_linear_6\n",
       "                    %\"linear_6\"<FLOAT,[1,1500,384]>  ::Add(%\"val_71\", %\"whisper_encoder.layers.1.self_attn.q_proj.bias\"{...})\n",
       "              41 |  # node_linear_7\n",
       "                    %\"linear_7\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_72\"{...})\n",
       "              42 |  # node_view_4\n",
       "                    %\"view_4\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_7\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              43 |  # node_transpose_4\n",
       "                    %\"transpose_4\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
       "              44 |  # node_MatMul_74\n",
       "                    %\"val_80\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_79\"{...})\n",
       "              45 |  # node_linear_8\n",
       "                    %\"linear_8\"<FLOAT,[1,1500,384]>  ::Add(%\"val_80\", %\"whisper_encoder.layers.1.self_attn.v_proj.bias\"{...})\n",
       "              46 |  # node_view_5\n",
       "                    %\"view_5\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_8\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              47 |  # node_transpose_5\n",
       "                    %\"transpose_5\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_5\") {perm=(0, 2, 1, 3)}\n",
       "              48 |  # node_view_6\n",
       "                    %\"view_6\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_6\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              49 |  # node_transpose_6\n",
       "                    %\"transpose_6\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_6\") {perm=(0, 2, 1, 3)}\n",
       "              50 |  # node_Reshape_104\n",
       "                    %\"val_110\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_4\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              51 |  # node_Transpose_105\n",
       "                    %\"val_111\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_110\") {perm=(0, 2, 1)}\n",
       "              52 |  # node_Reshape_107\n",
       "                    %\"val_113\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_111\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              53 |  # node_Mul_109\n",
       "                    %\"val_115\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_6\", %\"val_48\"{[0.3535533845424652]})\n",
       "              54 |  # node_Mul_112\n",
       "                    %\"val_118\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_113\", %\"val_48\"{[0.3535533845424652]})\n",
       "              55 |  # node_MatMul_113\n",
       "                    %\"val_119\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_115\", %\"val_118\")\n",
       "              56 |  # node_Softmax_114\n",
       "                    %\"val_120\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_119\") {axis=-1}\n",
       "              57 |  # node_scaled_dot_product_attention_1\n",
       "                    %\"scaled_dot_product_attention_1\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_120\", %\"transpose_5\")\n",
       "              58 |  # node_transpose_7\n",
       "                    %\"transpose_7\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(0, 2, 1, 3)}\n",
       "              59 |  # node_view_7\n",
       "                    %\"view_7\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_7\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              60 |  # node_MatMul_121\n",
       "                    %\"val_127\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_7\", %\"val_126\"{...})\n",
       "              61 |  # node_linear_9\n",
       "                    %\"linear_9\"<FLOAT,[1,1500,384]>  ::Add(%\"val_127\", %\"whisper_encoder.layers.1.self_attn.out_proj.bias\"{...})\n",
       "              62 |  # node_add_3\n",
       "                    %\"add_3\"<FLOAT,[1,1500,384]>  ::Add(%\"add_2\", %\"linear_9\")\n",
       "              63 |  # node_layer_norm_3\n",
       "                    %\"layer_norm_3\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3\", %\"whisper_encoder.layers.1.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.1.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              64 |  # node_MatMul_123\n",
       "                    %\"val_131\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_3\", %\"val_130\"{...})\n",
       "              65 |  # node_linear_10\n",
       "                    %\"linear_10\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_131\", %\"whisper_encoder.layers.1.fc1.bias\"{...})\n",
       "              66 |  # node_gelu_3\n",
       "                    %\"gelu_3\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_10\") {approximate='none'}\n",
       "              67 |  # node_MatMul_125\n",
       "                    %\"val_133\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_3\", %\"val_132\"{...})\n",
       "              68 |  # node_linear_11\n",
       "                    %\"linear_11\"<FLOAT,[1,1500,384]>  ::Add(%\"val_133\", %\"whisper_encoder.layers.1.fc2.bias\"{...})\n",
       "              69 |  # node_add_4\n",
       "                    %\"add_4\"<FLOAT,[1,1500,384]>  ::Add(%\"add_3\", %\"linear_11\")\n",
       "              70 |  # node_layer_norm_4\n",
       "                    %\"layer_norm_4\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4\", %\"whisper_encoder.layers.2.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.2.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              71 |  # node_MatMul_127\n",
       "                    %\"val_137\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_136\"{...})\n",
       "              72 |  # node_linear_12\n",
       "                    %\"linear_12\"<FLOAT,[1,1500,384]>  ::Add(%\"val_137\", %\"whisper_encoder.layers.2.self_attn.q_proj.bias\"{...})\n",
       "              73 |  # node_linear_13\n",
       "                    %\"linear_13\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_138\"{...})\n",
       "              74 |  # node_view_8\n",
       "                    %\"view_8\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_13\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              75 |  # node_transpose_8\n",
       "                    %\"transpose_8\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
       "              76 |  # node_MatMul_136\n",
       "                    %\"val_146\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_145\"{...})\n",
       "              77 |  # node_linear_14\n",
       "                    %\"linear_14\"<FLOAT,[1,1500,384]>  ::Add(%\"val_146\", %\"whisper_encoder.layers.2.self_attn.v_proj.bias\"{...})\n",
       "              78 |  # node_view_9\n",
       "                    %\"view_9\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_14\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              79 |  # node_transpose_9\n",
       "                    %\"transpose_9\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_9\") {perm=(0, 2, 1, 3)}\n",
       "              80 |  # node_view_10\n",
       "                    %\"view_10\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_12\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              81 |  # node_transpose_10\n",
       "                    %\"transpose_10\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
       "              82 |  # node_Reshape_166\n",
       "                    %\"val_176\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_8\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              83 |  # node_Transpose_167\n",
       "                    %\"val_177\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_176\") {perm=(0, 2, 1)}\n",
       "              84 |  # node_Reshape_169\n",
       "                    %\"val_179\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_177\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              85 |  # node_Mul_171\n",
       "                    %\"val_181\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_10\", %\"val_48\"{[0.3535533845424652]})\n",
       "              86 |  # node_Mul_174\n",
       "                    %\"val_184\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_179\", %\"val_48\"{[0.3535533845424652]})\n",
       "              87 |  # node_MatMul_175\n",
       "                    %\"val_185\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_181\", %\"val_184\")\n",
       "              88 |  # node_Softmax_176\n",
       "                    %\"val_186\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_185\") {axis=-1}\n",
       "              89 |  # node_scaled_dot_product_attention_2\n",
       "                    %\"scaled_dot_product_attention_2\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_186\", %\"transpose_9\")\n",
       "              90 |  # node_transpose_11\n",
       "                    %\"transpose_11\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(0, 2, 1, 3)}\n",
       "              91 |  # node_view_11\n",
       "                    %\"view_11\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_11\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              92 |  # node_MatMul_183\n",
       "                    %\"val_193\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_11\", %\"val_192\"{...})\n",
       "              93 |  # node_linear_15\n",
       "                    %\"linear_15\"<FLOAT,[1,1500,384]>  ::Add(%\"val_193\", %\"whisper_encoder.layers.2.self_attn.out_proj.bias\"{...})\n",
       "              94 |  # node_add_5\n",
       "                    %\"add_5\"<FLOAT,[1,1500,384]>  ::Add(%\"add_4\", %\"linear_15\")\n",
       "              95 |  # node_layer_norm_5\n",
       "                    %\"layer_norm_5\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_5\", %\"whisper_encoder.layers.2.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.2.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              96 |  # node_MatMul_185\n",
       "                    %\"val_197\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_5\", %\"val_196\"{...})\n",
       "              97 |  # node_linear_16\n",
       "                    %\"linear_16\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_197\", %\"whisper_encoder.layers.2.fc1.bias\"{...})\n",
       "              98 |  # node_gelu_4\n",
       "                    %\"gelu_4\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_16\") {approximate='none'}\n",
       "              99 |  # node_MatMul_187\n",
       "                    %\"val_199\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_4\", %\"val_198\"{...})\n",
       "             100 |  # node_linear_17\n",
       "                    %\"linear_17\"<FLOAT,[1,1500,384]>  ::Add(%\"val_199\", %\"whisper_encoder.layers.2.fc2.bias\"{...})\n",
       "             101 |  # node_add_6\n",
       "                    %\"add_6\"<FLOAT,[1,1500,384]>  ::Add(%\"add_5\", %\"linear_17\")\n",
       "             102 |  # node_layer_norm_6\n",
       "                    %\"layer_norm_6\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_6\", %\"whisper_encoder.layers.3.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.3.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             103 |  # node_MatMul_189\n",
       "                    %\"val_203\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_202\"{...})\n",
       "             104 |  # node_linear_18\n",
       "                    %\"linear_18\"<FLOAT,[1,1500,384]>  ::Add(%\"val_203\", %\"whisper_encoder.layers.3.self_attn.q_proj.bias\"{...})\n",
       "             105 |  # node_linear_19\n",
       "                    %\"linear_19\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_204\"{...})\n",
       "             106 |  # node_view_12\n",
       "                    %\"view_12\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_19\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "             107 |  # node_transpose_12\n",
       "                    %\"transpose_12\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_12\") {perm=(0, 2, 1, 3)}\n",
       "             108 |  # node_MatMul_198\n",
       "                    %\"val_212\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_211\"{...})\n",
       "             109 |  # node_linear_20\n",
       "                    %\"linear_20\"<FLOAT,[1,1500,384]>  ::Add(%\"val_212\", %\"whisper_encoder.layers.3.self_attn.v_proj.bias\"{...})\n",
       "             110 |  # node_view_13\n",
       "                    %\"view_13\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_20\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "             111 |  # node_transpose_13\n",
       "                    %\"transpose_13\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_13\") {perm=(0, 2, 1, 3)}\n",
       "             112 |  # node_view_14\n",
       "                    %\"view_14\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_18\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "             113 |  # node_transpose_14\n",
       "                    %\"transpose_14\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_14\") {perm=(0, 2, 1, 3)}\n",
       "             114 |  # node_Reshape_228\n",
       "                    %\"val_242\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_12\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "             115 |  # node_Transpose_229\n",
       "                    %\"val_243\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_242\") {perm=(0, 2, 1)}\n",
       "             116 |  # node_Reshape_231\n",
       "                    %\"val_245\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_243\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "             117 |  # node_Mul_233\n",
       "                    %\"val_247\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_14\", %\"val_48\"{[0.3535533845424652]})\n",
       "             118 |  # node_Mul_236\n",
       "                    %\"val_250\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_245\", %\"val_48\"{[0.3535533845424652]})\n",
       "             119 |  # node_MatMul_237\n",
       "                    %\"val_251\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_247\", %\"val_250\")\n",
       "             120 |  # node_Softmax_238\n",
       "                    %\"val_252\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_251\") {axis=-1}\n",
       "             121 |  # node_scaled_dot_product_attention_3\n",
       "                    %\"scaled_dot_product_attention_3\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_252\", %\"transpose_13\")\n",
       "             122 |  # node_transpose_15\n",
       "                    %\"transpose_15\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(0, 2, 1, 3)}\n",
       "             123 |  # node_view_15\n",
       "                    %\"view_15\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_15\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "             124 |  # node_MatMul_245\n",
       "                    %\"val_259\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_15\", %\"val_258\"{...})\n",
       "             125 |  # node_linear_21\n",
       "                    %\"linear_21\"<FLOAT,[1,1500,384]>  ::Add(%\"val_259\", %\"whisper_encoder.layers.3.self_attn.out_proj.bias\"{...})\n",
       "             126 |  # node_add_7\n",
       "                    %\"add_7\"<FLOAT,[1,1500,384]>  ::Add(%\"add_6\", %\"linear_21\")\n",
       "             127 |  # node_layer_norm_7\n",
       "                    %\"layer_norm_7\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_7\", %\"whisper_encoder.layers.3.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.3.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             128 |  # node_MatMul_247\n",
       "                    %\"val_263\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_7\", %\"val_262\"{...})\n",
       "             129 |  # node_linear_22\n",
       "                    %\"linear_22\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_263\", %\"whisper_encoder.layers.3.fc1.bias\"{...})\n",
       "             130 |  # node_gelu_5\n",
       "                    %\"gelu_5\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_22\") {approximate='none'}\n",
       "             131 |  # node_MatMul_249\n",
       "                    %\"val_265\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_5\", %\"val_264\"{...})\n",
       "             132 |  # node_linear_23\n",
       "                    %\"linear_23\"<FLOAT,[1,1500,384]>  ::Add(%\"val_265\", %\"whisper_encoder.layers.3.fc2.bias\"{...})\n",
       "             133 |  # node_add_8\n",
       "                    %\"add_8\"<FLOAT,[1,1500,384]>  ::Add(%\"add_7\", %\"linear_23\")\n",
       "             134 |  # node_layer_norm_8\n",
       "                    %\"layer_norm_8\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_8\", %\"whisper_encoder.layer_norm.weight\"{...}, %\"whisper_encoder.layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             135 |  # node_Unsqueeze_251\n",
       "                    %\"val_269\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add\", %\"val_331\"{[2]})\n",
       "             136 |  # node_Unsqueeze_253\n",
       "                    %\"val_271\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_2\", %\"val_331\"{[2]})\n",
       "             137 |  # node_Unsqueeze_255\n",
       "                    %\"val_273\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_4\", %\"val_331\"{[2]})\n",
       "             138 |  # node_Unsqueeze_257\n",
       "                    %\"val_275\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_6\", %\"val_331\"{[2]})\n",
       "             139 |  # node_Unsqueeze_259\n",
       "                    %\"val_277\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"layer_norm_8\", %\"val_331\"{[2]})\n",
       "             140 |  # node_stack\n",
       "                    %\"stack\"<FLOAT,[1,1500,5,384]>  ::Concat(%\"val_269\", %\"val_271\", %\"val_273\", %\"val_275\", %\"val_277\") {axis=2}\n",
       "             141 |  # node_slice_1\n",
       "                    %\"slice_1\"<FLOAT,[1,332,5,384]>  ::Slice(%\"stack\", %\"val_281\"{[0]}, %\"val_285\"{[332]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             142 |  # node_cat_1\n",
       "                    %\"cat_1\"<FLOAT,[1,348,5,384]>  ::Concat(%\"zeros_like\"{...}, %\"slice_1\", %\"zeros_like_1\"{...}) {axis=1}\n",
       "             143 |  # node_slice_4\n",
       "                    %\"slice_4\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_281\"{[0]}, %\"val_323\"{[10]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             144 |  # node_slice_5\n",
       "                    %\"slice_5\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_331\"{[2]}, %\"val_334\"{[12]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             145 |  # node_slice_6\n",
       "                    %\"slice_6\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_341\"{[4]}, %\"val_345\"{[14]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             146 |  # node_slice_7\n",
       "                    %\"slice_7\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_353\"{[6]}, %\"val_357\"{[16]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             147 |  # node_slice_8\n",
       "                    %\"slice_8\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_365\"{[8]}, %\"val_369\"{[18]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             148 |  # node_slice_9\n",
       "                    %\"slice_9\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_323\"{[10]}, %\"val_380\"{[20]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             149 |  # node_slice_10\n",
       "                    %\"slice_10\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_334\"{[12]}, %\"val_391\"{[22]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             150 |  # node_slice_11\n",
       "                    %\"slice_11\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_345\"{[14]}, %\"val_402\"{[24]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             151 |  # node_slice_12\n",
       "                    %\"slice_12\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_357\"{[16]}, %\"val_413\"{[26]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             152 |  # node_slice_13\n",
       "                    %\"slice_13\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_369\"{[18]}, %\"val_424\"{[28]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             153 |  # node_slice_14\n",
       "                    %\"slice_14\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_380\"{[20]}, %\"val_435\"{[30]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             154 |  # node_slice_15\n",
       "                    %\"slice_15\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_391\"{[22]}, %\"val_446\"{[32]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             155 |  # node_slice_16\n",
       "                    %\"slice_16\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_402\"{[24]}, %\"val_457\"{[34]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             156 |  # node_slice_17\n",
       "                    %\"slice_17\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_413\"{[26]}, %\"val_468\"{[36]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             157 |  # node_slice_18\n",
       "                    %\"slice_18\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_424\"{[28]}, %\"val_479\"{[38]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             158 |  # node_slice_19\n",
       "                    %\"slice_19\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_435\"{[30]}, %\"val_490\"{[40]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             159 |  # node_slice_20\n",
       "                    %\"slice_20\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_446\"{[32]}, %\"val_501\"{[42]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             160 |  # node_slice_21\n",
       "                    %\"slice_21\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_457\"{[34]}, %\"val_512\"{[44]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             161 |  # node_slice_22\n",
       "                    %\"slice_22\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_468\"{[36]}, %\"val_523\"{[46]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             162 |  # node_slice_23\n",
       "                    %\"slice_23\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_479\"{[38]}, %\"val_534\"{[48]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             163 |  # node_slice_24\n",
       "                    %\"slice_24\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_490\"{[40]}, %\"val_545\"{[50]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             164 |  # node_slice_25\n",
       "                    %\"slice_25\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_501\"{[42]}, %\"val_556\"{[52]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             165 |  # node_slice_26\n",
       "                    %\"slice_26\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_512\"{[44]}, %\"val_567\"{[54]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             166 |  # node_slice_27\n",
       "                    %\"slice_27\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_523\"{[46]}, %\"val_578\"{[56]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             167 |  # node_slice_28\n",
       "                    %\"slice_28\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_534\"{[48]}, %\"val_589\"{[58]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             168 |  # node_slice_29\n",
       "                    %\"slice_29\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_545\"{[50]}, %\"val_600\"{[60]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             169 |  # node_slice_30\n",
       "                    %\"slice_30\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_556\"{[52]}, %\"val_611\"{[62]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             170 |  # node_slice_31\n",
       "                    %\"slice_31\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_567\"{[54]}, %\"val_622\"{[64]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             171 |  # node_slice_32\n",
       "                    %\"slice_32\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_578\"{[56]}, %\"val_633\"{[66]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             172 |  # node_slice_33\n",
       "                    %\"slice_33\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_589\"{[58]}, %\"val_644\"{[68]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             173 |  # node_slice_34\n",
       "                    %\"slice_34\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_600\"{[60]}, %\"val_655\"{[70]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             174 |  # node_slice_35\n",
       "                    %\"slice_35\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_611\"{[62]}, %\"val_666\"{[72]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             175 |  # node_slice_36\n",
       "                    %\"slice_36\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_622\"{[64]}, %\"val_677\"{[74]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             176 |  # node_slice_37\n",
       "                    %\"slice_37\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_633\"{[66]}, %\"val_688\"{[76]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             177 |  # node_slice_38\n",
       "                    %\"slice_38\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_644\"{[68]}, %\"val_699\"{[78]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             178 |  # node_slice_39\n",
       "                    %\"slice_39\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_655\"{[70]}, %\"val_710\"{[80]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             179 |  # node_slice_40\n",
       "                    %\"slice_40\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_666\"{[72]}, %\"val_721\"{[82]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             180 |  # node_slice_41\n",
       "                    %\"slice_41\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_677\"{[74]}, %\"val_732\"{[84]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             181 |  # node_slice_42\n",
       "                    %\"slice_42\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_688\"{[76]}, %\"val_743\"{[86]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             182 |  # node_slice_43\n",
       "                    %\"slice_43\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_699\"{[78]}, %\"val_754\"{[88]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             183 |  # node_slice_44\n",
       "                    %\"slice_44\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_710\"{[80]}, %\"val_765\"{[90]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             184 |  # node_slice_45\n",
       "                    %\"slice_45\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_721\"{[82]}, %\"val_776\"{[92]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             185 |  # node_slice_46\n",
       "                    %\"slice_46\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_732\"{[84]}, %\"val_787\"{[94]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             186 |  # node_slice_47\n",
       "                    %\"slice_47\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_743\"{[86]}, %\"val_798\"{[96]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             187 |  # node_slice_48\n",
       "                    %\"slice_48\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_754\"{[88]}, %\"val_809\"{[98]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             188 |  # node_slice_49\n",
       "                    %\"slice_49\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_765\"{[90]}, %\"val_820\"{[100]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             189 |  # node_slice_50\n",
       "                    %\"slice_50\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_776\"{[92]}, %\"val_831\"{[102]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             190 |  # node_slice_51\n",
       "                    %\"slice_51\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_787\"{[94]}, %\"val_842\"{[104]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             191 |  # node_slice_52\n",
       "                    %\"slice_52\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_798\"{[96]}, %\"val_853\"{[106]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             192 |  # node_slice_53\n",
       "                    %\"slice_53\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_809\"{[98]}, %\"val_864\"{[108]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             193 |  # node_slice_54\n",
       "                    %\"slice_54\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_820\"{[100]}, %\"val_875\"{[110]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             194 |  # node_slice_55\n",
       "                    %\"slice_55\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_831\"{[102]}, %\"val_886\"{[112]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             195 |  # node_slice_56\n",
       "                    %\"slice_56\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_842\"{[104]}, %\"val_897\"{[114]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             196 |  # node_slice_57\n",
       "                    %\"slice_57\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_853\"{[106]}, %\"val_908\"{[116]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             197 |  # node_slice_58\n",
       "                    %\"slice_58\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_864\"{[108]}, %\"val_919\"{[118]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             198 |  # node_slice_59\n",
       "                    %\"slice_59\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_875\"{[110]}, %\"val_930\"{[120]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             199 |  # node_slice_60\n",
       "                    %\"slice_60\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_886\"{[112]}, %\"val_941\"{[122]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             200 |  # node_slice_61\n",
       "                    %\"slice_61\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_897\"{[114]}, %\"val_952\"{[124]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             201 |  # node_slice_62\n",
       "                    %\"slice_62\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_908\"{[116]}, %\"val_963\"{[126]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             202 |  # node_slice_63\n",
       "                    %\"slice_63\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_919\"{[118]}, %\"val_974\"{[128]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             203 |  # node_slice_64\n",
       "                    %\"slice_64\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_930\"{[120]}, %\"val_985\"{[130]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             204 |  # node_slice_65\n",
       "                    %\"slice_65\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_941\"{[122]}, %\"val_996\"{[132]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             205 |  # node_slice_66\n",
       "                    %\"slice_66\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_952\"{[124]}, %\"val_1007\"{[134]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             206 |  # node_slice_67\n",
       "                    %\"slice_67\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_963\"{[126]}, %\"val_1018\"{[136]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             207 |  # node_slice_68\n",
       "                    %\"slice_68\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_974\"{[128]}, %\"val_1029\"{[138]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             208 |  # node_slice_69\n",
       "                    %\"slice_69\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_985\"{[130]}, %\"val_1040\"{[140]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             209 |  # node_slice_70\n",
       "                    %\"slice_70\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_996\"{[132]}, %\"val_1051\"{[142]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             210 |  # node_slice_71\n",
       "                    %\"slice_71\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1007\"{[134]}, %\"val_1062\"{[144]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             211 |  # node_slice_72\n",
       "                    %\"slice_72\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1018\"{[136]}, %\"val_1073\"{[146]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             212 |  # node_slice_73\n",
       "                    %\"slice_73\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1029\"{[138]}, %\"val_1084\"{[148]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             213 |  # node_slice_74\n",
       "                    %\"slice_74\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1040\"{[140]}, %\"val_1095\"{[150]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             214 |  # node_slice_75\n",
       "                    %\"slice_75\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1051\"{[142]}, %\"val_1106\"{[152]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             215 |  # node_slice_76\n",
       "                    %\"slice_76\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1062\"{[144]}, %\"val_1117\"{[154]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             216 |  # node_slice_77\n",
       "                    %\"slice_77\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1073\"{[146]}, %\"val_1128\"{[156]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             217 |  # node_slice_78\n",
       "                    %\"slice_78\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1084\"{[148]}, %\"val_1139\"{[158]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             218 |  # node_slice_79\n",
       "                    %\"slice_79\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1095\"{[150]}, %\"val_1150\"{[160]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             219 |  # node_slice_80\n",
       "                    %\"slice_80\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1106\"{[152]}, %\"val_1161\"{[162]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             220 |  # node_slice_81\n",
       "                    %\"slice_81\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1117\"{[154]}, %\"val_1172\"{[164]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             221 |  # node_slice_82\n",
       "                    %\"slice_82\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1128\"{[156]}, %\"val_1183\"{[166]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             222 |  # node_slice_83\n",
       "                    %\"slice_83\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1139\"{[158]}, %\"val_1194\"{[168]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             223 |  # node_slice_84\n",
       "                    %\"slice_84\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1150\"{[160]}, %\"val_1205\"{[170]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             224 |  # node_slice_85\n",
       "                    %\"slice_85\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1161\"{[162]}, %\"val_1216\"{[172]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             225 |  # node_slice_86\n",
       "                    %\"slice_86\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1172\"{[164]}, %\"val_1227\"{[174]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             226 |  # node_slice_87\n",
       "                    %\"slice_87\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1183\"{[166]}, %\"val_1238\"{[176]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             227 |  # node_slice_88\n",
       "                    %\"slice_88\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1194\"{[168]}, %\"val_1249\"{[178]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             228 |  # node_slice_89\n",
       "                    %\"slice_89\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1205\"{[170]}, %\"val_1260\"{[180]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             229 |  # node_slice_90\n",
       "                    %\"slice_90\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1216\"{[172]}, %\"val_1271\"{[182]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             230 |  # node_slice_91\n",
       "                    %\"slice_91\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1227\"{[174]}, %\"val_1282\"{[184]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             231 |  # node_slice_92\n",
       "                    %\"slice_92\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1238\"{[176]}, %\"val_1293\"{[186]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             232 |  # node_slice_93\n",
       "                    %\"slice_93\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1249\"{[178]}, %\"val_1304\"{[188]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             233 |  # node_slice_94\n",
       "                    %\"slice_94\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1260\"{[180]}, %\"val_1315\"{[190]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             234 |  # node_slice_95\n",
       "                    %\"slice_95\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1271\"{[182]}, %\"val_1326\"{[192]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             235 |  # node_slice_96\n",
       "                    %\"slice_96\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1282\"{[184]}, %\"val_1337\"{[194]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             236 |  # node_slice_97\n",
       "                    %\"slice_97\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1293\"{[186]}, %\"val_1348\"{[196]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             237 |  # node_slice_98\n",
       "                    %\"slice_98\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1304\"{[188]}, %\"val_1359\"{[198]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             238 |  # node_slice_99\n",
       "                    %\"slice_99\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1315\"{[190]}, %\"val_1370\"{[200]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             239 |  # node_slice_100\n",
       "                    %\"slice_100\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1326\"{[192]}, %\"val_1381\"{[202]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             240 |  # node_slice_101\n",
       "                    %\"slice_101\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1337\"{[194]}, %\"val_1392\"{[204]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             241 |  # node_slice_102\n",
       "                    %\"slice_102\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1348\"{[196]}, %\"val_1403\"{[206]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             242 |  # node_slice_103\n",
       "                    %\"slice_103\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1359\"{[198]}, %\"val_1414\"{[208]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             243 |  # node_slice_104\n",
       "                    %\"slice_104\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1370\"{[200]}, %\"val_1425\"{[210]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             244 |  # node_slice_105\n",
       "                    %\"slice_105\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1381\"{[202]}, %\"val_1436\"{[212]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             245 |  # node_slice_106\n",
       "                    %\"slice_106\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1392\"{[204]}, %\"val_1447\"{[214]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             246 |  # node_slice_107\n",
       "                    %\"slice_107\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1403\"{[206]}, %\"val_1458\"{[216]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             247 |  # node_slice_108\n",
       "                    %\"slice_108\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1414\"{[208]}, %\"val_1469\"{[218]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             248 |  # node_slice_109\n",
       "                    %\"slice_109\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1425\"{[210]}, %\"val_1480\"{[220]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             249 |  # node_slice_110\n",
       "                    %\"slice_110\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1436\"{[212]}, %\"val_1491\"{[222]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             250 |  # node_slice_111\n",
       "                    %\"slice_111\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1447\"{[214]}, %\"val_1502\"{[224]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             251 |  # node_slice_112\n",
       "                    %\"slice_112\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1458\"{[216]}, %\"val_1513\"{[226]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             252 |  # node_slice_113\n",
       "                    %\"slice_113\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1469\"{[218]}, %\"val_1524\"{[228]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             253 |  # node_slice_114\n",
       "                    %\"slice_114\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1480\"{[220]}, %\"val_1535\"{[230]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             254 |  # node_slice_115\n",
       "                    %\"slice_115\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1491\"{[222]}, %\"val_1546\"{[232]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             255 |  # node_slice_116\n",
       "                    %\"slice_116\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1502\"{[224]}, %\"val_1557\"{[234]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             256 |  # node_slice_117\n",
       "                    %\"slice_117\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1513\"{[226]}, %\"val_1568\"{[236]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             257 |  # node_slice_118\n",
       "                    %\"slice_118\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1524\"{[228]}, %\"val_1579\"{[238]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             258 |  # node_slice_119\n",
       "                    %\"slice_119\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1535\"{[230]}, %\"val_1590\"{[240]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             259 |  # node_slice_120\n",
       "                    %\"slice_120\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1546\"{[232]}, %\"val_1601\"{[242]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             260 |  # node_slice_121\n",
       "                    %\"slice_121\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1557\"{[234]}, %\"val_1612\"{[244]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             261 |  # node_slice_122\n",
       "                    %\"slice_122\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1568\"{[236]}, %\"val_1623\"{[246]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             262 |  # node_slice_123\n",
       "                    %\"slice_123\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1579\"{[238]}, %\"val_1634\"{[248]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             263 |  # node_slice_124\n",
       "                    %\"slice_124\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1590\"{[240]}, %\"val_1645\"{[250]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             264 |  # node_slice_125\n",
       "                    %\"slice_125\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1601\"{[242]}, %\"val_1656\"{[252]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             265 |  # node_slice_126\n",
       "                    %\"slice_126\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1612\"{[244]}, %\"val_1667\"{[254]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             266 |  # node_slice_127\n",
       "                    %\"slice_127\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1623\"{[246]}, %\"val_1678\"{[256]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             267 |  # node_slice_128\n",
       "                    %\"slice_128\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1634\"{[248]}, %\"val_1689\"{[258]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             268 |  # node_slice_129\n",
       "                    %\"slice_129\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1645\"{[250]}, %\"val_1700\"{[260]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             269 |  # node_slice_130\n",
       "                    %\"slice_130\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1656\"{[252]}, %\"val_1711\"{[262]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             270 |  # node_slice_131\n",
       "                    %\"slice_131\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1667\"{[254]}, %\"val_1722\"{[264]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             271 |  # node_slice_132\n",
       "                    %\"slice_132\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1678\"{[256]}, %\"val_1733\"{[266]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             272 |  # node_slice_133\n",
       "                    %\"slice_133\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1689\"{[258]}, %\"val_1744\"{[268]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             273 |  # node_slice_134\n",
       "                    %\"slice_134\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1700\"{[260]}, %\"val_1755\"{[270]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             274 |  # node_slice_135\n",
       "                    %\"slice_135\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1711\"{[262]}, %\"val_1766\"{[272]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             275 |  # node_slice_136\n",
       "                    %\"slice_136\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1722\"{[264]}, %\"val_1777\"{[274]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             276 |  # node_slice_137\n",
       "                    %\"slice_137\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1733\"{[266]}, %\"val_1788\"{[276]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             277 |  # node_slice_138\n",
       "                    %\"slice_138\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1744\"{[268]}, %\"val_1799\"{[278]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             278 |  # node_slice_139\n",
       "                    %\"slice_139\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1755\"{[270]}, %\"val_1810\"{[280]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             279 |  # node_slice_140\n",
       "                    %\"slice_140\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1766\"{[272]}, %\"val_1821\"{[282]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             280 |  # node_slice_141\n",
       "                    %\"slice_141\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1777\"{[274]}, %\"val_1832\"{[284]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             281 |  # node_slice_142\n",
       "                    %\"slice_142\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1788\"{[276]}, %\"val_1843\"{[286]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             282 |  # node_slice_143\n",
       "                    %\"slice_143\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1799\"{[278]}, %\"val_1854\"{[288]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             283 |  # node_slice_144\n",
       "                    %\"slice_144\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1810\"{[280]}, %\"val_1865\"{[290]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             284 |  # node_slice_145\n",
       "                    %\"slice_145\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1821\"{[282]}, %\"val_1876\"{[292]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             285 |  # node_slice_146\n",
       "                    %\"slice_146\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1832\"{[284]}, %\"val_1887\"{[294]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             286 |  # node_slice_147\n",
       "                    %\"slice_147\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1843\"{[286]}, %\"val_1898\"{[296]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             287 |  # node_slice_148\n",
       "                    %\"slice_148\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1854\"{[288]}, %\"val_1909\"{[298]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             288 |  # node_slice_149\n",
       "                    %\"slice_149\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1865\"{[290]}, %\"val_1920\"{[300]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             289 |  # node_slice_150\n",
       "                    %\"slice_150\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1876\"{[292]}, %\"val_1931\"{[302]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             290 |  # node_slice_151\n",
       "                    %\"slice_151\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1887\"{[294]}, %\"val_1942\"{[304]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             291 |  # node_slice_152\n",
       "                    %\"slice_152\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1898\"{[296]}, %\"val_1953\"{[306]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             292 |  # node_slice_153\n",
       "                    %\"slice_153\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1909\"{[298]}, %\"val_1964\"{[308]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             293 |  # node_slice_154\n",
       "                    %\"slice_154\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1920\"{[300]}, %\"val_1975\"{[310]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             294 |  # node_slice_155\n",
       "                    %\"slice_155\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1931\"{[302]}, %\"val_1986\"{[312]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             295 |  # node_slice_156\n",
       "                    %\"slice_156\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1942\"{[304]}, %\"val_1997\"{[314]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             296 |  # node_slice_157\n",
       "                    %\"slice_157\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1953\"{[306]}, %\"val_2008\"{[316]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             297 |  # node_slice_158\n",
       "                    %\"slice_158\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1964\"{[308]}, %\"val_2019\"{[318]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             298 |  # node_slice_159\n",
       "                    %\"slice_159\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1975\"{[310]}, %\"val_2030\"{[320]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             299 |  # node_slice_160\n",
       "                    %\"slice_160\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1986\"{[312]}, %\"val_2041\"{[322]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             300 |  # node_slice_161\n",
       "                    %\"slice_161\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1997\"{[314]}, %\"val_2052\"{[324]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             301 |  # node_slice_162\n",
       "                    %\"slice_162\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2008\"{[316]}, %\"val_2063\"{[326]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             302 |  # node_slice_163\n",
       "                    %\"slice_163\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2019\"{[318]}, %\"val_2074\"{[328]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             303 |  # node_slice_164\n",
       "                    %\"slice_164\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2030\"{[320]}, %\"val_2085\"{[330]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             304 |  # node_slice_165\n",
       "                    %\"slice_165\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2041\"{[322]}, %\"val_285\"{[332]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             305 |  # node_slice_166\n",
       "                    %\"slice_166\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2052\"{[324]}, %\"val_2106\"{[334]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             306 |  # node_slice_167\n",
       "                    %\"slice_167\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2063\"{[326]}, %\"val_2117\"{[336]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             307 |  # node_slice_168\n",
       "                    %\"slice_168\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2074\"{[328]}, %\"val_2128\"{[338]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             308 |  # node_slice_169\n",
       "                    %\"slice_169\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2085\"{[330]}, %\"val_2139\"{[340]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             309 |  # node_cat_2\n",
       "                    %\"cat_2\"<FLOAT,[166,10,5,384]>  ::Concat(%\"slice_4\", %\"slice_5\", %\"slice_6\", %\"slice_7\", %\"slice_8\", %\"slice_9\", %\"slice_10\", %\"slice_11\", %\"slice_12\", %\"slice_13\", %\"slice_14\", %\"slice_15\", %\"slice_16\", %\"slice_17\", %\"slice_18\", %\"slice_19\", %\"slice_20\", %\"slice_21\", %\"slice_22\", %\"slice_23\", %\"slice_24\", %\"slice_25\", %\"slice_26\", %\"slice_27\", %\"slice_28\", %\"slice_29\", %\"slice_30\", %\"slice_31\", %\"slice_32\", %\"slice_33\", %\"slice_34\", %\"slice_35\", %\"slice_36\", %\"slice_37\", %\"slice_38\", %\"slice_39\", %\"slice_40\", %\"slice_41\", %\"slice_42\", %\"slice_43\", %\"slice_44\", %\"slice_45\", %\"slice_46\", %\"slice_47\", %\"slice_48\", %\"slice_49\", %\"slice_50\", %\"slice_51\", %\"slice_52\", %\"slice_53\", %\"slice_54\", %\"slice_55\", %\"slice_56\", %\"slice_57\", %\"slice_58\", %\"slice_59\", %\"slice_60\", %\"slice_61\", %\"slice_62\", %\"slice_63\", %\"slice_64\", %\"slice_65\", %\"slice_66\", %\"slice_67\", %\"slice_68\", %\"slice_69\", %\"slice_70\", %\"slice_71\", %\"slice_72\", %\"slice_73\", %\"slice_74\", %\"slice_75\", %\"slice_76\", %\"slice_77\", %\"slice_78\", %\"slice_79\", %\"slice_80\", %\"slice_81\", %\"slice_82\", %\"slice_83\", %\"slice_84\", %\"slice_85\", %\"slice_86\", %\"slice_87\", %\"slice_88\", %\"slice_89\", %\"slice_90\", %\"slice_91\", %\"slice_92\", %\"slice_93\", %\"slice_94\", %\"slice_95\", %\"slice_96\", %\"slice_97\", %\"slice_98\", %\"slice_99\", %\"slice_100\", %\"slice_101\", %\"slice_102\", %\"slice_103\", %\"slice_104\", %\"slice_105\", %\"slice_106\", %\"slice_107\", %\"slice_108\", %\"slice_109\", %\"slice_110\", %\"slice_111\", %\"slice_112\", %\"slice_113\", %\"slice_114\", %\"slice_115\", %\"slice_116\", %\"slice_117\", %\"slice_118\", %\"slice_119\", %\"slice_120\", %\"slice_121\", %\"slice_122\", %\"slice_123\", %\"slice_124\", %\"slice_125\", %\"slice_126\", %\"slice_127\", %\"slice_128\", %\"slice_129\", %\"slice_130\", %\"slice_131\", %\"slice_132\", %\"slice_133\", %\"slice_134\", %\"slice_135\", %\"slice_136\", %\"slice_137\", %\"slice_138\", %\"slice_139\", %\"slice_140\", %\"slice_141\", %\"slice_142\", %\"slice_143\", %\"slice_144\", %\"slice_145\", %\"slice_146\", %\"slice_147\", %\"slice_148\", %\"slice_149\", %\"slice_150\", %\"slice_151\", %\"slice_152\", %\"slice_153\", %\"slice_154\", %\"slice_155\", %\"slice_156\", %\"slice_157\", %\"slice_158\", %\"slice_159\", %\"slice_160\", %\"slice_161\", %\"slice_162\", %\"slice_163\", %\"slice_164\", %\"slice_165\", %\"slice_166\", %\"slice_167\", %\"slice_168\", %\"slice_169\") {axis=0}\n",
       "             310 |  # node_view_16\n",
       "                    %\"view_16\"<FLOAT,[166,50,384]>  ::Reshape(%\"cat_2\", %\"val_2148\"{[166, 50, 384]}) {allowzero=1}\n",
       "             311 |  # node_slice_170\n",
       "                    %\"slice_170\"<FLOAT,[8,50,384]>  ::Slice(%\"view_16\", %\"val_281\"{[0]}, %\"val_365\"{[8]}, %\"val_281\"{[0]}, %\"val_289\"{[1]})\n",
       "             312 |  # node_slice_171\n",
       "                    %\"slice_171\"<FLOAT,[1,50,384]>  ::Slice(%\"pe.pe\"{...}, %\"val_281\"{[0]}, %\"val_545\"{[50]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             313 |  # node_add_9\n",
       "                    %\"add_9\"<FLOAT,[8,50,384]>  ::Add(%\"slice_170\", %\"slice_171\")\n",
       "             314 |  # node_linear_24\n",
       "                    %\"linear_24\"<FLOAT,[8,1280]>  ::Gemm(%\"cat_4\"{...}, %\"unet.time_embedding.linear_1.weight\"{...}, %\"unet.time_embedding.linear_1.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             315 |  # node_Sigmoid_2181\n",
       "                    %\"val_2199\"<FLOAT,[8,1280]>  ::Sigmoid(%\"linear_24\")\n",
       "             316 |  # node_silu\n",
       "                    %\"silu\"<FLOAT,[8,1280]>  ::Mul(%\"linear_24\", %\"val_2199\")\n",
       "             317 |  # node_linear_25\n",
       "                    %\"linear_25\"<FLOAT,[8,1280]>  ::Gemm(%\"silu\", %\"unet.time_embedding.linear_2.weight\"{...}, %\"unet.time_embedding.linear_2.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             318 |  # node_conv2d\n",
       "                    %\"conv2d\"<FLOAT,[8,320,32,32]>  ::Conv(%\"latent_inputs\", %\"unet.conv_in.weight\"{...}, %\"unet.conv_in.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             319 |  # node_Reshape_2186\n",
       "                    %\"val_2204\"<FLOAT,[8,32,10240]>  ::Reshape(%\"conv2d\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             320 |  # node_InstanceNormalization_2193\n",
       "                    %\"val_2211\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2204\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             321 |  # node_Reshape_2195\n",
       "                    %\"val_2213\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2211\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             322 |  # node_Mul_2202\n",
       "                    %\"val_2220\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2213\", %\"val_2219\"{...})\n",
       "             323 |  # node_group_norm\n",
       "                    %\"group_norm\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2220\", %\"val_2221\"{...})\n",
       "             324 |  # node_Sigmoid_2204\n",
       "                    %\"val_2222\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm\")\n",
       "             325 |  # node_silu_1\n",
       "                    %\"silu_1\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm\", %\"val_2222\")\n",
       "             326 |  # node_conv2d_1\n",
       "                    %\"conv2d_1\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_1\", %\"unet.down_blocks.0.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             327 |  # node_Sigmoid_2205\n",
       "                    %\"val_2223\"<FLOAT,[8,1280]>  ::Sigmoid(%\"linear_25\")\n",
       "             328 |  # node_silu_2\n",
       "                    %\"silu_2\"<FLOAT,[8,1280]>  ::Mul(%\"linear_25\", %\"val_2223\")\n",
       "             329 |  # node_linear_26\n",
       "                    %\"linear_26\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.0.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.0.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             330 |  # node_Unsqueeze_7717\n",
       "                    %\"unsqueeze_3\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_26\", %\"val_6521\"{[2, 3]})\n",
       "             331 |  # node_add_10\n",
       "                    %\"add_10\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_1\", %\"unsqueeze_3\")\n",
       "             332 |  # node_Reshape_2212\n",
       "                    %\"val_2230\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_10\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             333 |  # node_InstanceNormalization_2219\n",
       "                    %\"val_2237\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2230\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             334 |  # node_Reshape_2221\n",
       "                    %\"val_2239\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2237\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             335 |  # node_Mul_2228\n",
       "                    %\"val_2246\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2239\", %\"val_2245\"{...})\n",
       "             336 |  # node_group_norm_1\n",
       "                    %\"group_norm_1\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2246\", %\"val_2247\"{...})\n",
       "             337 |  # node_Sigmoid_2230\n",
       "                    %\"val_2248\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_1\")\n",
       "             338 |  # node_silu_3\n",
       "                    %\"silu_3\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_1\", %\"val_2248\")\n",
       "             339 |  # node_conv2d_2\n",
       "                    %\"conv2d_2\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_3\", %\"unet.down_blocks.0.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             340 |  # node_add_11\n",
       "                    %\"add_11\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d\", %\"conv2d_2\")\n",
       "             341 |  # node_Reshape_2236\n",
       "                    %\"val_2254\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_11\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             342 |  # node_InstanceNormalization_2243\n",
       "                    %\"val_2261\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2254\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             343 |  # node_Reshape_2245\n",
       "                    %\"val_2263\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2261\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             344 |  # node_Mul_2252\n",
       "                    %\"val_2270\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2263\", %\"val_2269\"{...})\n",
       "             345 |  # node_group_norm_2\n",
       "                    %\"group_norm_2\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2270\", %\"val_2271\"{...})\n",
       "             346 |  # node_conv2d_3\n",
       "                    %\"conv2d_3\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_2\", %\"unet.down_blocks.0.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.0.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             347 |  # node_permute_1\n",
       "                    %\"permute_1\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_3\") {perm=(0, 2, 3, 1)}\n",
       "             348 |  # node_view_17\n",
       "                    %\"view_17\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_1\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "             349 |  # node_layer_norm_9\n",
       "                    %\"layer_norm_9\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_17\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             350 |  # node_linear_27\n",
       "                    %\"linear_27\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2279\"{...})\n",
       "             351 |  # node_linear_28\n",
       "                    %\"linear_28\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2280\"{...})\n",
       "             352 |  # node_linear_29\n",
       "                    %\"linear_29\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2281\"{...})\n",
       "             353 |  # node_view_18\n",
       "                    %\"view_18\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_27\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             354 |  # node_transpose_16\n",
       "                    %\"transpose_16\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
       "             355 |  # node_view_19\n",
       "                    %\"view_19\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_28\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             356 |  # node_transpose_17\n",
       "                    %\"transpose_17\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_19\") {perm=(0, 2, 1, 3)}\n",
       "             357 |  # node_view_20\n",
       "                    %\"view_20\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_29\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             358 |  # node_transpose_18\n",
       "                    %\"transpose_18\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_20\") {perm=(0, 2, 1, 3)}\n",
       "             359 |  # node_Reshape_2297\n",
       "                    %\"val_2317\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_17\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "             360 |  # node_Transpose_2298\n",
       "                    %\"val_2318\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_2317\") {perm=(0, 2, 1)}\n",
       "             361 |  # node_Reshape_2300\n",
       "                    %\"val_2320\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_2318\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "             362 |  # node_Mul_2302\n",
       "                    %\"val_2322\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_16\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             363 |  # node_Mul_2305\n",
       "                    %\"val_2325\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_2320\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             364 |  # node_MatMul_2306\n",
       "                    %\"val_2326\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_2322\", %\"val_2325\")\n",
       "             365 |  # node_Softmax_2307\n",
       "                    %\"val_2327\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_2326\") {axis=-1}\n",
       "             366 |  # node_scaled_dot_product_attention_4\n",
       "                    %\"scaled_dot_product_attention_4\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2327\", %\"transpose_18\")\n",
       "             367 |  # node_transpose_19\n",
       "                    %\"transpose_19\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(0, 2, 1, 3)}\n",
       "             368 |  # node_view_21\n",
       "                    %\"view_21\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_19\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             369 |  # node_MatMul_2314\n",
       "                    %\"val_2334\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_21\", %\"val_2333\"{...})\n",
       "             370 |  # node_linear_30\n",
       "                    %\"linear_30\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2334\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             371 |  # node_add_12\n",
       "                    %\"add_12\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_30\", %\"view_17\")\n",
       "             372 |  # node_layer_norm_10\n",
       "                    %\"layer_norm_10\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_12\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             373 |  # node_linear_31\n",
       "                    %\"linear_31\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_10\", %\"val_2337\"{...})\n",
       "             374 |  # node_linear_32\n",
       "                    %\"linear_32\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2338\"{...})\n",
       "             375 |  # node_linear_33\n",
       "                    %\"linear_33\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2339\"{...})\n",
       "             376 |  # node_view_22\n",
       "                    %\"view_22\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_31\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             377 |  # node_transpose_20\n",
       "                    %\"transpose_20\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
       "             378 |  # node_view_23\n",
       "                    %\"view_23\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_32\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             379 |  # node_transpose_21\n",
       "                    %\"transpose_21\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_23\") {perm=(0, 2, 1, 3)}\n",
       "             380 |  # node_view_24\n",
       "                    %\"view_24\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_33\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             381 |  # node_transpose_22\n",
       "                    %\"transpose_22\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_24\") {perm=(0, 2, 1, 3)}\n",
       "             382 |  # node_Reshape_2353\n",
       "                    %\"val_2375\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_21\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "             383 |  # node_Transpose_2354\n",
       "                    %\"val_2376\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_2375\") {perm=(0, 2, 1)}\n",
       "             384 |  # node_Reshape_2356\n",
       "                    %\"val_2378\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_2376\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "             385 |  # node_Mul_2358\n",
       "                    %\"val_2380\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_20\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             386 |  # node_Mul_2361\n",
       "                    %\"val_2383\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_2378\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             387 |  # node_MatMul_2362\n",
       "                    %\"val_2384\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_2380\", %\"val_2383\")\n",
       "             388 |  # node_Softmax_2363\n",
       "                    %\"val_2385\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_2384\") {axis=-1}\n",
       "             389 |  # node_scaled_dot_product_attention_5\n",
       "                    %\"scaled_dot_product_attention_5\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2385\", %\"transpose_22\")\n",
       "             390 |  # node_transpose_23\n",
       "                    %\"transpose_23\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(0, 2, 1, 3)}\n",
       "             391 |  # node_view_25\n",
       "                    %\"view_25\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_23\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             392 |  # node_MatMul_2370\n",
       "                    %\"val_2392\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_25\", %\"val_2391\"{...})\n",
       "             393 |  # node_linear_34\n",
       "                    %\"linear_34\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2392\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             394 |  # node_add_13\n",
       "                    %\"add_13\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_34\", %\"add_12\")\n",
       "             395 |  # node_layer_norm_11\n",
       "                    %\"layer_norm_11\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_13\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             396 |  # node_MatMul_2372\n",
       "                    %\"val_2396\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_11\", %\"val_2395\"{...})\n",
       "             397 |  # node_linear_35\n",
       "                    %\"linear_35\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_2396\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             398 |  # node_Split_6991\n",
       "                    %\"split_split_0\"<FLOAT,[8,1024,1280]>, %\"split_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_35\") {axis=2, num_outputs=2}\n",
       "             399 |  # node_gelu_6\n",
       "                    %\"gelu_6\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_split_1\") {approximate='none'}\n",
       "             400 |  # node_mul_3\n",
       "                    %\"mul_3\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_split_0\", %\"gelu_6\")\n",
       "             401 |  # node_MatMul_2375\n",
       "                    %\"val_2399\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_3\", %\"val_2398\"{...})\n",
       "             402 |  # node_linear_36\n",
       "                    %\"linear_36\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2399\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             403 |  # node_add_14\n",
       "                    %\"add_14\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_36\", %\"add_13\")\n",
       "             404 |  # node_view_26\n",
       "                    %\"view_26\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_14\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "             405 |  # node_permute_2\n",
       "                    %\"permute_2\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_26\") {perm=(0, 3, 1, 2)}\n",
       "             406 |  # node_conv2d_4\n",
       "                    %\"conv2d_4\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_2\", %\"unet.down_blocks.0.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.0.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             407 |  # node_add_15\n",
       "                    %\"add_15\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_4\", %\"add_11\")\n",
       "             408 |  # node_Reshape_2386\n",
       "                    %\"val_2410\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_15\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             409 |  # node_InstanceNormalization_2393\n",
       "                    %\"val_2417\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2410\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             410 |  # node_Reshape_2395\n",
       "                    %\"val_2419\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2417\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             411 |  # node_Mul_2402\n",
       "                    %\"val_2426\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2419\", %\"val_2425\"{...})\n",
       "             412 |  # node_group_norm_3\n",
       "                    %\"group_norm_3\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2426\", %\"val_2427\"{...})\n",
       "             413 |  # node_Sigmoid_2404\n",
       "                    %\"val_2428\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_3\")\n",
       "             414 |  # node_silu_4\n",
       "                    %\"silu_4\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_3\", %\"val_2428\")\n",
       "             415 |  # node_conv2d_5\n",
       "                    %\"conv2d_5\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_4\", %\"unet.down_blocks.0.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             416 |  # node_linear_37\n",
       "                    %\"linear_37\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.0.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.0.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             417 |  # node_Unsqueeze_7722\n",
       "                    %\"unsqueeze_5\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_37\", %\"val_6521\"{[2, 3]})\n",
       "             418 |  # node_add_16\n",
       "                    %\"add_16\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_5\", %\"unsqueeze_5\")\n",
       "             419 |  # node_Reshape_2410\n",
       "                    %\"val_2434\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_16\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             420 |  # node_InstanceNormalization_2417\n",
       "                    %\"val_2441\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2434\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             421 |  # node_Reshape_2419\n",
       "                    %\"val_2443\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2441\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             422 |  # node_Mul_2426\n",
       "                    %\"val_2450\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2443\", %\"val_2449\"{...})\n",
       "             423 |  # node_group_norm_4\n",
       "                    %\"group_norm_4\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2450\", %\"val_2451\"{...})\n",
       "             424 |  # node_Sigmoid_2428\n",
       "                    %\"val_2452\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_4\")\n",
       "             425 |  # node_silu_6\n",
       "                    %\"silu_6\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_4\", %\"val_2452\")\n",
       "             426 |  # node_conv2d_6\n",
       "                    %\"conv2d_6\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_6\", %\"unet.down_blocks.0.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             427 |  # node_add_17\n",
       "                    %\"add_17\"<FLOAT,[8,320,32,32]>  ::Add(%\"add_15\", %\"conv2d_6\")\n",
       "             428 |  # node_Reshape_2433\n",
       "                    %\"val_2457\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_17\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             429 |  # node_InstanceNormalization_2440\n",
       "                    %\"val_2464\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2457\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             430 |  # node_Reshape_2442\n",
       "                    %\"val_2466\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2464\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             431 |  # node_Mul_2449\n",
       "                    %\"val_2473\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2466\", %\"val_2472\"{...})\n",
       "             432 |  # node_group_norm_5\n",
       "                    %\"group_norm_5\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2473\", %\"val_2474\"{...})\n",
       "             433 |  # node_conv2d_7\n",
       "                    %\"conv2d_7\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_5\", %\"unet.down_blocks.0.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.0.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             434 |  # node_permute_3\n",
       "                    %\"permute_3\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_7\") {perm=(0, 2, 3, 1)}\n",
       "             435 |  # node_view_27\n",
       "                    %\"view_27\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_3\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "             436 |  # node_layer_norm_12\n",
       "                    %\"layer_norm_12\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_27\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             437 |  # node_linear_38\n",
       "                    %\"linear_38\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2482\"{...})\n",
       "             438 |  # node_linear_39\n",
       "                    %\"linear_39\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2483\"{...})\n",
       "             439 |  # node_linear_40\n",
       "                    %\"linear_40\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2484\"{...})\n",
       "             440 |  # node_view_28\n",
       "                    %\"view_28\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_38\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             441 |  # node_transpose_24\n",
       "                    %\"transpose_24\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_28\") {perm=(0, 2, 1, 3)}\n",
       "             442 |  # node_view_29\n",
       "                    %\"view_29\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_39\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             443 |  # node_transpose_25\n",
       "                    %\"transpose_25\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_29\") {perm=(0, 2, 1, 3)}\n",
       "             444 |  # node_view_30\n",
       "                    %\"view_30\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_40\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             445 |  # node_transpose_26\n",
       "                    %\"transpose_26\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_30\") {perm=(0, 2, 1, 3)}\n",
       "             446 |  # node_Reshape_2494\n",
       "                    %\"val_2520\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_25\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "             447 |  # node_Transpose_2495\n",
       "                    %\"val_2521\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_2520\") {perm=(0, 2, 1)}\n",
       "             448 |  # node_Reshape_2497\n",
       "                    %\"val_2523\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_2521\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "             449 |  # node_Mul_2499\n",
       "                    %\"val_2525\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_24\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             450 |  # node_Mul_2502\n",
       "                    %\"val_2528\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_2523\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             451 |  # node_MatMul_2503\n",
       "                    %\"val_2529\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_2525\", %\"val_2528\")\n",
       "             452 |  # node_Softmax_2504\n",
       "                    %\"val_2530\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_2529\") {axis=-1}\n",
       "             453 |  # node_scaled_dot_product_attention_6\n",
       "                    %\"scaled_dot_product_attention_6\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2530\", %\"transpose_26\")\n",
       "             454 |  # node_transpose_27\n",
       "                    %\"transpose_27\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_6\") {perm=(0, 2, 1, 3)}\n",
       "             455 |  # node_view_31\n",
       "                    %\"view_31\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_27\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             456 |  # node_MatMul_2511\n",
       "                    %\"val_2537\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_31\", %\"val_2536\"{...})\n",
       "             457 |  # node_linear_41\n",
       "                    %\"linear_41\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2537\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             458 |  # node_add_18\n",
       "                    %\"add_18\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_41\", %\"view_27\")\n",
       "             459 |  # node_layer_norm_13\n",
       "                    %\"layer_norm_13\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_18\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             460 |  # node_linear_42\n",
       "                    %\"linear_42\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_13\", %\"val_2540\"{...})\n",
       "             461 |  # node_linear_43\n",
       "                    %\"linear_43\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2541\"{...})\n",
       "             462 |  # node_linear_44\n",
       "                    %\"linear_44\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2542\"{...})\n",
       "             463 |  # node_view_32\n",
       "                    %\"view_32\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_42\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             464 |  # node_transpose_28\n",
       "                    %\"transpose_28\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_32\") {perm=(0, 2, 1, 3)}\n",
       "             465 |  # node_view_33\n",
       "                    %\"view_33\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_43\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             466 |  # node_transpose_29\n",
       "                    %\"transpose_29\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_33\") {perm=(0, 2, 1, 3)}\n",
       "             467 |  # node_view_34\n",
       "                    %\"view_34\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_44\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             468 |  # node_transpose_30\n",
       "                    %\"transpose_30\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_34\") {perm=(0, 2, 1, 3)}\n",
       "             469 |  # node_Reshape_2550\n",
       "                    %\"val_2578\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_29\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "             470 |  # node_Transpose_2551\n",
       "                    %\"val_2579\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_2578\") {perm=(0, 2, 1)}\n",
       "             471 |  # node_Reshape_2553\n",
       "                    %\"val_2581\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_2579\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "             472 |  # node_Mul_2555\n",
       "                    %\"val_2583\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_28\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             473 |  # node_Mul_2558\n",
       "                    %\"val_2586\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_2581\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             474 |  # node_MatMul_2559\n",
       "                    %\"val_2587\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_2583\", %\"val_2586\")\n",
       "             475 |  # node_Softmax_2560\n",
       "                    %\"val_2588\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_2587\") {axis=-1}\n",
       "             476 |  # node_scaled_dot_product_attention_7\n",
       "                    %\"scaled_dot_product_attention_7\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2588\", %\"transpose_30\")\n",
       "             477 |  # node_transpose_31\n",
       "                    %\"transpose_31\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_7\") {perm=(0, 2, 1, 3)}\n",
       "             478 |  # node_view_35\n",
       "                    %\"view_35\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_31\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             479 |  # node_MatMul_2567\n",
       "                    %\"val_2595\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_35\", %\"val_2594\"{...})\n",
       "             480 |  # node_linear_45\n",
       "                    %\"linear_45\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2595\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             481 |  # node_add_19\n",
       "                    %\"add_19\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_45\", %\"add_18\")\n",
       "             482 |  # node_layer_norm_14\n",
       "                    %\"layer_norm_14\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_19\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             483 |  # node_MatMul_2569\n",
       "                    %\"val_2599\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_14\", %\"val_2598\"{...})\n",
       "             484 |  # node_linear_46\n",
       "                    %\"linear_46\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_2599\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             485 |  # node_Split_7024\n",
       "                    %\"split_1_split_0\"<FLOAT,[8,1024,1280]>, %\"split_1_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_46\") {axis=2, num_outputs=2}\n",
       "             486 |  # node_gelu_7\n",
       "                    %\"gelu_7\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_1_split_1\") {approximate='none'}\n",
       "             487 |  # node_mul_4\n",
       "                    %\"mul_4\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_1_split_0\", %\"gelu_7\")\n",
       "             488 |  # node_MatMul_2571\n",
       "                    %\"val_2601\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_4\", %\"val_2600\"{...})\n",
       "             489 |  # node_linear_47\n",
       "                    %\"linear_47\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2601\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             490 |  # node_add_20\n",
       "                    %\"add_20\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_47\", %\"add_19\")\n",
       "             491 |  # node_view_36\n",
       "                    %\"view_36\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_20\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "             492 |  # node_permute_4\n",
       "                    %\"permute_4\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_36\") {perm=(0, 3, 1, 2)}\n",
       "             493 |  # node_conv2d_8\n",
       "                    %\"conv2d_8\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_4\", %\"unet.down_blocks.0.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.0.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             494 |  # node_add_21\n",
       "                    %\"add_21\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_8\", %\"add_17\")\n",
       "             495 |  # node_conv2d_9\n",
       "                    %\"conv2d_9\"<FLOAT,[8,320,16,16]>  ::Conv(%\"add_21\", %\"unet.down_blocks.0.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.0.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             496 |  # node_Reshape_2582\n",
       "                    %\"val_2612\"<FLOAT,[8,32,2560]>  ::Reshape(%\"conv2d_9\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             497 |  # node_InstanceNormalization_2589\n",
       "                    %\"val_2619\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_2612\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             498 |  # node_Reshape_2591\n",
       "                    %\"val_2621\"<FLOAT,[8,320,16,16]>  ::Reshape(%\"val_2619\", %\"val_2620\"{[8, 320, 16, 16]}) {allowzero=0}\n",
       "             499 |  # node_Mul_2598\n",
       "                    %\"val_2628\"<FLOAT,[8,320,16,16]>  ::Mul(%\"val_2621\", %\"val_2627\"{...})\n",
       "             500 |  # node_group_norm_6\n",
       "                    %\"group_norm_6\"<FLOAT,[8,320,16,16]>  ::Add(%\"val_2628\", %\"val_2629\"{...})\n",
       "             501 |  # node_Sigmoid_2600\n",
       "                    %\"val_2630\"<FLOAT,[8,320,16,16]>  ::Sigmoid(%\"group_norm_6\")\n",
       "             502 |  # node_silu_7\n",
       "                    %\"silu_7\"<FLOAT,[8,320,16,16]>  ::Mul(%\"group_norm_6\", %\"val_2630\")\n",
       "             503 |  # node_conv2d_10\n",
       "                    %\"conv2d_10\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_7\", %\"unet.down_blocks.1.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             504 |  # node_linear_48\n",
       "                    %\"linear_48\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.1.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.1.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             505 |  # node_Unsqueeze_7727\n",
       "                    %\"unsqueeze_7\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_48\", %\"val_6521\"{[2, 3]})\n",
       "             506 |  # node_add_22\n",
       "                    %\"add_22\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_10\", %\"unsqueeze_7\")\n",
       "             507 |  # node_Reshape_2606\n",
       "                    %\"val_2636\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_22\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             508 |  # node_InstanceNormalization_2613\n",
       "                    %\"val_2643\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2636\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             509 |  # node_Reshape_2615\n",
       "                    %\"val_2645\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2643\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             510 |  # node_Mul_2622\n",
       "                    %\"val_2652\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2645\", %\"val_2651\"{...})\n",
       "             511 |  # node_group_norm_7\n",
       "                    %\"group_norm_7\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2652\", %\"val_2653\"{...})\n",
       "             512 |  # node_Sigmoid_2624\n",
       "                    %\"val_2654\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_7\")\n",
       "             513 |  # node_silu_9\n",
       "                    %\"silu_9\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_7\", %\"val_2654\")\n",
       "             514 |  # node_conv2d_11\n",
       "                    %\"conv2d_11\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_9\", %\"unet.down_blocks.1.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             515 |  # node_conv2d_12\n",
       "                    %\"conv2d_12\"<FLOAT,[8,640,16,16]>  ::Conv(%\"conv2d_9\", %\"unet.down_blocks.1.resnets.0.conv_shortcut.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             516 |  # node_add_23\n",
       "                    %\"add_23\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_12\", %\"conv2d_11\")\n",
       "             517 |  # node_Reshape_2629\n",
       "                    %\"val_2659\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_23\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             518 |  # node_InstanceNormalization_2636\n",
       "                    %\"val_2666\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2659\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             519 |  # node_Reshape_2638\n",
       "                    %\"val_2668\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2666\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             520 |  # node_Mul_2645\n",
       "                    %\"val_2675\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2668\", %\"val_2674\"{...})\n",
       "             521 |  # node_group_norm_8\n",
       "                    %\"group_norm_8\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2675\", %\"val_2676\"{...})\n",
       "             522 |  # node_conv2d_13\n",
       "                    %\"conv2d_13\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_8\", %\"unet.down_blocks.1.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.1.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             523 |  # node_permute_5\n",
       "                    %\"permute_5\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_13\") {perm=(0, 2, 3, 1)}\n",
       "             524 |  # node_view_37\n",
       "                    %\"view_37\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_5\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "             525 |  # node_layer_norm_15\n",
       "                    %\"layer_norm_15\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_37\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             526 |  # node_linear_49\n",
       "                    %\"linear_49\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2684\"{...})\n",
       "             527 |  # node_linear_50\n",
       "                    %\"linear_50\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2685\"{...})\n",
       "             528 |  # node_linear_51\n",
       "                    %\"linear_51\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2686\"{...})\n",
       "             529 |  # node_view_38\n",
       "                    %\"view_38\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_49\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             530 |  # node_transpose_32\n",
       "                    %\"transpose_32\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_38\") {perm=(0, 2, 1, 3)}\n",
       "             531 |  # node_view_39\n",
       "                    %\"view_39\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_50\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             532 |  # node_transpose_33\n",
       "                    %\"transpose_33\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_39\") {perm=(0, 2, 1, 3)}\n",
       "             533 |  # node_view_40\n",
       "                    %\"view_40\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_51\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             534 |  # node_transpose_34\n",
       "                    %\"transpose_34\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_40\") {perm=(0, 2, 1, 3)}\n",
       "             535 |  # node_Reshape_2690\n",
       "                    %\"val_2722\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_33\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "             536 |  # node_Transpose_2691\n",
       "                    %\"val_2723\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_2722\") {perm=(0, 2, 1)}\n",
       "             537 |  # node_Reshape_2693\n",
       "                    %\"val_2725\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_2723\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "             538 |  # node_Mul_2695\n",
       "                    %\"val_2727\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_32\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             539 |  # node_Mul_2698\n",
       "                    %\"val_2730\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_2725\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             540 |  # node_MatMul_2699\n",
       "                    %\"val_2731\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_2727\", %\"val_2730\")\n",
       "             541 |  # node_Softmax_2700\n",
       "                    %\"val_2732\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_2731\") {axis=-1}\n",
       "             542 |  # node_scaled_dot_product_attention_8\n",
       "                    %\"scaled_dot_product_attention_8\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2732\", %\"transpose_34\")\n",
       "             543 |  # node_transpose_35\n",
       "                    %\"transpose_35\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_8\") {perm=(0, 2, 1, 3)}\n",
       "             544 |  # node_view_41\n",
       "                    %\"view_41\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_35\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             545 |  # node_MatMul_2707\n",
       "                    %\"val_2739\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_41\", %\"val_2738\"{...})\n",
       "             546 |  # node_linear_52\n",
       "                    %\"linear_52\"<FLOAT,[8,256,640]>  ::Add(%\"val_2739\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             547 |  # node_add_24\n",
       "                    %\"add_24\"<FLOAT,[8,256,640]>  ::Add(%\"linear_52\", %\"view_37\")\n",
       "             548 |  # node_layer_norm_16\n",
       "                    %\"layer_norm_16\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_24\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             549 |  # node_linear_53\n",
       "                    %\"linear_53\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_16\", %\"val_2742\"{...})\n",
       "             550 |  # node_linear_54\n",
       "                    %\"linear_54\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2743\"{...})\n",
       "             551 |  # node_linear_55\n",
       "                    %\"linear_55\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2744\"{...})\n",
       "             552 |  # node_view_42\n",
       "                    %\"view_42\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_53\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             553 |  # node_transpose_36\n",
       "                    %\"transpose_36\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_42\") {perm=(0, 2, 1, 3)}\n",
       "             554 |  # node_view_43\n",
       "                    %\"view_43\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_54\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             555 |  # node_transpose_37\n",
       "                    %\"transpose_37\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_43\") {perm=(0, 2, 1, 3)}\n",
       "             556 |  # node_view_44\n",
       "                    %\"view_44\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_55\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             557 |  # node_transpose_38\n",
       "                    %\"transpose_38\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_44\") {perm=(0, 2, 1, 3)}\n",
       "             558 |  # node_Reshape_2746\n",
       "                    %\"val_2780\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_37\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "             559 |  # node_Transpose_2747\n",
       "                    %\"val_2781\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_2780\") {perm=(0, 2, 1)}\n",
       "             560 |  # node_Reshape_2749\n",
       "                    %\"val_2783\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_2781\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "             561 |  # node_Mul_2751\n",
       "                    %\"val_2785\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_36\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             562 |  # node_Mul_2754\n",
       "                    %\"val_2788\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_2783\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             563 |  # node_MatMul_2755\n",
       "                    %\"val_2789\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_2785\", %\"val_2788\")\n",
       "             564 |  # node_Softmax_2756\n",
       "                    %\"val_2790\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_2789\") {axis=-1}\n",
       "             565 |  # node_scaled_dot_product_attention_9\n",
       "                    %\"scaled_dot_product_attention_9\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2790\", %\"transpose_38\")\n",
       "             566 |  # node_transpose_39\n",
       "                    %\"transpose_39\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_9\") {perm=(0, 2, 1, 3)}\n",
       "             567 |  # node_view_45\n",
       "                    %\"view_45\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_39\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             568 |  # node_MatMul_2763\n",
       "                    %\"val_2797\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_45\", %\"val_2796\"{...})\n",
       "             569 |  # node_linear_56\n",
       "                    %\"linear_56\"<FLOAT,[8,256,640]>  ::Add(%\"val_2797\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             570 |  # node_add_25\n",
       "                    %\"add_25\"<FLOAT,[8,256,640]>  ::Add(%\"linear_56\", %\"add_24\")\n",
       "             571 |  # node_layer_norm_17\n",
       "                    %\"layer_norm_17\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_25\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             572 |  # node_MatMul_2765\n",
       "                    %\"val_2801\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_17\", %\"val_2800\"{...})\n",
       "             573 |  # node_linear_57\n",
       "                    %\"linear_57\"<FLOAT,[8,256,5120]>  ::Add(%\"val_2801\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             574 |  # node_Split_7057\n",
       "                    %\"split_2_split_0\"<FLOAT,[8,256,2560]>, %\"split_2_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_57\") {axis=2, num_outputs=2}\n",
       "             575 |  # node_gelu_8\n",
       "                    %\"gelu_8\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_2_split_1\") {approximate='none'}\n",
       "             576 |  # node_mul_5\n",
       "                    %\"mul_5\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_2_split_0\", %\"gelu_8\")\n",
       "             577 |  # node_MatMul_2768\n",
       "                    %\"val_2804\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_5\", %\"val_2803\"{...})\n",
       "             578 |  # node_linear_58\n",
       "                    %\"linear_58\"<FLOAT,[8,256,640]>  ::Add(%\"val_2804\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             579 |  # node_add_26\n",
       "                    %\"add_26\"<FLOAT,[8,256,640]>  ::Add(%\"linear_58\", %\"add_25\")\n",
       "             580 |  # node_view_46\n",
       "                    %\"view_46\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_26\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "             581 |  # node_permute_6\n",
       "                    %\"permute_6\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_46\") {perm=(0, 3, 1, 2)}\n",
       "             582 |  # node_conv2d_14\n",
       "                    %\"conv2d_14\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_6\", %\"unet.down_blocks.1.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.1.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             583 |  # node_add_27\n",
       "                    %\"add_27\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_14\", %\"add_23\")\n",
       "             584 |  # node_Reshape_2779\n",
       "                    %\"val_2815\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_27\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             585 |  # node_InstanceNormalization_2786\n",
       "                    %\"val_2822\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2815\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             586 |  # node_Reshape_2788\n",
       "                    %\"val_2824\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2822\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             587 |  # node_Mul_2795\n",
       "                    %\"val_2831\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2824\", %\"val_2830\"{...})\n",
       "             588 |  # node_group_norm_9\n",
       "                    %\"group_norm_9\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2831\", %\"val_2832\"{...})\n",
       "             589 |  # node_Sigmoid_2797\n",
       "                    %\"val_2833\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_9\")\n",
       "             590 |  # node_silu_10\n",
       "                    %\"silu_10\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_9\", %\"val_2833\")\n",
       "             591 |  # node_conv2d_15\n",
       "                    %\"conv2d_15\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_10\", %\"unet.down_blocks.1.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             592 |  # node_linear_59\n",
       "                    %\"linear_59\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.1.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.1.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             593 |  # node_Unsqueeze_7732\n",
       "                    %\"unsqueeze_9\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_59\", %\"val_6521\"{[2, 3]})\n",
       "             594 |  # node_add_28\n",
       "                    %\"add_28\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_15\", %\"unsqueeze_9\")\n",
       "             595 |  # node_Reshape_2803\n",
       "                    %\"val_2839\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_28\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             596 |  # node_InstanceNormalization_2810\n",
       "                    %\"val_2846\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2839\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             597 |  # node_Reshape_2812\n",
       "                    %\"val_2848\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2846\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             598 |  # node_Mul_2819\n",
       "                    %\"val_2855\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2848\", %\"val_2854\"{...})\n",
       "             599 |  # node_group_norm_10\n",
       "                    %\"group_norm_10\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2855\", %\"val_2856\"{...})\n",
       "             600 |  # node_Sigmoid_2821\n",
       "                    %\"val_2857\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_10\")\n",
       "             601 |  # node_silu_12\n",
       "                    %\"silu_12\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_10\", %\"val_2857\")\n",
       "             602 |  # node_conv2d_16\n",
       "                    %\"conv2d_16\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_12\", %\"unet.down_blocks.1.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             603 |  # node_add_29\n",
       "                    %\"add_29\"<FLOAT,[8,640,16,16]>  ::Add(%\"add_27\", %\"conv2d_16\")\n",
       "             604 |  # node_Reshape_2826\n",
       "                    %\"val_2862\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_29\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             605 |  # node_InstanceNormalization_2833\n",
       "                    %\"val_2869\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2862\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             606 |  # node_Reshape_2835\n",
       "                    %\"val_2871\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2869\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             607 |  # node_Mul_2842\n",
       "                    %\"val_2878\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2871\", %\"val_2877\"{...})\n",
       "             608 |  # node_group_norm_11\n",
       "                    %\"group_norm_11\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2878\", %\"val_2879\"{...})\n",
       "             609 |  # node_conv2d_17\n",
       "                    %\"conv2d_17\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_11\", %\"unet.down_blocks.1.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.1.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             610 |  # node_permute_7\n",
       "                    %\"permute_7\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_17\") {perm=(0, 2, 3, 1)}\n",
       "             611 |  # node_view_47\n",
       "                    %\"view_47\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_7\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "             612 |  # node_layer_norm_18\n",
       "                    %\"layer_norm_18\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_47\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             613 |  # node_linear_60\n",
       "                    %\"linear_60\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2887\"{...})\n",
       "             614 |  # node_linear_61\n",
       "                    %\"linear_61\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2888\"{...})\n",
       "             615 |  # node_linear_62\n",
       "                    %\"linear_62\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2889\"{...})\n",
       "             616 |  # node_view_48\n",
       "                    %\"view_48\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_60\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             617 |  # node_transpose_40\n",
       "                    %\"transpose_40\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_48\") {perm=(0, 2, 1, 3)}\n",
       "             618 |  # node_view_49\n",
       "                    %\"view_49\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_61\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             619 |  # node_transpose_41\n",
       "                    %\"transpose_41\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_49\") {perm=(0, 2, 1, 3)}\n",
       "             620 |  # node_view_50\n",
       "                    %\"view_50\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_62\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             621 |  # node_transpose_42\n",
       "                    %\"transpose_42\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_50\") {perm=(0, 2, 1, 3)}\n",
       "             622 |  # node_Reshape_2887\n",
       "                    %\"val_2925\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_41\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "             623 |  # node_Transpose_2888\n",
       "                    %\"val_2926\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_2925\") {perm=(0, 2, 1)}\n",
       "             624 |  # node_Reshape_2890\n",
       "                    %\"val_2928\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_2926\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "             625 |  # node_Mul_2892\n",
       "                    %\"val_2930\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_40\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             626 |  # node_Mul_2895\n",
       "                    %\"val_2933\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_2928\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             627 |  # node_MatMul_2896\n",
       "                    %\"val_2934\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_2930\", %\"val_2933\")\n",
       "             628 |  # node_Softmax_2897\n",
       "                    %\"val_2935\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_2934\") {axis=-1}\n",
       "             629 |  # node_scaled_dot_product_attention_10\n",
       "                    %\"scaled_dot_product_attention_10\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2935\", %\"transpose_42\")\n",
       "             630 |  # node_transpose_43\n",
       "                    %\"transpose_43\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_10\") {perm=(0, 2, 1, 3)}\n",
       "             631 |  # node_view_51\n",
       "                    %\"view_51\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_43\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             632 |  # node_MatMul_2904\n",
       "                    %\"val_2942\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_51\", %\"val_2941\"{...})\n",
       "             633 |  # node_linear_63\n",
       "                    %\"linear_63\"<FLOAT,[8,256,640]>  ::Add(%\"val_2942\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             634 |  # node_add_30\n",
       "                    %\"add_30\"<FLOAT,[8,256,640]>  ::Add(%\"linear_63\", %\"view_47\")\n",
       "             635 |  # node_layer_norm_19\n",
       "                    %\"layer_norm_19\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_30\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             636 |  # node_linear_64\n",
       "                    %\"linear_64\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_19\", %\"val_2945\"{...})\n",
       "             637 |  # node_linear_65\n",
       "                    %\"linear_65\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2946\"{...})\n",
       "             638 |  # node_linear_66\n",
       "                    %\"linear_66\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2947\"{...})\n",
       "             639 |  # node_view_52\n",
       "                    %\"view_52\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_64\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             640 |  # node_transpose_44\n",
       "                    %\"transpose_44\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_52\") {perm=(0, 2, 1, 3)}\n",
       "             641 |  # node_view_53\n",
       "                    %\"view_53\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_65\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             642 |  # node_transpose_45\n",
       "                    %\"transpose_45\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_53\") {perm=(0, 2, 1, 3)}\n",
       "             643 |  # node_view_54\n",
       "                    %\"view_54\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_66\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             644 |  # node_transpose_46\n",
       "                    %\"transpose_46\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_54\") {perm=(0, 2, 1, 3)}\n",
       "             645 |  # node_Reshape_2943\n",
       "                    %\"val_2983\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_45\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "             646 |  # node_Transpose_2944\n",
       "                    %\"val_2984\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_2983\") {perm=(0, 2, 1)}\n",
       "             647 |  # node_Reshape_2946\n",
       "                    %\"val_2986\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_2984\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "             648 |  # node_Mul_2948\n",
       "                    %\"val_2988\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_44\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             649 |  # node_Mul_2951\n",
       "                    %\"val_2991\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_2986\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             650 |  # node_MatMul_2952\n",
       "                    %\"val_2992\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_2988\", %\"val_2991\")\n",
       "             651 |  # node_Softmax_2953\n",
       "                    %\"val_2993\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_2992\") {axis=-1}\n",
       "             652 |  # node_scaled_dot_product_attention_11\n",
       "                    %\"scaled_dot_product_attention_11\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2993\", %\"transpose_46\")\n",
       "             653 |  # node_transpose_47\n",
       "                    %\"transpose_47\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_11\") {perm=(0, 2, 1, 3)}\n",
       "             654 |  # node_view_55\n",
       "                    %\"view_55\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_47\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             655 |  # node_MatMul_2960\n",
       "                    %\"val_3000\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_55\", %\"val_2999\"{...})\n",
       "             656 |  # node_linear_67\n",
       "                    %\"linear_67\"<FLOAT,[8,256,640]>  ::Add(%\"val_3000\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             657 |  # node_add_31\n",
       "                    %\"add_31\"<FLOAT,[8,256,640]>  ::Add(%\"linear_67\", %\"add_30\")\n",
       "             658 |  # node_layer_norm_20\n",
       "                    %\"layer_norm_20\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_31\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             659 |  # node_MatMul_2962\n",
       "                    %\"val_3004\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_20\", %\"val_3003\"{...})\n",
       "             660 |  # node_linear_68\n",
       "                    %\"linear_68\"<FLOAT,[8,256,5120]>  ::Add(%\"val_3004\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             661 |  # node_Split_7090\n",
       "                    %\"split_3_split_0\"<FLOAT,[8,256,2560]>, %\"split_3_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_68\") {axis=2, num_outputs=2}\n",
       "             662 |  # node_gelu_9\n",
       "                    %\"gelu_9\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_3_split_1\") {approximate='none'}\n",
       "             663 |  # node_mul_6\n",
       "                    %\"mul_6\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_3_split_0\", %\"gelu_9\")\n",
       "             664 |  # node_MatMul_2964\n",
       "                    %\"val_3006\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_6\", %\"val_3005\"{...})\n",
       "             665 |  # node_linear_69\n",
       "                    %\"linear_69\"<FLOAT,[8,256,640]>  ::Add(%\"val_3006\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             666 |  # node_add_32\n",
       "                    %\"add_32\"<FLOAT,[8,256,640]>  ::Add(%\"linear_69\", %\"add_31\")\n",
       "             667 |  # node_view_56\n",
       "                    %\"view_56\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_32\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "             668 |  # node_permute_8\n",
       "                    %\"permute_8\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_56\") {perm=(0, 3, 1, 2)}\n",
       "             669 |  # node_conv2d_18\n",
       "                    %\"conv2d_18\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_8\", %\"unet.down_blocks.1.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.1.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             670 |  # node_add_33\n",
       "                    %\"add_33\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_18\", %\"add_29\")\n",
       "             671 |  # node_conv2d_19\n",
       "                    %\"conv2d_19\"<FLOAT,[8,640,8,8]>  ::Conv(%\"add_33\", %\"unet.down_blocks.1.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.1.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             672 |  # node_Reshape_2975\n",
       "                    %\"val_3017\"<FLOAT,[8,32,1280]>  ::Reshape(%\"conv2d_19\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             673 |  # node_InstanceNormalization_2982\n",
       "                    %\"val_3024\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3017\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             674 |  # node_Reshape_2984\n",
       "                    %\"val_3026\"<FLOAT,[8,640,8,8]>  ::Reshape(%\"val_3024\", %\"val_3025\"{[8, 640, 8, 8]}) {allowzero=0}\n",
       "             675 |  # node_Mul_2991\n",
       "                    %\"val_3033\"<FLOAT,[8,640,8,8]>  ::Mul(%\"val_3026\", %\"val_3032\"{...})\n",
       "             676 |  # node_group_norm_12\n",
       "                    %\"group_norm_12\"<FLOAT,[8,640,8,8]>  ::Add(%\"val_3033\", %\"val_3034\"{...})\n",
       "             677 |  # node_Sigmoid_2993\n",
       "                    %\"val_3035\"<FLOAT,[8,640,8,8]>  ::Sigmoid(%\"group_norm_12\")\n",
       "             678 |  # node_silu_13\n",
       "                    %\"silu_13\"<FLOAT,[8,640,8,8]>  ::Mul(%\"group_norm_12\", %\"val_3035\")\n",
       "             679 |  # node_conv2d_20\n",
       "                    %\"conv2d_20\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_13\", %\"unet.down_blocks.2.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             680 |  # node_linear_70\n",
       "                    %\"linear_70\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.2.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.2.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             681 |  # node_Unsqueeze_7737\n",
       "                    %\"unsqueeze_11\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_70\", %\"val_6521\"{[2, 3]})\n",
       "             682 |  # node_add_34\n",
       "                    %\"add_34\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_20\", %\"unsqueeze_11\")\n",
       "             683 |  # node_Reshape_2999\n",
       "                    %\"val_3041\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_34\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             684 |  # node_InstanceNormalization_3006\n",
       "                    %\"val_3048\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3041\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             685 |  # node_Reshape_3008\n",
       "                    %\"val_3050\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3048\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             686 |  # node_Mul_3015\n",
       "                    %\"val_3057\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3050\", %\"val_3056\"{...})\n",
       "             687 |  # node_group_norm_13\n",
       "                    %\"group_norm_13\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3057\", %\"val_3058\"{...})\n",
       "             688 |  # node_Sigmoid_3017\n",
       "                    %\"val_3059\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_13\")\n",
       "             689 |  # node_silu_15\n",
       "                    %\"silu_15\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_13\", %\"val_3059\")\n",
       "             690 |  # node_conv2d_21\n",
       "                    %\"conv2d_21\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_15\", %\"unet.down_blocks.2.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             691 |  # node_conv2d_22\n",
       "                    %\"conv2d_22\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"conv2d_19\", %\"unet.down_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             692 |  # node_add_35\n",
       "                    %\"add_35\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_22\", %\"conv2d_21\")\n",
       "             693 |  # node_Reshape_3022\n",
       "                    %\"val_3064\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_35\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             694 |  # node_InstanceNormalization_3029\n",
       "                    %\"val_3071\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3064\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             695 |  # node_Reshape_3031\n",
       "                    %\"val_3073\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3071\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             696 |  # node_Mul_3038\n",
       "                    %\"val_3080\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3073\", %\"val_3079\"{...})\n",
       "             697 |  # node_group_norm_14\n",
       "                    %\"group_norm_14\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3080\", %\"val_3081\"{...})\n",
       "             698 |  # node_conv2d_23\n",
       "                    %\"conv2d_23\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_14\", %\"unet.down_blocks.2.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.2.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             699 |  # node_permute_9\n",
       "                    %\"permute_9\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_23\") {perm=(0, 2, 3, 1)}\n",
       "             700 |  # node_view_57\n",
       "                    %\"view_57\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_9\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "             701 |  # node_layer_norm_21\n",
       "                    %\"layer_norm_21\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_57\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             702 |  # node_linear_71\n",
       "                    %\"linear_71\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3089\"{...})\n",
       "             703 |  # node_linear_72\n",
       "                    %\"linear_72\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3090\"{...})\n",
       "             704 |  # node_linear_73\n",
       "                    %\"linear_73\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3091\"{...})\n",
       "             705 |  # node_view_58\n",
       "                    %\"view_58\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_71\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             706 |  # node_transpose_48\n",
       "                    %\"transpose_48\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_58\") {perm=(0, 2, 1, 3)}\n",
       "             707 |  # node_view_59\n",
       "                    %\"view_59\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_72\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             708 |  # node_transpose_49\n",
       "                    %\"transpose_49\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_59\") {perm=(0, 2, 1, 3)}\n",
       "             709 |  # node_view_60\n",
       "                    %\"view_60\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_73\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             710 |  # node_transpose_50\n",
       "                    %\"transpose_50\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_60\") {perm=(0, 2, 1, 3)}\n",
       "             711 |  # node_Reshape_3083\n",
       "                    %\"val_3127\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_49\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "             712 |  # node_Transpose_3084\n",
       "                    %\"val_3128\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_3127\") {perm=(0, 2, 1)}\n",
       "             713 |  # node_Reshape_3086\n",
       "                    %\"val_3130\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_3128\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "             714 |  # node_Mul_3088\n",
       "                    %\"val_3132\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_48\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             715 |  # node_Mul_3091\n",
       "                    %\"val_3135\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_3130\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             716 |  # node_MatMul_3092\n",
       "                    %\"val_3136\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_3132\", %\"val_3135\")\n",
       "             717 |  # node_Softmax_3093\n",
       "                    %\"val_3137\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_3136\") {axis=-1}\n",
       "             718 |  # node_scaled_dot_product_attention_12\n",
       "                    %\"scaled_dot_product_attention_12\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3137\", %\"transpose_50\")\n",
       "             719 |  # node_transpose_51\n",
       "                    %\"transpose_51\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_12\") {perm=(0, 2, 1, 3)}\n",
       "             720 |  # node_view_61\n",
       "                    %\"view_61\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_51\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             721 |  # node_MatMul_3100\n",
       "                    %\"val_3144\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_61\", %\"val_3143\"{...})\n",
       "             722 |  # node_linear_74\n",
       "                    %\"linear_74\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3144\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             723 |  # node_add_36\n",
       "                    %\"add_36\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_74\", %\"view_57\")\n",
       "             724 |  # node_layer_norm_22\n",
       "                    %\"layer_norm_22\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_36\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             725 |  # node_linear_75\n",
       "                    %\"linear_75\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_22\", %\"val_3147\"{...})\n",
       "             726 |  # node_linear_76\n",
       "                    %\"linear_76\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3148\"{...})\n",
       "             727 |  # node_linear_77\n",
       "                    %\"linear_77\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3149\"{...})\n",
       "             728 |  # node_view_62\n",
       "                    %\"view_62\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_75\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             729 |  # node_transpose_52\n",
       "                    %\"transpose_52\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_62\") {perm=(0, 2, 1, 3)}\n",
       "             730 |  # node_view_63\n",
       "                    %\"view_63\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_76\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             731 |  # node_transpose_53\n",
       "                    %\"transpose_53\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_63\") {perm=(0, 2, 1, 3)}\n",
       "             732 |  # node_view_64\n",
       "                    %\"view_64\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_77\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             733 |  # node_transpose_54\n",
       "                    %\"transpose_54\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_64\") {perm=(0, 2, 1, 3)}\n",
       "             734 |  # node_Reshape_3139\n",
       "                    %\"val_3185\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_53\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             735 |  # node_Transpose_3140\n",
       "                    %\"val_3186\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3185\") {perm=(0, 2, 1)}\n",
       "             736 |  # node_Reshape_3142\n",
       "                    %\"val_3188\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3186\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             737 |  # node_Mul_3144\n",
       "                    %\"val_3190\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_52\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             738 |  # node_Mul_3147\n",
       "                    %\"val_3193\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3188\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             739 |  # node_MatMul_3148\n",
       "                    %\"val_3194\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_3190\", %\"val_3193\")\n",
       "             740 |  # node_Softmax_3149\n",
       "                    %\"val_3195\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_3194\") {axis=-1}\n",
       "             741 |  # node_scaled_dot_product_attention_13\n",
       "                    %\"scaled_dot_product_attention_13\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3195\", %\"transpose_54\")\n",
       "             742 |  # node_transpose_55\n",
       "                    %\"transpose_55\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_13\") {perm=(0, 2, 1, 3)}\n",
       "             743 |  # node_view_65\n",
       "                    %\"view_65\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_55\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             744 |  # node_MatMul_3156\n",
       "                    %\"val_3202\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_65\", %\"val_3201\"{...})\n",
       "             745 |  # node_linear_78\n",
       "                    %\"linear_78\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3202\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             746 |  # node_add_37\n",
       "                    %\"add_37\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_78\", %\"add_36\")\n",
       "             747 |  # node_layer_norm_23\n",
       "                    %\"layer_norm_23\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_37\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             748 |  # node_MatMul_3158\n",
       "                    %\"val_3206\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_23\", %\"val_3205\"{...})\n",
       "             749 |  # node_linear_79\n",
       "                    %\"linear_79\"<FLOAT,[8,64,10240]>  ::Add(%\"val_3206\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             750 |  # node_Split_7123\n",
       "                    %\"split_4_split_0\"<FLOAT,[8,64,5120]>, %\"split_4_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_79\") {axis=2, num_outputs=2}\n",
       "             751 |  # node_gelu_10\n",
       "                    %\"gelu_10\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_4_split_1\") {approximate='none'}\n",
       "             752 |  # node_mul_7\n",
       "                    %\"mul_7\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_4_split_0\", %\"gelu_10\")\n",
       "             753 |  # node_MatMul_3161\n",
       "                    %\"val_3209\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_7\", %\"val_3208\"{...})\n",
       "             754 |  # node_linear_80\n",
       "                    %\"linear_80\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3209\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             755 |  # node_add_38\n",
       "                    %\"add_38\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_80\", %\"add_37\")\n",
       "             756 |  # node_view_66\n",
       "                    %\"view_66\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_38\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "             757 |  # node_permute_10\n",
       "                    %\"permute_10\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_66\") {perm=(0, 3, 1, 2)}\n",
       "             758 |  # node_conv2d_24\n",
       "                    %\"conv2d_24\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_10\", %\"unet.down_blocks.2.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.2.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             759 |  # node_add_39\n",
       "                    %\"add_39\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_24\", %\"add_35\")\n",
       "             760 |  # node_Reshape_3172\n",
       "                    %\"val_3220\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_39\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             761 |  # node_InstanceNormalization_3179\n",
       "                    %\"val_3227\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3220\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             762 |  # node_Reshape_3181\n",
       "                    %\"val_3229\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3227\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             763 |  # node_Mul_3188\n",
       "                    %\"val_3236\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3229\", %\"val_3235\"{...})\n",
       "             764 |  # node_group_norm_15\n",
       "                    %\"group_norm_15\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3236\", %\"val_3237\"{...})\n",
       "             765 |  # node_Sigmoid_3190\n",
       "                    %\"val_3238\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_15\")\n",
       "             766 |  # node_silu_16\n",
       "                    %\"silu_16\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_15\", %\"val_3238\")\n",
       "             767 |  # node_conv2d_25\n",
       "                    %\"conv2d_25\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_16\", %\"unet.down_blocks.2.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             768 |  # node_linear_81\n",
       "                    %\"linear_81\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.2.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.2.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             769 |  # node_Unsqueeze_7742\n",
       "                    %\"unsqueeze_13\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_81\", %\"val_6521\"{[2, 3]})\n",
       "             770 |  # node_add_40\n",
       "                    %\"add_40\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_25\", %\"unsqueeze_13\")\n",
       "             771 |  # node_Reshape_3196\n",
       "                    %\"val_3244\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_40\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             772 |  # node_InstanceNormalization_3203\n",
       "                    %\"val_3251\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3244\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             773 |  # node_Reshape_3205\n",
       "                    %\"val_3253\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3251\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             774 |  # node_Mul_3212\n",
       "                    %\"val_3260\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3253\", %\"val_3259\"{...})\n",
       "             775 |  # node_group_norm_16\n",
       "                    %\"group_norm_16\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3260\", %\"val_3261\"{...})\n",
       "             776 |  # node_Sigmoid_3214\n",
       "                    %\"val_3262\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_16\")\n",
       "             777 |  # node_silu_18\n",
       "                    %\"silu_18\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_16\", %\"val_3262\")\n",
       "             778 |  # node_conv2d_26\n",
       "                    %\"conv2d_26\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_18\", %\"unet.down_blocks.2.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             779 |  # node_add_41\n",
       "                    %\"add_41\"<FLOAT,[8,1280,8,8]>  ::Add(%\"add_39\", %\"conv2d_26\")\n",
       "             780 |  # node_Reshape_3219\n",
       "                    %\"val_3267\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_41\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             781 |  # node_InstanceNormalization_3226\n",
       "                    %\"val_3274\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3267\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             782 |  # node_Reshape_3228\n",
       "                    %\"val_3276\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3274\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             783 |  # node_Mul_3235\n",
       "                    %\"val_3283\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3276\", %\"val_3282\"{...})\n",
       "             784 |  # node_group_norm_17\n",
       "                    %\"group_norm_17\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3283\", %\"val_3284\"{...})\n",
       "             785 |  # node_conv2d_27\n",
       "                    %\"conv2d_27\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_17\", %\"unet.down_blocks.2.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.2.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             786 |  # node_permute_11\n",
       "                    %\"permute_11\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_27\") {perm=(0, 2, 3, 1)}\n",
       "             787 |  # node_view_67\n",
       "                    %\"view_67\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_11\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "             788 |  # node_layer_norm_24\n",
       "                    %\"layer_norm_24\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_67\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             789 |  # node_linear_82\n",
       "                    %\"linear_82\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3292\"{...})\n",
       "             790 |  # node_linear_83\n",
       "                    %\"linear_83\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3293\"{...})\n",
       "             791 |  # node_linear_84\n",
       "                    %\"linear_84\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3294\"{...})\n",
       "             792 |  # node_view_68\n",
       "                    %\"view_68\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_82\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             793 |  # node_transpose_56\n",
       "                    %\"transpose_56\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_68\") {perm=(0, 2, 1, 3)}\n",
       "             794 |  # node_view_69\n",
       "                    %\"view_69\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_83\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             795 |  # node_transpose_57\n",
       "                    %\"transpose_57\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_69\") {perm=(0, 2, 1, 3)}\n",
       "             796 |  # node_view_70\n",
       "                    %\"view_70\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_84\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             797 |  # node_transpose_58\n",
       "                    %\"transpose_58\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_70\") {perm=(0, 2, 1, 3)}\n",
       "             798 |  # node_Reshape_3280\n",
       "                    %\"val_3330\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_57\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "             799 |  # node_Transpose_3281\n",
       "                    %\"val_3331\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_3330\") {perm=(0, 2, 1)}\n",
       "             800 |  # node_Reshape_3283\n",
       "                    %\"val_3333\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_3331\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "             801 |  # node_Mul_3285\n",
       "                    %\"val_3335\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_56\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             802 |  # node_Mul_3288\n",
       "                    %\"val_3338\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_3333\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             803 |  # node_MatMul_3289\n",
       "                    %\"val_3339\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_3335\", %\"val_3338\")\n",
       "             804 |  # node_Softmax_3290\n",
       "                    %\"val_3340\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_3339\") {axis=-1}\n",
       "             805 |  # node_scaled_dot_product_attention_14\n",
       "                    %\"scaled_dot_product_attention_14\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3340\", %\"transpose_58\")\n",
       "             806 |  # node_transpose_59\n",
       "                    %\"transpose_59\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_14\") {perm=(0, 2, 1, 3)}\n",
       "             807 |  # node_view_71\n",
       "                    %\"view_71\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_59\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             808 |  # node_MatMul_3297\n",
       "                    %\"val_3347\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_71\", %\"val_3346\"{...})\n",
       "             809 |  # node_linear_85\n",
       "                    %\"linear_85\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3347\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             810 |  # node_add_42\n",
       "                    %\"add_42\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_85\", %\"view_67\")\n",
       "             811 |  # node_layer_norm_25\n",
       "                    %\"layer_norm_25\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_42\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             812 |  # node_linear_86\n",
       "                    %\"linear_86\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_25\", %\"val_3350\"{...})\n",
       "             813 |  # node_linear_87\n",
       "                    %\"linear_87\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3351\"{...})\n",
       "             814 |  # node_linear_88\n",
       "                    %\"linear_88\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3352\"{...})\n",
       "             815 |  # node_view_72\n",
       "                    %\"view_72\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_86\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             816 |  # node_transpose_60\n",
       "                    %\"transpose_60\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_72\") {perm=(0, 2, 1, 3)}\n",
       "             817 |  # node_view_73\n",
       "                    %\"view_73\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_87\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             818 |  # node_transpose_61\n",
       "                    %\"transpose_61\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_73\") {perm=(0, 2, 1, 3)}\n",
       "             819 |  # node_view_74\n",
       "                    %\"view_74\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_88\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             820 |  # node_transpose_62\n",
       "                    %\"transpose_62\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_74\") {perm=(0, 2, 1, 3)}\n",
       "             821 |  # node_Reshape_3336\n",
       "                    %\"val_3388\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_61\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             822 |  # node_Transpose_3337\n",
       "                    %\"val_3389\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3388\") {perm=(0, 2, 1)}\n",
       "             823 |  # node_Reshape_3339\n",
       "                    %\"val_3391\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3389\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             824 |  # node_Mul_3341\n",
       "                    %\"val_3393\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_60\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             825 |  # node_Mul_3344\n",
       "                    %\"val_3396\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3391\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             826 |  # node_MatMul_3345\n",
       "                    %\"val_3397\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_3393\", %\"val_3396\")\n",
       "             827 |  # node_Softmax_3346\n",
       "                    %\"val_3398\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_3397\") {axis=-1}\n",
       "             828 |  # node_scaled_dot_product_attention_15\n",
       "                    %\"scaled_dot_product_attention_15\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3398\", %\"transpose_62\")\n",
       "             829 |  # node_transpose_63\n",
       "                    %\"transpose_63\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_15\") {perm=(0, 2, 1, 3)}\n",
       "             830 |  # node_view_75\n",
       "                    %\"view_75\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_63\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             831 |  # node_MatMul_3353\n",
       "                    %\"val_3405\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_75\", %\"val_3404\"{...})\n",
       "             832 |  # node_linear_89\n",
       "                    %\"linear_89\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3405\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             833 |  # node_add_43\n",
       "                    %\"add_43\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_89\", %\"add_42\")\n",
       "             834 |  # node_layer_norm_26\n",
       "                    %\"layer_norm_26\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_43\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             835 |  # node_MatMul_3355\n",
       "                    %\"val_3409\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_26\", %\"val_3408\"{...})\n",
       "             836 |  # node_linear_90\n",
       "                    %\"linear_90\"<FLOAT,[8,64,10240]>  ::Add(%\"val_3409\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             837 |  # node_Split_7156\n",
       "                    %\"split_5_split_0\"<FLOAT,[8,64,5120]>, %\"split_5_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_90\") {axis=2, num_outputs=2}\n",
       "             838 |  # node_gelu_11\n",
       "                    %\"gelu_11\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_5_split_1\") {approximate='none'}\n",
       "             839 |  # node_mul_8\n",
       "                    %\"mul_8\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_5_split_0\", %\"gelu_11\")\n",
       "             840 |  # node_MatMul_3357\n",
       "                    %\"val_3411\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_8\", %\"val_3410\"{...})\n",
       "             841 |  # node_linear_91\n",
       "                    %\"linear_91\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3411\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             842 |  # node_add_44\n",
       "                    %\"add_44\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_91\", %\"add_43\")\n",
       "             843 |  # node_view_76\n",
       "                    %\"view_76\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_44\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "             844 |  # node_permute_12\n",
       "                    %\"permute_12\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_76\") {perm=(0, 3, 1, 2)}\n",
       "             845 |  # node_conv2d_28\n",
       "                    %\"conv2d_28\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_12\", %\"unet.down_blocks.2.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.2.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             846 |  # node_add_45\n",
       "                    %\"add_45\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_28\", %\"add_41\")\n",
       "             847 |  # node_conv2d_29\n",
       "                    %\"conv2d_29\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"add_45\", %\"unet.down_blocks.2.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.2.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             848 |  # node_Reshape_3368\n",
       "                    %\"val_3422\"<FLOAT,[8,32,640]>  ::Reshape(%\"conv2d_29\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             849 |  # node_InstanceNormalization_3375\n",
       "                    %\"val_3429\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3422\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             850 |  # node_Reshape_3377\n",
       "                    %\"val_3431\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3429\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             851 |  # node_Mul_3384\n",
       "                    %\"val_3438\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3431\", %\"val_3437\"{...})\n",
       "             852 |  # node_group_norm_18\n",
       "                    %\"group_norm_18\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3438\", %\"val_3439\"{...})\n",
       "             853 |  # node_Sigmoid_3386\n",
       "                    %\"val_3440\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_18\")\n",
       "             854 |  # node_silu_19\n",
       "                    %\"silu_19\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_18\", %\"val_3440\")\n",
       "             855 |  # node_conv2d_30\n",
       "                    %\"conv2d_30\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_19\", %\"unet.down_blocks.3.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             856 |  # node_linear_92\n",
       "                    %\"linear_92\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.3.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.3.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             857 |  # node_Unsqueeze_7747\n",
       "                    %\"unsqueeze_15\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_92\", %\"val_6521\"{[2, 3]})\n",
       "             858 |  # node_add_46\n",
       "                    %\"add_46\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_30\", %\"unsqueeze_15\")\n",
       "             859 |  # node_Reshape_3392\n",
       "                    %\"val_3446\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_46\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             860 |  # node_InstanceNormalization_3399\n",
       "                    %\"val_3453\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3446\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             861 |  # node_Reshape_3401\n",
       "                    %\"val_3455\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3453\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             862 |  # node_Mul_3408\n",
       "                    %\"val_3462\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3455\", %\"val_3461\"{...})\n",
       "             863 |  # node_group_norm_19\n",
       "                    %\"group_norm_19\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3462\", %\"val_3463\"{...})\n",
       "             864 |  # node_Sigmoid_3410\n",
       "                    %\"val_3464\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_19\")\n",
       "             865 |  # node_silu_21\n",
       "                    %\"silu_21\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_19\", %\"val_3464\")\n",
       "             866 |  # node_conv2d_31\n",
       "                    %\"conv2d_31\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_21\", %\"unet.down_blocks.3.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             867 |  # node_add_47\n",
       "                    %\"add_47\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_29\", %\"conv2d_31\")\n",
       "             868 |  # node_Reshape_3415\n",
       "                    %\"val_3469\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_47\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             869 |  # node_InstanceNormalization_3422\n",
       "                    %\"val_3476\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3469\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             870 |  # node_Reshape_3424\n",
       "                    %\"val_3478\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3476\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             871 |  # node_Mul_3431\n",
       "                    %\"val_3485\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3478\", %\"val_3484\"{...})\n",
       "             872 |  # node_group_norm_20\n",
       "                    %\"group_norm_20\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3485\", %\"val_3486\"{...})\n",
       "             873 |  # node_Sigmoid_3433\n",
       "                    %\"val_3487\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_20\")\n",
       "             874 |  # node_silu_22\n",
       "                    %\"silu_22\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_20\", %\"val_3487\")\n",
       "             875 |  # node_conv2d_32\n",
       "                    %\"conv2d_32\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_22\", %\"unet.down_blocks.3.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             876 |  # node_linear_93\n",
       "                    %\"linear_93\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.3.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.3.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             877 |  # node_Unsqueeze_7750\n",
       "                    %\"unsqueeze_17\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_93\", %\"val_6521\"{[2, 3]})\n",
       "             878 |  # node_add_48\n",
       "                    %\"add_48\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_32\", %\"unsqueeze_17\")\n",
       "             879 |  # node_Reshape_3439\n",
       "                    %\"val_3493\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_48\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             880 |  # node_InstanceNormalization_3446\n",
       "                    %\"val_3500\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3493\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             881 |  # node_Reshape_3448\n",
       "                    %\"val_3502\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3500\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             882 |  # node_Mul_3455\n",
       "                    %\"val_3509\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3502\", %\"val_3508\"{...})\n",
       "             883 |  # node_group_norm_21\n",
       "                    %\"group_norm_21\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3509\", %\"val_3510\"{...})\n",
       "             884 |  # node_Sigmoid_3457\n",
       "                    %\"val_3511\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_21\")\n",
       "             885 |  # node_silu_24\n",
       "                    %\"silu_24\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_21\", %\"val_3511\")\n",
       "             886 |  # node_conv2d_33\n",
       "                    %\"conv2d_33\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_24\", %\"unet.down_blocks.3.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             887 |  # node_add_49\n",
       "                    %\"add_49\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_47\", %\"conv2d_33\")\n",
       "             888 |  # node_Reshape_3462\n",
       "                    %\"val_3516\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_49\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             889 |  # node_InstanceNormalization_3469\n",
       "                    %\"val_3523\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3516\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             890 |  # node_Reshape_3471\n",
       "                    %\"val_3525\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3523\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             891 |  # node_Mul_3478\n",
       "                    %\"val_3532\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3525\", %\"val_3531\"{...})\n",
       "             892 |  # node_group_norm_22\n",
       "                    %\"group_norm_22\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3532\", %\"val_3533\"{...})\n",
       "             893 |  # node_Sigmoid_3480\n",
       "                    %\"val_3534\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_22\")\n",
       "             894 |  # node_silu_25\n",
       "                    %\"silu_25\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_22\", %\"val_3534\")\n",
       "             895 |  # node_conv2d_34\n",
       "                    %\"conv2d_34\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_25\", %\"unet.mid_block.resnets.0.conv1.weight\"{...}, %\"unet.mid_block.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             896 |  # node_linear_94\n",
       "                    %\"linear_94\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.mid_block.resnets.0.time_emb_proj.weight\"{...}, %\"unet.mid_block.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             897 |  # node_Unsqueeze_7753\n",
       "                    %\"unsqueeze_19\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_94\", %\"val_6521\"{[2, 3]})\n",
       "             898 |  # node_add_50\n",
       "                    %\"add_50\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_34\", %\"unsqueeze_19\")\n",
       "             899 |  # node_Reshape_3486\n",
       "                    %\"val_3540\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_50\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             900 |  # node_InstanceNormalization_3493\n",
       "                    %\"val_3547\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3540\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             901 |  # node_Reshape_3495\n",
       "                    %\"val_3549\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3547\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             902 |  # node_Mul_3502\n",
       "                    %\"val_3556\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3549\", %\"val_3555\"{...})\n",
       "             903 |  # node_group_norm_23\n",
       "                    %\"group_norm_23\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3556\", %\"val_3557\"{...})\n",
       "             904 |  # node_Sigmoid_3504\n",
       "                    %\"val_3558\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_23\")\n",
       "             905 |  # node_silu_27\n",
       "                    %\"silu_27\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_23\", %\"val_3558\")\n",
       "             906 |  # node_conv2d_35\n",
       "                    %\"conv2d_35\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_27\", %\"unet.mid_block.resnets.0.conv2.weight\"{...}, %\"unet.mid_block.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             907 |  # node_add_51\n",
       "                    %\"add_51\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_49\", %\"conv2d_35\")\n",
       "             908 |  # node_Reshape_3509\n",
       "                    %\"val_3563\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_51\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             909 |  # node_InstanceNormalization_3516\n",
       "                    %\"val_3570\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3563\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             910 |  # node_Reshape_3518\n",
       "                    %\"val_3572\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3570\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             911 |  # node_Mul_3525\n",
       "                    %\"val_3579\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3572\", %\"val_3578\"{...})\n",
       "             912 |  # node_group_norm_24\n",
       "                    %\"group_norm_24\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3579\", %\"val_3580\"{...})\n",
       "             913 |  # node_conv2d_36\n",
       "                    %\"conv2d_36\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"group_norm_24\", %\"unet.mid_block.attentions.0.proj_in.weight\"{...}, %\"unet.mid_block.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             914 |  # node_permute_13\n",
       "                    %\"permute_13\"<FLOAT,[8,4,4,1280]>  ::Transpose(%\"conv2d_36\") {perm=(0, 2, 3, 1)}\n",
       "             915 |  # node_view_77\n",
       "                    %\"view_77\"<FLOAT,[8,16,1280]>  ::Reshape(%\"permute_13\", %\"val_3585\"{[8, 16, 1280]}) {allowzero=1}\n",
       "             916 |  # node_layer_norm_27\n",
       "                    %\"layer_norm_27\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_77\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             917 |  # node_linear_95\n",
       "                    %\"linear_95\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3588\"{...})\n",
       "             918 |  # node_linear_96\n",
       "                    %\"linear_96\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3589\"{...})\n",
       "             919 |  # node_linear_97\n",
       "                    %\"linear_97\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3590\"{...})\n",
       "             920 |  # node_view_78\n",
       "                    %\"view_78\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_95\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             921 |  # node_transpose_64\n",
       "                    %\"transpose_64\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_78\") {perm=(0, 2, 1, 3)}\n",
       "             922 |  # node_view_79\n",
       "                    %\"view_79\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_96\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             923 |  # node_transpose_65\n",
       "                    %\"transpose_65\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_79\") {perm=(0, 2, 1, 3)}\n",
       "             924 |  # node_view_80\n",
       "                    %\"view_80\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_97\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             925 |  # node_transpose_66\n",
       "                    %\"transpose_66\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_80\") {perm=(0, 2, 1, 3)}\n",
       "             926 |  # node_Reshape_3570\n",
       "                    %\"val_3626\"<FLOAT,[64,16,160]>  ::Reshape(%\"transpose_65\", %\"val_3625\"{[-1, 16, 160]}) {allowzero=0}\n",
       "             927 |  # node_Transpose_3571\n",
       "                    %\"val_3627\"<FLOAT,[64,160,16]>  ::Transpose(%\"val_3626\") {perm=(0, 2, 1)}\n",
       "             928 |  # node_Reshape_3573\n",
       "                    %\"val_3629\"<FLOAT,[8,8,160,16]>  ::Reshape(%\"val_3627\", %\"val_3628\"{[8, 8, 160, 16]}) {allowzero=0}\n",
       "             929 |  # node_Mul_3575\n",
       "                    %\"val_3631\"<FLOAT,[8,8,16,160]>  ::Mul(%\"transpose_64\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             930 |  # node_Mul_3578\n",
       "                    %\"val_3634\"<FLOAT,[8,8,160,16]>  ::Mul(%\"val_3629\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             931 |  # node_MatMul_3579\n",
       "                    %\"val_3635\"<FLOAT,[8,8,16,16]>  ::MatMul(%\"val_3631\", %\"val_3634\")\n",
       "             932 |  # node_Softmax_3580\n",
       "                    %\"val_3636\"<FLOAT,[8,8,16,16]>  ::Softmax(%\"val_3635\") {axis=-1}\n",
       "             933 |  # node_scaled_dot_product_attention_16\n",
       "                    %\"scaled_dot_product_attention_16\"<FLOAT,[8,8,16,160]>  ::MatMul(%\"val_3636\", %\"transpose_66\")\n",
       "             934 |  # node_transpose_67\n",
       "                    %\"transpose_67\"<FLOAT,[8,16,8,160]>  ::Transpose(%\"scaled_dot_product_attention_16\") {perm=(0, 2, 1, 3)}\n",
       "             935 |  # node_view_81\n",
       "                    %\"view_81\"<FLOAT,[8,16,1280]>  ::Reshape(%\"transpose_67\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             936 |  # node_MatMul_3587\n",
       "                    %\"val_3643\"<FLOAT,[8,16,1280]>  ::MatMul(%\"view_81\", %\"val_3642\"{...})\n",
       "             937 |  # node_linear_98\n",
       "                    %\"linear_98\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3643\", %\"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             938 |  # node_add_52\n",
       "                    %\"add_52\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_98\", %\"view_77\")\n",
       "             939 |  # node_layer_norm_28\n",
       "                    %\"layer_norm_28\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_52\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             940 |  # node_linear_99\n",
       "                    %\"linear_99\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_28\", %\"val_3646\"{...})\n",
       "             941 |  # node_linear_100\n",
       "                    %\"linear_100\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3647\"{...})\n",
       "             942 |  # node_linear_101\n",
       "                    %\"linear_101\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3648\"{...})\n",
       "             943 |  # node_view_82\n",
       "                    %\"view_82\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_99\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             944 |  # node_transpose_68\n",
       "                    %\"transpose_68\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_82\") {perm=(0, 2, 1, 3)}\n",
       "             945 |  # node_view_83\n",
       "                    %\"view_83\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_100\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             946 |  # node_transpose_69\n",
       "                    %\"transpose_69\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_83\") {perm=(0, 2, 1, 3)}\n",
       "             947 |  # node_view_84\n",
       "                    %\"view_84\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_101\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             948 |  # node_transpose_70\n",
       "                    %\"transpose_70\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_84\") {perm=(0, 2, 1, 3)}\n",
       "             949 |  # node_Reshape_3626\n",
       "                    %\"val_3684\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_69\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             950 |  # node_Transpose_3627\n",
       "                    %\"val_3685\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3684\") {perm=(0, 2, 1)}\n",
       "             951 |  # node_Reshape_3629\n",
       "                    %\"val_3687\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3685\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             952 |  # node_Mul_3631\n",
       "                    %\"val_3689\"<FLOAT,[8,8,16,160]>  ::Mul(%\"transpose_68\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             953 |  # node_Mul_3634\n",
       "                    %\"val_3692\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3687\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             954 |  # node_MatMul_3635\n",
       "                    %\"val_3693\"<FLOAT,[8,8,16,50]>  ::MatMul(%\"val_3689\", %\"val_3692\")\n",
       "             955 |  # node_Softmax_3636\n",
       "                    %\"val_3694\"<FLOAT,[8,8,16,50]>  ::Softmax(%\"val_3693\") {axis=-1}\n",
       "             956 |  # node_scaled_dot_product_attention_17\n",
       "                    %\"scaled_dot_product_attention_17\"<FLOAT,[8,8,16,160]>  ::MatMul(%\"val_3694\", %\"transpose_70\")\n",
       "             957 |  # node_transpose_71\n",
       "                    %\"transpose_71\"<FLOAT,[8,16,8,160]>  ::Transpose(%\"scaled_dot_product_attention_17\") {perm=(0, 2, 1, 3)}\n",
       "             958 |  # node_view_85\n",
       "                    %\"view_85\"<FLOAT,[8,16,1280]>  ::Reshape(%\"transpose_71\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             959 |  # node_MatMul_3643\n",
       "                    %\"val_3701\"<FLOAT,[8,16,1280]>  ::MatMul(%\"view_85\", %\"val_3700\"{...})\n",
       "             960 |  # node_linear_102\n",
       "                    %\"linear_102\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3701\", %\"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             961 |  # node_add_53\n",
       "                    %\"add_53\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_102\", %\"add_52\")\n",
       "             962 |  # node_layer_norm_29\n",
       "                    %\"layer_norm_29\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_53\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             963 |  # node_MatMul_3645\n",
       "                    %\"val_3705\"<FLOAT,[8,16,10240]>  ::MatMul(%\"layer_norm_29\", %\"val_3704\"{...})\n",
       "             964 |  # node_linear_103\n",
       "                    %\"linear_103\"<FLOAT,[8,16,10240]>  ::Add(%\"val_3705\", %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             965 |  # node_Split_7209\n",
       "                    %\"split_6_split_0\"<FLOAT,[8,16,5120]>, %\"split_6_split_1\"<FLOAT,[8,16,5120]>  ::Split(%\"linear_103\") {axis=2, num_outputs=2}\n",
       "             966 |  # node_gelu_12\n",
       "                    %\"gelu_12\"<FLOAT,[8,16,5120]>  ::Gelu(%\"split_6_split_1\") {approximate='none'}\n",
       "             967 |  # node_mul_9\n",
       "                    %\"mul_9\"<FLOAT,[8,16,5120]>  ::Mul(%\"split_6_split_0\", %\"gelu_12\")\n",
       "             968 |  # node_MatMul_3647\n",
       "                    %\"val_3707\"<FLOAT,[8,16,1280]>  ::MatMul(%\"mul_9\", %\"val_3706\"{...})\n",
       "             969 |  # node_linear_104\n",
       "                    %\"linear_104\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3707\", %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             970 |  # node_add_54\n",
       "                    %\"add_54\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_104\", %\"add_53\")\n",
       "             971 |  # node_view_86\n",
       "                    %\"view_86\"<FLOAT,[8,4,4,1280]>  ::Reshape(%\"add_54\", %\"val_3713\"{[8, 4, 4, 1280]}) {allowzero=1}\n",
       "             972 |  # node_permute_14\n",
       "                    %\"permute_14\"<FLOAT,[8,1280,4,4]>  ::Transpose(%\"view_86\") {perm=(0, 3, 1, 2)}\n",
       "             973 |  # node_conv2d_37\n",
       "                    %\"conv2d_37\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"permute_14\", %\"unet.mid_block.attentions.0.proj_out.weight\"{...}, %\"unet.mid_block.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             974 |  # node_add_55\n",
       "                    %\"add_55\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_37\", %\"add_51\")\n",
       "             975 |  # node_Reshape_3658\n",
       "                    %\"val_3718\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_55\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             976 |  # node_InstanceNormalization_3665\n",
       "                    %\"val_3725\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3718\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             977 |  # node_Reshape_3667\n",
       "                    %\"val_3727\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3725\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             978 |  # node_Mul_3674\n",
       "                    %\"val_3734\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3727\", %\"val_3733\"{...})\n",
       "             979 |  # node_group_norm_25\n",
       "                    %\"group_norm_25\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3734\", %\"val_3735\"{...})\n",
       "             980 |  # node_Sigmoid_3676\n",
       "                    %\"val_3736\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_25\")\n",
       "             981 |  # node_silu_28\n",
       "                    %\"silu_28\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_25\", %\"val_3736\")\n",
       "             982 |  # node_conv2d_38\n",
       "                    %\"conv2d_38\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_28\", %\"unet.mid_block.resnets.1.conv1.weight\"{...}, %\"unet.mid_block.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             983 |  # node_linear_105\n",
       "                    %\"linear_105\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.mid_block.resnets.1.time_emb_proj.weight\"{...}, %\"unet.mid_block.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             984 |  # node_Unsqueeze_7758\n",
       "                    %\"unsqueeze_21\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_105\", %\"val_6521\"{[2, 3]})\n",
       "             985 |  # node_add_56\n",
       "                    %\"add_56\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_38\", %\"unsqueeze_21\")\n",
       "             986 |  # node_Reshape_3682\n",
       "                    %\"val_3742\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_56\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             987 |  # node_InstanceNormalization_3689\n",
       "                    %\"val_3749\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3742\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             988 |  # node_Reshape_3691\n",
       "                    %\"val_3751\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3749\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             989 |  # node_Mul_3698\n",
       "                    %\"val_3758\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3751\", %\"val_3757\"{...})\n",
       "             990 |  # node_group_norm_26\n",
       "                    %\"group_norm_26\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3758\", %\"val_3759\"{...})\n",
       "             991 |  # node_Sigmoid_3700\n",
       "                    %\"val_3760\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_26\")\n",
       "             992 |  # node_silu_30\n",
       "                    %\"silu_30\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_26\", %\"val_3760\")\n",
       "             993 |  # node_conv2d_39\n",
       "                    %\"conv2d_39\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_30\", %\"unet.mid_block.resnets.1.conv2.weight\"{...}, %\"unet.mid_block.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             994 |  # node_add_57\n",
       "                    %\"add_57\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_55\", %\"conv2d_39\")\n",
       "             995 |  # node_cat_5\n",
       "                    %\"cat_5\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_57\", %\"add_49\") {axis=1}\n",
       "             996 |  # node_Reshape_3705\n",
       "                    %\"val_3765\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_5\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             997 |  # node_InstanceNormalization_3712\n",
       "                    %\"val_3772\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3765\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             998 |  # node_Reshape_3714\n",
       "                    %\"val_3774\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3772\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "             999 |  # node_Mul_3721\n",
       "                    %\"val_3781\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3774\", %\"val_3780\"{...})\n",
       "            1000 |  # node_group_norm_27\n",
       "                    %\"group_norm_27\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3781\", %\"val_3782\"{...})\n",
       "            1001 |  # node_Sigmoid_3723\n",
       "                    %\"val_3783\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_27\")\n",
       "            1002 |  # node_silu_31\n",
       "                    %\"silu_31\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_27\", %\"val_3783\")\n",
       "            1003 |  # node_conv2d_40\n",
       "                    %\"conv2d_40\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_31\", %\"unet.up_blocks.0.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1004 |  # node_linear_106\n",
       "                    %\"linear_106\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1005 |  # node_Unsqueeze_7761\n",
       "                    %\"unsqueeze_23\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_106\", %\"val_6521\"{[2, 3]})\n",
       "            1006 |  # node_add_58\n",
       "                    %\"add_58\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_40\", %\"unsqueeze_23\")\n",
       "            1007 |  # node_Reshape_3729\n",
       "                    %\"val_3789\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_58\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1008 |  # node_InstanceNormalization_3736\n",
       "                    %\"val_3796\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3789\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1009 |  # node_Reshape_3738\n",
       "                    %\"val_3798\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3796\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1010 |  # node_Mul_3745\n",
       "                    %\"val_3805\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3798\", %\"val_3804\"{...})\n",
       "            1011 |  # node_group_norm_28\n",
       "                    %\"group_norm_28\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3805\", %\"val_3806\"{...})\n",
       "            1012 |  # node_Sigmoid_3747\n",
       "                    %\"val_3807\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_28\")\n",
       "            1013 |  # node_silu_33\n",
       "                    %\"silu_33\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_28\", %\"val_3807\")\n",
       "            1014 |  # node_conv2d_41\n",
       "                    %\"conv2d_41\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_33\", %\"unet.up_blocks.0.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1015 |  # node_conv2d_42\n",
       "                    %\"conv2d_42\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_5\", %\"unet.up_blocks.0.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1016 |  # node_add_59\n",
       "                    %\"add_59\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_42\", %\"conv2d_41\")\n",
       "            1017 |  # node_cat_6\n",
       "                    %\"cat_6\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_59\", %\"add_47\") {axis=1}\n",
       "            1018 |  # node_Reshape_3752\n",
       "                    %\"val_3812\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_6\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1019 |  # node_InstanceNormalization_3759\n",
       "                    %\"val_3819\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3812\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1020 |  # node_Reshape_3761\n",
       "                    %\"val_3821\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3819\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "            1021 |  # node_Mul_3768\n",
       "                    %\"val_3828\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3821\", %\"val_3827\"{...})\n",
       "            1022 |  # node_group_norm_29\n",
       "                    %\"group_norm_29\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3828\", %\"val_3829\"{...})\n",
       "            1023 |  # node_Sigmoid_3770\n",
       "                    %\"val_3830\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_29\")\n",
       "            1024 |  # node_silu_34\n",
       "                    %\"silu_34\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_29\", %\"val_3830\")\n",
       "            1025 |  # node_conv2d_43\n",
       "                    %\"conv2d_43\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_34\", %\"unet.up_blocks.0.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1026 |  # node_linear_107\n",
       "                    %\"linear_107\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1027 |  # node_Unsqueeze_7764\n",
       "                    %\"unsqueeze_25\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_107\", %\"val_6521\"{[2, 3]})\n",
       "            1028 |  # node_add_60\n",
       "                    %\"add_60\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_43\", %\"unsqueeze_25\")\n",
       "            1029 |  # node_Reshape_3776\n",
       "                    %\"val_3836\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_60\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1030 |  # node_InstanceNormalization_3783\n",
       "                    %\"val_3843\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3836\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1031 |  # node_Reshape_3785\n",
       "                    %\"val_3845\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3843\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1032 |  # node_Mul_3792\n",
       "                    %\"val_3852\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3845\", %\"val_3851\"{...})\n",
       "            1033 |  # node_group_norm_30\n",
       "                    %\"group_norm_30\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3852\", %\"val_3853\"{...})\n",
       "            1034 |  # node_Sigmoid_3794\n",
       "                    %\"val_3854\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_30\")\n",
       "            1035 |  # node_silu_36\n",
       "                    %\"silu_36\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_30\", %\"val_3854\")\n",
       "            1036 |  # node_conv2d_44\n",
       "                    %\"conv2d_44\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_36\", %\"unet.up_blocks.0.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1037 |  # node_conv2d_45\n",
       "                    %\"conv2d_45\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_6\", %\"unet.up_blocks.0.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1038 |  # node_add_61\n",
       "                    %\"add_61\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_45\", %\"conv2d_44\")\n",
       "            1039 |  # node_cat_7\n",
       "                    %\"cat_7\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_61\", %\"conv2d_29\") {axis=1}\n",
       "            1040 |  # node_Reshape_3799\n",
       "                    %\"val_3859\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_7\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1041 |  # node_InstanceNormalization_3806\n",
       "                    %\"val_3866\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3859\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1042 |  # node_Reshape_3808\n",
       "                    %\"val_3868\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3866\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "            1043 |  # node_Mul_3815\n",
       "                    %\"val_3875\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3868\", %\"val_3874\"{...})\n",
       "            1044 |  # node_group_norm_31\n",
       "                    %\"group_norm_31\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3875\", %\"val_3876\"{...})\n",
       "            1045 |  # node_Sigmoid_3817\n",
       "                    %\"val_3877\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_31\")\n",
       "            1046 |  # node_silu_37\n",
       "                    %\"silu_37\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_31\", %\"val_3877\")\n",
       "            1047 |  # node_conv2d_46\n",
       "                    %\"conv2d_46\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_37\", %\"unet.up_blocks.0.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1048 |  # node_linear_108\n",
       "                    %\"linear_108\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1049 |  # node_Unsqueeze_7767\n",
       "                    %\"unsqueeze_27\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_108\", %\"val_6521\"{[2, 3]})\n",
       "            1050 |  # node_add_62\n",
       "                    %\"add_62\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_46\", %\"unsqueeze_27\")\n",
       "            1051 |  # node_Reshape_3823\n",
       "                    %\"val_3883\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_62\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1052 |  # node_InstanceNormalization_3830\n",
       "                    %\"val_3890\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3883\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1053 |  # node_Reshape_3832\n",
       "                    %\"val_3892\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3890\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1054 |  # node_Mul_3839\n",
       "                    %\"val_3899\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3892\", %\"val_3898\"{...})\n",
       "            1055 |  # node_group_norm_32\n",
       "                    %\"group_norm_32\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3899\", %\"val_3900\"{...})\n",
       "            1056 |  # node_Sigmoid_3841\n",
       "                    %\"val_3901\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_32\")\n",
       "            1057 |  # node_silu_39\n",
       "                    %\"silu_39\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_32\", %\"val_3901\")\n",
       "            1058 |  # node_conv2d_47\n",
       "                    %\"conv2d_47\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_39\", %\"unet.up_blocks.0.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1059 |  # node_conv2d_48\n",
       "                    %\"conv2d_48\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_7\", %\"unet.up_blocks.0.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1060 |  # node_add_63\n",
       "                    %\"add_63\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_48\", %\"conv2d_47\")\n",
       "            1061 |  # node_upsample_nearest2d\n",
       "                    %\"upsample_nearest2d\"<FLOAT,[8,1280,8,8]>  ::Resize(%\"add_63\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1062 |  # node_conv2d_49\n",
       "                    %\"conv2d_49\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"upsample_nearest2d\", %\"unet.up_blocks.0.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.0.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1063 |  # node_cat_8\n",
       "                    %\"cat_8\"<FLOAT,[8,2560,8,8]>  ::Concat(%\"conv2d_49\", %\"add_45\") {axis=1}\n",
       "            1064 |  # node_Reshape_3847\n",
       "                    %\"val_3907\"<FLOAT,[8,32,5120]>  ::Reshape(%\"cat_8\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1065 |  # node_InstanceNormalization_3854\n",
       "                    %\"val_3914\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_3907\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1066 |  # node_Reshape_3856\n",
       "                    %\"val_3916\"<FLOAT,[8,2560,8,8]>  ::Reshape(%\"val_3914\", %\"val_3915\"{[8, 2560, 8, 8]}) {allowzero=0}\n",
       "            1067 |  # node_Mul_3863\n",
       "                    %\"val_3923\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"val_3916\", %\"val_3922\"{...})\n",
       "            1068 |  # node_group_norm_33\n",
       "                    %\"group_norm_33\"<FLOAT,[8,2560,8,8]>  ::Add(%\"val_3923\", %\"val_3924\"{...})\n",
       "            1069 |  # node_Sigmoid_3865\n",
       "                    %\"val_3925\"<FLOAT,[8,2560,8,8]>  ::Sigmoid(%\"group_norm_33\")\n",
       "            1070 |  # node_silu_40\n",
       "                    %\"silu_40\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"group_norm_33\", %\"val_3925\")\n",
       "            1071 |  # node_conv2d_50\n",
       "                    %\"conv2d_50\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_40\", %\"unet.up_blocks.1.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1072 |  # node_linear_109\n",
       "                    %\"linear_109\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1073 |  # node_Unsqueeze_7770\n",
       "                    %\"unsqueeze_29\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_109\", %\"val_6521\"{[2, 3]})\n",
       "            1074 |  # node_add_64\n",
       "                    %\"add_64\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_50\", %\"unsqueeze_29\")\n",
       "            1075 |  # node_Reshape_3871\n",
       "                    %\"val_3931\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_64\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1076 |  # node_InstanceNormalization_3878\n",
       "                    %\"val_3938\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3931\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1077 |  # node_Reshape_3880\n",
       "                    %\"val_3940\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3938\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1078 |  # node_Mul_3887\n",
       "                    %\"val_3947\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3940\", %\"val_3946\"{...})\n",
       "            1079 |  # node_group_norm_34\n",
       "                    %\"group_norm_34\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3947\", %\"val_3948\"{...})\n",
       "            1080 |  # node_Sigmoid_3889\n",
       "                    %\"val_3949\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_34\")\n",
       "            1081 |  # node_silu_42\n",
       "                    %\"silu_42\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_34\", %\"val_3949\")\n",
       "            1082 |  # node_conv2d_51\n",
       "                    %\"conv2d_51\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_42\", %\"unet.up_blocks.1.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1083 |  # node_conv2d_52\n",
       "                    %\"conv2d_52\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_8\", %\"unet.up_blocks.1.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1084 |  # node_add_65\n",
       "                    %\"add_65\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_52\", %\"conv2d_51\")\n",
       "            1085 |  # node_Reshape_3894\n",
       "                    %\"val_3954\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_65\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1086 |  # node_InstanceNormalization_3901\n",
       "                    %\"val_3961\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3954\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1087 |  # node_Reshape_3903\n",
       "                    %\"val_3963\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3961\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1088 |  # node_Mul_3910\n",
       "                    %\"val_3970\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3963\", %\"val_3969\"{...})\n",
       "            1089 |  # node_group_norm_35\n",
       "                    %\"group_norm_35\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3970\", %\"val_3971\"{...})\n",
       "            1090 |  # node_conv2d_53\n",
       "                    %\"conv2d_53\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_35\", %\"unet.up_blocks.1.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1091 |  # node_permute_15\n",
       "                    %\"permute_15\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_53\") {perm=(0, 2, 3, 1)}\n",
       "            1092 |  # node_view_87\n",
       "                    %\"view_87\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_15\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1093 |  # node_layer_norm_30\n",
       "                    %\"layer_norm_30\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_87\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1094 |  # node_linear_110\n",
       "                    %\"linear_110\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3979\"{...})\n",
       "            1095 |  # node_linear_111\n",
       "                    %\"linear_111\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3980\"{...})\n",
       "            1096 |  # node_linear_112\n",
       "                    %\"linear_112\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3981\"{...})\n",
       "            1097 |  # node_view_88\n",
       "                    %\"view_88\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_110\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1098 |  # node_transpose_72\n",
       "                    %\"transpose_72\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_88\") {perm=(0, 2, 1, 3)}\n",
       "            1099 |  # node_view_89\n",
       "                    %\"view_89\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_111\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1100 |  # node_transpose_73\n",
       "                    %\"transpose_73\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_89\") {perm=(0, 2, 1, 3)}\n",
       "            1101 |  # node_view_90\n",
       "                    %\"view_90\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_112\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1102 |  # node_transpose_74\n",
       "                    %\"transpose_74\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_90\") {perm=(0, 2, 1, 3)}\n",
       "            1103 |  # node_Reshape_3955\n",
       "                    %\"val_4017\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_73\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1104 |  # node_Transpose_3956\n",
       "                    %\"val_4018\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4017\") {perm=(0, 2, 1)}\n",
       "            1105 |  # node_Reshape_3958\n",
       "                    %\"val_4020\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4018\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1106 |  # node_Mul_3960\n",
       "                    %\"val_4022\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_72\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1107 |  # node_Mul_3963\n",
       "                    %\"val_4025\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4020\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1108 |  # node_MatMul_3964\n",
       "                    %\"val_4026\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4022\", %\"val_4025\")\n",
       "            1109 |  # node_Softmax_3965\n",
       "                    %\"val_4027\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4026\") {axis=-1}\n",
       "            1110 |  # node_scaled_dot_product_attention_18\n",
       "                    %\"scaled_dot_product_attention_18\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4027\", %\"transpose_74\")\n",
       "            1111 |  # node_transpose_75\n",
       "                    %\"transpose_75\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_18\") {perm=(0, 2, 1, 3)}\n",
       "            1112 |  # node_view_91\n",
       "                    %\"view_91\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_75\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1113 |  # node_MatMul_3972\n",
       "                    %\"val_4034\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_91\", %\"val_4033\"{...})\n",
       "            1114 |  # node_linear_113\n",
       "                    %\"linear_113\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4034\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1115 |  # node_add_66\n",
       "                    %\"add_66\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_113\", %\"view_87\")\n",
       "            1116 |  # node_layer_norm_31\n",
       "                    %\"layer_norm_31\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_66\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1117 |  # node_linear_114\n",
       "                    %\"linear_114\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_31\", %\"val_4037\"{...})\n",
       "            1118 |  # node_linear_115\n",
       "                    %\"linear_115\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4038\"{...})\n",
       "            1119 |  # node_linear_116\n",
       "                    %\"linear_116\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4039\"{...})\n",
       "            1120 |  # node_view_92\n",
       "                    %\"view_92\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_114\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1121 |  # node_transpose_76\n",
       "                    %\"transpose_76\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_92\") {perm=(0, 2, 1, 3)}\n",
       "            1122 |  # node_view_93\n",
       "                    %\"view_93\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_115\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1123 |  # node_transpose_77\n",
       "                    %\"transpose_77\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_93\") {perm=(0, 2, 1, 3)}\n",
       "            1124 |  # node_view_94\n",
       "                    %\"view_94\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_116\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1125 |  # node_transpose_78\n",
       "                    %\"transpose_78\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_94\") {perm=(0, 2, 1, 3)}\n",
       "            1126 |  # node_Reshape_4011\n",
       "                    %\"val_4075\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_77\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1127 |  # node_Transpose_4012\n",
       "                    %\"val_4076\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4075\") {perm=(0, 2, 1)}\n",
       "            1128 |  # node_Reshape_4014\n",
       "                    %\"val_4078\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4076\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1129 |  # node_Mul_4016\n",
       "                    %\"val_4080\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_76\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1130 |  # node_Mul_4019\n",
       "                    %\"val_4083\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4078\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1131 |  # node_MatMul_4020\n",
       "                    %\"val_4084\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4080\", %\"val_4083\")\n",
       "            1132 |  # node_Softmax_4021\n",
       "                    %\"val_4085\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4084\") {axis=-1}\n",
       "            1133 |  # node_scaled_dot_product_attention_19\n",
       "                    %\"scaled_dot_product_attention_19\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4085\", %\"transpose_78\")\n",
       "            1134 |  # node_transpose_79\n",
       "                    %\"transpose_79\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_19\") {perm=(0, 2, 1, 3)}\n",
       "            1135 |  # node_view_95\n",
       "                    %\"view_95\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_79\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1136 |  # node_MatMul_4028\n",
       "                    %\"val_4092\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_95\", %\"val_4091\"{...})\n",
       "            1137 |  # node_linear_117\n",
       "                    %\"linear_117\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4092\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1138 |  # node_add_67\n",
       "                    %\"add_67\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_117\", %\"add_66\")\n",
       "            1139 |  # node_layer_norm_32\n",
       "                    %\"layer_norm_32\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_67\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1140 |  # node_MatMul_4030\n",
       "                    %\"val_4096\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_32\", %\"val_4095\"{...})\n",
       "            1141 |  # node_linear_118\n",
       "                    %\"linear_118\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4096\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1142 |  # node_Split_7282\n",
       "                    %\"split_7_split_0\"<FLOAT,[8,64,5120]>, %\"split_7_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_118\") {axis=2, num_outputs=2}\n",
       "            1143 |  # node_gelu_13\n",
       "                    %\"gelu_13\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_7_split_1\") {approximate='none'}\n",
       "            1144 |  # node_mul_10\n",
       "                    %\"mul_10\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_7_split_0\", %\"gelu_13\")\n",
       "            1145 |  # node_MatMul_4032\n",
       "                    %\"val_4098\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_10\", %\"val_4097\"{...})\n",
       "            1146 |  # node_linear_119\n",
       "                    %\"linear_119\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4098\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1147 |  # node_add_68\n",
       "                    %\"add_68\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_119\", %\"add_67\")\n",
       "            1148 |  # node_view_96\n",
       "                    %\"view_96\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_68\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1149 |  # node_permute_16\n",
       "                    %\"permute_16\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_96\") {perm=(0, 3, 1, 2)}\n",
       "            1150 |  # node_conv2d_54\n",
       "                    %\"conv2d_54\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_16\", %\"unet.up_blocks.1.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1151 |  # node_add_69\n",
       "                    %\"add_69\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_54\", %\"add_65\")\n",
       "            1152 |  # node_cat_9\n",
       "                    %\"cat_9\"<FLOAT,[8,2560,8,8]>  ::Concat(%\"add_69\", %\"add_39\") {axis=1}\n",
       "            1153 |  # node_Reshape_4043\n",
       "                    %\"val_4109\"<FLOAT,[8,32,5120]>  ::Reshape(%\"cat_9\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1154 |  # node_InstanceNormalization_4050\n",
       "                    %\"val_4116\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4109\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1155 |  # node_Reshape_4052\n",
       "                    %\"val_4118\"<FLOAT,[8,2560,8,8]>  ::Reshape(%\"val_4116\", %\"val_3915\"{[8, 2560, 8, 8]}) {allowzero=0}\n",
       "            1156 |  # node_Mul_4059\n",
       "                    %\"val_4125\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"val_4118\", %\"val_4124\"{...})\n",
       "            1157 |  # node_group_norm_36\n",
       "                    %\"group_norm_36\"<FLOAT,[8,2560,8,8]>  ::Add(%\"val_4125\", %\"val_4126\"{...})\n",
       "            1158 |  # node_Sigmoid_4061\n",
       "                    %\"val_4127\"<FLOAT,[8,2560,8,8]>  ::Sigmoid(%\"group_norm_36\")\n",
       "            1159 |  # node_silu_43\n",
       "                    %\"silu_43\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"group_norm_36\", %\"val_4127\")\n",
       "            1160 |  # node_conv2d_55\n",
       "                    %\"conv2d_55\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_43\", %\"unet.up_blocks.1.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1161 |  # node_linear_120\n",
       "                    %\"linear_120\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1162 |  # node_Unsqueeze_7775\n",
       "                    %\"unsqueeze_31\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_120\", %\"val_6521\"{[2, 3]})\n",
       "            1163 |  # node_add_70\n",
       "                    %\"add_70\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_55\", %\"unsqueeze_31\")\n",
       "            1164 |  # node_Reshape_4067\n",
       "                    %\"val_4133\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_70\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1165 |  # node_InstanceNormalization_4074\n",
       "                    %\"val_4140\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4133\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1166 |  # node_Reshape_4076\n",
       "                    %\"val_4142\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4140\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1167 |  # node_Mul_4083\n",
       "                    %\"val_4149\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4142\", %\"val_4148\"{...})\n",
       "            1168 |  # node_group_norm_37\n",
       "                    %\"group_norm_37\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4149\", %\"val_4150\"{...})\n",
       "            1169 |  # node_Sigmoid_4085\n",
       "                    %\"val_4151\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_37\")\n",
       "            1170 |  # node_silu_45\n",
       "                    %\"silu_45\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_37\", %\"val_4151\")\n",
       "            1171 |  # node_conv2d_56\n",
       "                    %\"conv2d_56\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_45\", %\"unet.up_blocks.1.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1172 |  # node_conv2d_57\n",
       "                    %\"conv2d_57\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_9\", %\"unet.up_blocks.1.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1173 |  # node_add_71\n",
       "                    %\"add_71\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_57\", %\"conv2d_56\")\n",
       "            1174 |  # node_Reshape_4090\n",
       "                    %\"val_4156\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_71\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1175 |  # node_InstanceNormalization_4097\n",
       "                    %\"val_4163\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4156\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1176 |  # node_Reshape_4099\n",
       "                    %\"val_4165\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4163\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1177 |  # node_Mul_4106\n",
       "                    %\"val_4172\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4165\", %\"val_4171\"{...})\n",
       "            1178 |  # node_group_norm_38\n",
       "                    %\"group_norm_38\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4172\", %\"val_4173\"{...})\n",
       "            1179 |  # node_conv2d_58\n",
       "                    %\"conv2d_58\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_38\", %\"unet.up_blocks.1.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1180 |  # node_permute_17\n",
       "                    %\"permute_17\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_58\") {perm=(0, 2, 3, 1)}\n",
       "            1181 |  # node_view_97\n",
       "                    %\"view_97\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_17\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1182 |  # node_layer_norm_33\n",
       "                    %\"layer_norm_33\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_97\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1183 |  # node_linear_121\n",
       "                    %\"linear_121\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4181\"{...})\n",
       "            1184 |  # node_linear_122\n",
       "                    %\"linear_122\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4182\"{...})\n",
       "            1185 |  # node_linear_123\n",
       "                    %\"linear_123\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4183\"{...})\n",
       "            1186 |  # node_view_98\n",
       "                    %\"view_98\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_121\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1187 |  # node_transpose_80\n",
       "                    %\"transpose_80\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_98\") {perm=(0, 2, 1, 3)}\n",
       "            1188 |  # node_view_99\n",
       "                    %\"view_99\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_122\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1189 |  # node_transpose_81\n",
       "                    %\"transpose_81\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_99\") {perm=(0, 2, 1, 3)}\n",
       "            1190 |  # node_view_100\n",
       "                    %\"view_100\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_123\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1191 |  # node_transpose_82\n",
       "                    %\"transpose_82\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_100\") {perm=(0, 2, 1, 3)}\n",
       "            1192 |  # node_Reshape_4151\n",
       "                    %\"val_4219\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_81\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1193 |  # node_Transpose_4152\n",
       "                    %\"val_4220\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4219\") {perm=(0, 2, 1)}\n",
       "            1194 |  # node_Reshape_4154\n",
       "                    %\"val_4222\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4220\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1195 |  # node_Mul_4156\n",
       "                    %\"val_4224\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_80\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1196 |  # node_Mul_4159\n",
       "                    %\"val_4227\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4222\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1197 |  # node_MatMul_4160\n",
       "                    %\"val_4228\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4224\", %\"val_4227\")\n",
       "            1198 |  # node_Softmax_4161\n",
       "                    %\"val_4229\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4228\") {axis=-1}\n",
       "            1199 |  # node_scaled_dot_product_attention_20\n",
       "                    %\"scaled_dot_product_attention_20\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4229\", %\"transpose_82\")\n",
       "            1200 |  # node_transpose_83\n",
       "                    %\"transpose_83\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_20\") {perm=(0, 2, 1, 3)}\n",
       "            1201 |  # node_view_101\n",
       "                    %\"view_101\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_83\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1202 |  # node_MatMul_4168\n",
       "                    %\"val_4236\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_101\", %\"val_4235\"{...})\n",
       "            1203 |  # node_linear_124\n",
       "                    %\"linear_124\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4236\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1204 |  # node_add_72\n",
       "                    %\"add_72\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_124\", %\"view_97\")\n",
       "            1205 |  # node_layer_norm_34\n",
       "                    %\"layer_norm_34\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_72\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1206 |  # node_linear_125\n",
       "                    %\"linear_125\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_34\", %\"val_4239\"{...})\n",
       "            1207 |  # node_linear_126\n",
       "                    %\"linear_126\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4240\"{...})\n",
       "            1208 |  # node_linear_127\n",
       "                    %\"linear_127\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4241\"{...})\n",
       "            1209 |  # node_view_102\n",
       "                    %\"view_102\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_125\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1210 |  # node_transpose_84\n",
       "                    %\"transpose_84\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_102\") {perm=(0, 2, 1, 3)}\n",
       "            1211 |  # node_view_103\n",
       "                    %\"view_103\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_126\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1212 |  # node_transpose_85\n",
       "                    %\"transpose_85\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_103\") {perm=(0, 2, 1, 3)}\n",
       "            1213 |  # node_view_104\n",
       "                    %\"view_104\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_127\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1214 |  # node_transpose_86\n",
       "                    %\"transpose_86\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_104\") {perm=(0, 2, 1, 3)}\n",
       "            1215 |  # node_Reshape_4207\n",
       "                    %\"val_4277\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_85\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1216 |  # node_Transpose_4208\n",
       "                    %\"val_4278\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4277\") {perm=(0, 2, 1)}\n",
       "            1217 |  # node_Reshape_4210\n",
       "                    %\"val_4280\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4278\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1218 |  # node_Mul_4212\n",
       "                    %\"val_4282\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_84\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1219 |  # node_Mul_4215\n",
       "                    %\"val_4285\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4280\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1220 |  # node_MatMul_4216\n",
       "                    %\"val_4286\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4282\", %\"val_4285\")\n",
       "            1221 |  # node_Softmax_4217\n",
       "                    %\"val_4287\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4286\") {axis=-1}\n",
       "            1222 |  # node_scaled_dot_product_attention_21\n",
       "                    %\"scaled_dot_product_attention_21\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4287\", %\"transpose_86\")\n",
       "            1223 |  # node_transpose_87\n",
       "                    %\"transpose_87\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_21\") {perm=(0, 2, 1, 3)}\n",
       "            1224 |  # node_view_105\n",
       "                    %\"view_105\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_87\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1225 |  # node_MatMul_4224\n",
       "                    %\"val_4294\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_105\", %\"val_4293\"{...})\n",
       "            1226 |  # node_linear_128\n",
       "                    %\"linear_128\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4294\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1227 |  # node_add_73\n",
       "                    %\"add_73\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_128\", %\"add_72\")\n",
       "            1228 |  # node_layer_norm_35\n",
       "                    %\"layer_norm_35\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_73\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1229 |  # node_MatMul_4226\n",
       "                    %\"val_4298\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_35\", %\"val_4297\"{...})\n",
       "            1230 |  # node_linear_129\n",
       "                    %\"linear_129\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4298\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1231 |  # node_Split_7315\n",
       "                    %\"split_8_split_0\"<FLOAT,[8,64,5120]>, %\"split_8_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_129\") {axis=2, num_outputs=2}\n",
       "            1232 |  # node_gelu_14\n",
       "                    %\"gelu_14\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_8_split_1\") {approximate='none'}\n",
       "            1233 |  # node_mul_11\n",
       "                    %\"mul_11\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_8_split_0\", %\"gelu_14\")\n",
       "            1234 |  # node_MatMul_4228\n",
       "                    %\"val_4300\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_11\", %\"val_4299\"{...})\n",
       "            1235 |  # node_linear_130\n",
       "                    %\"linear_130\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4300\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1236 |  # node_add_74\n",
       "                    %\"add_74\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_130\", %\"add_73\")\n",
       "            1237 |  # node_view_106\n",
       "                    %\"view_106\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_74\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1238 |  # node_permute_18\n",
       "                    %\"permute_18\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_106\") {perm=(0, 3, 1, 2)}\n",
       "            1239 |  # node_conv2d_59\n",
       "                    %\"conv2d_59\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_18\", %\"unet.up_blocks.1.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1240 |  # node_add_75\n",
       "                    %\"add_75\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_59\", %\"add_71\")\n",
       "            1241 |  # node_cat_10\n",
       "                    %\"cat_10\"<FLOAT,[8,1920,8,8]>  ::Concat(%\"add_75\", %\"conv2d_19\") {axis=1}\n",
       "            1242 |  # node_Reshape_4239\n",
       "                    %\"val_4311\"<FLOAT,[8,32,3840]>  ::Reshape(%\"cat_10\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1243 |  # node_InstanceNormalization_4246\n",
       "                    %\"val_4318\"<FLOAT,[8,32,3840]>  ::InstanceNormalization(%\"val_4311\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1244 |  # node_Reshape_4248\n",
       "                    %\"val_4320\"<FLOAT,[8,1920,8,8]>  ::Reshape(%\"val_4318\", %\"val_4319\"{[8, 1920, 8, 8]}) {allowzero=0}\n",
       "            1245 |  # node_Mul_4255\n",
       "                    %\"val_4327\"<FLOAT,[8,1920,8,8]>  ::Mul(%\"val_4320\", %\"val_4326\"{...})\n",
       "            1246 |  # node_group_norm_39\n",
       "                    %\"group_norm_39\"<FLOAT,[8,1920,8,8]>  ::Add(%\"val_4327\", %\"val_4328\"{...})\n",
       "            1247 |  # node_Sigmoid_4257\n",
       "                    %\"val_4329\"<FLOAT,[8,1920,8,8]>  ::Sigmoid(%\"group_norm_39\")\n",
       "            1248 |  # node_silu_46\n",
       "                    %\"silu_46\"<FLOAT,[8,1920,8,8]>  ::Mul(%\"group_norm_39\", %\"val_4329\")\n",
       "            1249 |  # node_conv2d_60\n",
       "                    %\"conv2d_60\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_46\", %\"unet.up_blocks.1.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1250 |  # node_linear_131\n",
       "                    %\"linear_131\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1251 |  # node_Unsqueeze_7780\n",
       "                    %\"unsqueeze_33\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_131\", %\"val_6521\"{[2, 3]})\n",
       "            1252 |  # node_add_76\n",
       "                    %\"add_76\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_60\", %\"unsqueeze_33\")\n",
       "            1253 |  # node_Reshape_4263\n",
       "                    %\"val_4335\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_76\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1254 |  # node_InstanceNormalization_4270\n",
       "                    %\"val_4342\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4335\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1255 |  # node_Reshape_4272\n",
       "                    %\"val_4344\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4342\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1256 |  # node_Mul_4279\n",
       "                    %\"val_4351\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4344\", %\"val_4350\"{...})\n",
       "            1257 |  # node_group_norm_40\n",
       "                    %\"group_norm_40\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4351\", %\"val_4352\"{...})\n",
       "            1258 |  # node_Sigmoid_4281\n",
       "                    %\"val_4353\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_40\")\n",
       "            1259 |  # node_silu_48\n",
       "                    %\"silu_48\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_40\", %\"val_4353\")\n",
       "            1260 |  # node_conv2d_61\n",
       "                    %\"conv2d_61\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_48\", %\"unet.up_blocks.1.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1261 |  # node_conv2d_62\n",
       "                    %\"conv2d_62\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_10\", %\"unet.up_blocks.1.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1262 |  # node_add_77\n",
       "                    %\"add_77\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_62\", %\"conv2d_61\")\n",
       "            1263 |  # node_Reshape_4286\n",
       "                    %\"val_4358\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_77\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1264 |  # node_InstanceNormalization_4293\n",
       "                    %\"val_4365\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4358\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1265 |  # node_Reshape_4295\n",
       "                    %\"val_4367\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4365\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1266 |  # node_Mul_4302\n",
       "                    %\"val_4374\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4367\", %\"val_4373\"{...})\n",
       "            1267 |  # node_group_norm_41\n",
       "                    %\"group_norm_41\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4374\", %\"val_4375\"{...})\n",
       "            1268 |  # node_conv2d_63\n",
       "                    %\"conv2d_63\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_41\", %\"unet.up_blocks.1.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1269 |  # node_permute_19\n",
       "                    %\"permute_19\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_63\") {perm=(0, 2, 3, 1)}\n",
       "            1270 |  # node_view_107\n",
       "                    %\"view_107\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_19\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1271 |  # node_layer_norm_36\n",
       "                    %\"layer_norm_36\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_107\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1272 |  # node_linear_132\n",
       "                    %\"linear_132\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4383\"{...})\n",
       "            1273 |  # node_linear_133\n",
       "                    %\"linear_133\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4384\"{...})\n",
       "            1274 |  # node_linear_134\n",
       "                    %\"linear_134\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4385\"{...})\n",
       "            1275 |  # node_view_108\n",
       "                    %\"view_108\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_132\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1276 |  # node_transpose_88\n",
       "                    %\"transpose_88\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_108\") {perm=(0, 2, 1, 3)}\n",
       "            1277 |  # node_view_109\n",
       "                    %\"view_109\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_133\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1278 |  # node_transpose_89\n",
       "                    %\"transpose_89\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_109\") {perm=(0, 2, 1, 3)}\n",
       "            1279 |  # node_view_110\n",
       "                    %\"view_110\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_134\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1280 |  # node_transpose_90\n",
       "                    %\"transpose_90\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_110\") {perm=(0, 2, 1, 3)}\n",
       "            1281 |  # node_Reshape_4347\n",
       "                    %\"val_4421\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_89\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1282 |  # node_Transpose_4348\n",
       "                    %\"val_4422\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4421\") {perm=(0, 2, 1)}\n",
       "            1283 |  # node_Reshape_4350\n",
       "                    %\"val_4424\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4422\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1284 |  # node_Mul_4352\n",
       "                    %\"val_4426\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_88\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1285 |  # node_Mul_4355\n",
       "                    %\"val_4429\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4424\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1286 |  # node_MatMul_4356\n",
       "                    %\"val_4430\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4426\", %\"val_4429\")\n",
       "            1287 |  # node_Softmax_4357\n",
       "                    %\"val_4431\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4430\") {axis=-1}\n",
       "            1288 |  # node_scaled_dot_product_attention_22\n",
       "                    %\"scaled_dot_product_attention_22\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4431\", %\"transpose_90\")\n",
       "            1289 |  # node_transpose_91\n",
       "                    %\"transpose_91\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_22\") {perm=(0, 2, 1, 3)}\n",
       "            1290 |  # node_view_111\n",
       "                    %\"view_111\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_91\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1291 |  # node_MatMul_4364\n",
       "                    %\"val_4438\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_111\", %\"val_4437\"{...})\n",
       "            1292 |  # node_linear_135\n",
       "                    %\"linear_135\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4438\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1293 |  # node_add_78\n",
       "                    %\"add_78\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_135\", %\"view_107\")\n",
       "            1294 |  # node_layer_norm_37\n",
       "                    %\"layer_norm_37\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_78\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1295 |  # node_linear_136\n",
       "                    %\"linear_136\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_37\", %\"val_4441\"{...})\n",
       "            1296 |  # node_linear_137\n",
       "                    %\"linear_137\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4442\"{...})\n",
       "            1297 |  # node_linear_138\n",
       "                    %\"linear_138\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4443\"{...})\n",
       "            1298 |  # node_view_112\n",
       "                    %\"view_112\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_136\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1299 |  # node_transpose_92\n",
       "                    %\"transpose_92\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_112\") {perm=(0, 2, 1, 3)}\n",
       "            1300 |  # node_view_113\n",
       "                    %\"view_113\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_137\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1301 |  # node_transpose_93\n",
       "                    %\"transpose_93\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_113\") {perm=(0, 2, 1, 3)}\n",
       "            1302 |  # node_view_114\n",
       "                    %\"view_114\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_138\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1303 |  # node_transpose_94\n",
       "                    %\"transpose_94\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_114\") {perm=(0, 2, 1, 3)}\n",
       "            1304 |  # node_Reshape_4403\n",
       "                    %\"val_4479\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_93\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1305 |  # node_Transpose_4404\n",
       "                    %\"val_4480\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4479\") {perm=(0, 2, 1)}\n",
       "            1306 |  # node_Reshape_4406\n",
       "                    %\"val_4482\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4480\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1307 |  # node_Mul_4408\n",
       "                    %\"val_4484\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_92\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1308 |  # node_Mul_4411\n",
       "                    %\"val_4487\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4482\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1309 |  # node_MatMul_4412\n",
       "                    %\"val_4488\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4484\", %\"val_4487\")\n",
       "            1310 |  # node_Softmax_4413\n",
       "                    %\"val_4489\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4488\") {axis=-1}\n",
       "            1311 |  # node_scaled_dot_product_attention_23\n",
       "                    %\"scaled_dot_product_attention_23\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4489\", %\"transpose_94\")\n",
       "            1312 |  # node_transpose_95\n",
       "                    %\"transpose_95\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_23\") {perm=(0, 2, 1, 3)}\n",
       "            1313 |  # node_view_115\n",
       "                    %\"view_115\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_95\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1314 |  # node_MatMul_4420\n",
       "                    %\"val_4496\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_115\", %\"val_4495\"{...})\n",
       "            1315 |  # node_linear_139\n",
       "                    %\"linear_139\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4496\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1316 |  # node_add_79\n",
       "                    %\"add_79\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_139\", %\"add_78\")\n",
       "            1317 |  # node_layer_norm_38\n",
       "                    %\"layer_norm_38\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_79\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1318 |  # node_MatMul_4422\n",
       "                    %\"val_4500\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_38\", %\"val_4499\"{...})\n",
       "            1319 |  # node_linear_140\n",
       "                    %\"linear_140\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4500\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1320 |  # node_Split_7348\n",
       "                    %\"split_9_split_0\"<FLOAT,[8,64,5120]>, %\"split_9_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_140\") {axis=2, num_outputs=2}\n",
       "            1321 |  # node_gelu_15\n",
       "                    %\"gelu_15\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_9_split_1\") {approximate='none'}\n",
       "            1322 |  # node_mul_12\n",
       "                    %\"mul_12\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_9_split_0\", %\"gelu_15\")\n",
       "            1323 |  # node_MatMul_4424\n",
       "                    %\"val_4502\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_12\", %\"val_4501\"{...})\n",
       "            1324 |  # node_linear_141\n",
       "                    %\"linear_141\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4502\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1325 |  # node_add_80\n",
       "                    %\"add_80\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_141\", %\"add_79\")\n",
       "            1326 |  # node_view_116\n",
       "                    %\"view_116\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_80\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1327 |  # node_permute_20\n",
       "                    %\"permute_20\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_116\") {perm=(0, 3, 1, 2)}\n",
       "            1328 |  # node_conv2d_64\n",
       "                    %\"conv2d_64\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_20\", %\"unet.up_blocks.1.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1329 |  # node_add_81\n",
       "                    %\"add_81\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_64\", %\"add_77\")\n",
       "            1330 |  # node_upsample_nearest2d_1\n",
       "                    %\"upsample_nearest2d_1\"<FLOAT,[8,1280,16,16]>  ::Resize(%\"add_81\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1331 |  # node_conv2d_65\n",
       "                    %\"conv2d_65\"<FLOAT,[8,1280,16,16]>  ::Conv(%\"upsample_nearest2d_1\", %\"unet.up_blocks.1.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.1.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1332 |  # node_cat_11\n",
       "                    %\"cat_11\"<FLOAT,[8,1920,16,16]>  ::Concat(%\"conv2d_65\", %\"add_33\") {axis=1}\n",
       "            1333 |  # node_Reshape_4436\n",
       "                    %\"val_4514\"<FLOAT,[8,32,15360]>  ::Reshape(%\"cat_11\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1334 |  # node_InstanceNormalization_4443\n",
       "                    %\"val_4521\"<FLOAT,[8,32,15360]>  ::InstanceNormalization(%\"val_4514\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1335 |  # node_Reshape_4445\n",
       "                    %\"val_4523\"<FLOAT,[8,1920,16,16]>  ::Reshape(%\"val_4521\", %\"val_4522\"{[8, 1920, 16, 16]}) {allowzero=0}\n",
       "            1336 |  # node_Mul_4452\n",
       "                    %\"val_4530\"<FLOAT,[8,1920,16,16]>  ::Mul(%\"val_4523\", %\"val_4529\"{...})\n",
       "            1337 |  # node_group_norm_42\n",
       "                    %\"group_norm_42\"<FLOAT,[8,1920,16,16]>  ::Add(%\"val_4530\", %\"val_4531\"{...})\n",
       "            1338 |  # node_Sigmoid_4454\n",
       "                    %\"val_4532\"<FLOAT,[8,1920,16,16]>  ::Sigmoid(%\"group_norm_42\")\n",
       "            1339 |  # node_silu_49\n",
       "                    %\"silu_49\"<FLOAT,[8,1920,16,16]>  ::Mul(%\"group_norm_42\", %\"val_4532\")\n",
       "            1340 |  # node_conv2d_66\n",
       "                    %\"conv2d_66\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_49\", %\"unet.up_blocks.2.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1341 |  # node_linear_142\n",
       "                    %\"linear_142\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1342 |  # node_Unsqueeze_7785\n",
       "                    %\"unsqueeze_35\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_142\", %\"val_6521\"{[2, 3]})\n",
       "            1343 |  # node_add_82\n",
       "                    %\"add_82\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_66\", %\"unsqueeze_35\")\n",
       "            1344 |  # node_Reshape_4460\n",
       "                    %\"val_4538\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_82\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1345 |  # node_InstanceNormalization_4467\n",
       "                    %\"val_4545\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4538\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1346 |  # node_Reshape_4469\n",
       "                    %\"val_4547\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4545\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1347 |  # node_Mul_4476\n",
       "                    %\"val_4554\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4547\", %\"val_4553\"{...})\n",
       "            1348 |  # node_group_norm_43\n",
       "                    %\"group_norm_43\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4554\", %\"val_4555\"{...})\n",
       "            1349 |  # node_Sigmoid_4478\n",
       "                    %\"val_4556\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_43\")\n",
       "            1350 |  # node_silu_51\n",
       "                    %\"silu_51\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_43\", %\"val_4556\")\n",
       "            1351 |  # node_conv2d_67\n",
       "                    %\"conv2d_67\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_51\", %\"unet.up_blocks.2.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1352 |  # node_conv2d_68\n",
       "                    %\"conv2d_68\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_11\", %\"unet.up_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1353 |  # node_add_83\n",
       "                    %\"add_83\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_68\", %\"conv2d_67\")\n",
       "            1354 |  # node_Reshape_4483\n",
       "                    %\"val_4561\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_83\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1355 |  # node_InstanceNormalization_4490\n",
       "                    %\"val_4568\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4561\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1356 |  # node_Reshape_4492\n",
       "                    %\"val_4570\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4568\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1357 |  # node_Mul_4499\n",
       "                    %\"val_4577\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4570\", %\"val_4576\"{...})\n",
       "            1358 |  # node_group_norm_44\n",
       "                    %\"group_norm_44\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4577\", %\"val_4578\"{...})\n",
       "            1359 |  # node_conv2d_69\n",
       "                    %\"conv2d_69\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_44\", %\"unet.up_blocks.2.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1360 |  # node_permute_21\n",
       "                    %\"permute_21\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_69\") {perm=(0, 2, 3, 1)}\n",
       "            1361 |  # node_view_117\n",
       "                    %\"view_117\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_21\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1362 |  # node_layer_norm_39\n",
       "                    %\"layer_norm_39\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_117\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1363 |  # node_linear_143\n",
       "                    %\"linear_143\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4586\"{...})\n",
       "            1364 |  # node_linear_144\n",
       "                    %\"linear_144\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4587\"{...})\n",
       "            1365 |  # node_linear_145\n",
       "                    %\"linear_145\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4588\"{...})\n",
       "            1366 |  # node_view_118\n",
       "                    %\"view_118\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_143\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1367 |  # node_transpose_96\n",
       "                    %\"transpose_96\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_118\") {perm=(0, 2, 1, 3)}\n",
       "            1368 |  # node_view_119\n",
       "                    %\"view_119\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_144\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1369 |  # node_transpose_97\n",
       "                    %\"transpose_97\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_119\") {perm=(0, 2, 1, 3)}\n",
       "            1370 |  # node_view_120\n",
       "                    %\"view_120\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_145\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1371 |  # node_transpose_98\n",
       "                    %\"transpose_98\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_120\") {perm=(0, 2, 1, 3)}\n",
       "            1372 |  # node_Reshape_4544\n",
       "                    %\"val_4624\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_97\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1373 |  # node_Transpose_4545\n",
       "                    %\"val_4625\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_4624\") {perm=(0, 2, 1)}\n",
       "            1374 |  # node_Reshape_4547\n",
       "                    %\"val_4627\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_4625\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1375 |  # node_Mul_4549\n",
       "                    %\"val_4629\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_96\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1376 |  # node_Mul_4552\n",
       "                    %\"val_4632\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_4627\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1377 |  # node_MatMul_4553\n",
       "                    %\"val_4633\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_4629\", %\"val_4632\")\n",
       "            1378 |  # node_Softmax_4554\n",
       "                    %\"val_4634\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_4633\") {axis=-1}\n",
       "            1379 |  # node_scaled_dot_product_attention_24\n",
       "                    %\"scaled_dot_product_attention_24\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4634\", %\"transpose_98\")\n",
       "            1380 |  # node_transpose_99\n",
       "                    %\"transpose_99\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_24\") {perm=(0, 2, 1, 3)}\n",
       "            1381 |  # node_view_121\n",
       "                    %\"view_121\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_99\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1382 |  # node_MatMul_4561\n",
       "                    %\"val_4641\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_121\", %\"val_4640\"{...})\n",
       "            1383 |  # node_linear_146\n",
       "                    %\"linear_146\"<FLOAT,[8,256,640]>  ::Add(%\"val_4641\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1384 |  # node_add_84\n",
       "                    %\"add_84\"<FLOAT,[8,256,640]>  ::Add(%\"linear_146\", %\"view_117\")\n",
       "            1385 |  # node_layer_norm_40\n",
       "                    %\"layer_norm_40\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_84\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1386 |  # node_linear_147\n",
       "                    %\"linear_147\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_40\", %\"val_4644\"{...})\n",
       "            1387 |  # node_linear_148\n",
       "                    %\"linear_148\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4645\"{...})\n",
       "            1388 |  # node_linear_149\n",
       "                    %\"linear_149\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4646\"{...})\n",
       "            1389 |  # node_view_122\n",
       "                    %\"view_122\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_147\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1390 |  # node_transpose_100\n",
       "                    %\"transpose_100\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_122\") {perm=(0, 2, 1, 3)}\n",
       "            1391 |  # node_view_123\n",
       "                    %\"view_123\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_148\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1392 |  # node_transpose_101\n",
       "                    %\"transpose_101\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_123\") {perm=(0, 2, 1, 3)}\n",
       "            1393 |  # node_view_124\n",
       "                    %\"view_124\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_149\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1394 |  # node_transpose_102\n",
       "                    %\"transpose_102\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_124\") {perm=(0, 2, 1, 3)}\n",
       "            1395 |  # node_Reshape_4600\n",
       "                    %\"val_4682\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_101\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1396 |  # node_Transpose_4601\n",
       "                    %\"val_4683\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_4682\") {perm=(0, 2, 1)}\n",
       "            1397 |  # node_Reshape_4603\n",
       "                    %\"val_4685\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_4683\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1398 |  # node_Mul_4605\n",
       "                    %\"val_4687\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_100\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1399 |  # node_Mul_4608\n",
       "                    %\"val_4690\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_4685\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1400 |  # node_MatMul_4609\n",
       "                    %\"val_4691\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_4687\", %\"val_4690\")\n",
       "            1401 |  # node_Softmax_4610\n",
       "                    %\"val_4692\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_4691\") {axis=-1}\n",
       "            1402 |  # node_scaled_dot_product_attention_25\n",
       "                    %\"scaled_dot_product_attention_25\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4692\", %\"transpose_102\")\n",
       "            1403 |  # node_transpose_103\n",
       "                    %\"transpose_103\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_25\") {perm=(0, 2, 1, 3)}\n",
       "            1404 |  # node_view_125\n",
       "                    %\"view_125\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_103\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1405 |  # node_MatMul_4617\n",
       "                    %\"val_4699\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_125\", %\"val_4698\"{...})\n",
       "            1406 |  # node_linear_150\n",
       "                    %\"linear_150\"<FLOAT,[8,256,640]>  ::Add(%\"val_4699\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1407 |  # node_add_85\n",
       "                    %\"add_85\"<FLOAT,[8,256,640]>  ::Add(%\"linear_150\", %\"add_84\")\n",
       "            1408 |  # node_layer_norm_41\n",
       "                    %\"layer_norm_41\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_85\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1409 |  # node_MatMul_4619\n",
       "                    %\"val_4703\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_41\", %\"val_4702\"{...})\n",
       "            1410 |  # node_linear_151\n",
       "                    %\"linear_151\"<FLOAT,[8,256,5120]>  ::Add(%\"val_4703\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1411 |  # node_Split_7381\n",
       "                    %\"split_10_split_0\"<FLOAT,[8,256,2560]>, %\"split_10_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_151\") {axis=2, num_outputs=2}\n",
       "            1412 |  # node_gelu_16\n",
       "                    %\"gelu_16\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_10_split_1\") {approximate='none'}\n",
       "            1413 |  # node_mul_13\n",
       "                    %\"mul_13\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_10_split_0\", %\"gelu_16\")\n",
       "            1414 |  # node_MatMul_4621\n",
       "                    %\"val_4705\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_13\", %\"val_4704\"{...})\n",
       "            1415 |  # node_linear_152\n",
       "                    %\"linear_152\"<FLOAT,[8,256,640]>  ::Add(%\"val_4705\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1416 |  # node_add_86\n",
       "                    %\"add_86\"<FLOAT,[8,256,640]>  ::Add(%\"linear_152\", %\"add_85\")\n",
       "            1417 |  # node_view_126\n",
       "                    %\"view_126\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_86\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1418 |  # node_permute_22\n",
       "                    %\"permute_22\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_126\") {perm=(0, 3, 1, 2)}\n",
       "            1419 |  # node_conv2d_70\n",
       "                    %\"conv2d_70\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_22\", %\"unet.up_blocks.2.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1420 |  # node_add_87\n",
       "                    %\"add_87\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_70\", %\"add_83\")\n",
       "            1421 |  # node_cat_12\n",
       "                    %\"cat_12\"<FLOAT,[8,1280,16,16]>  ::Concat(%\"add_87\", %\"add_27\") {axis=1}\n",
       "            1422 |  # node_Reshape_4632\n",
       "                    %\"val_4716\"<FLOAT,[8,32,10240]>  ::Reshape(%\"cat_12\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1423 |  # node_InstanceNormalization_4639\n",
       "                    %\"val_4723\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_4716\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1424 |  # node_Reshape_4641\n",
       "                    %\"val_4725\"<FLOAT,[8,1280,16,16]>  ::Reshape(%\"val_4723\", %\"val_4724\"{[8, 1280, 16, 16]}) {allowzero=0}\n",
       "            1425 |  # node_Mul_4648\n",
       "                    %\"val_4732\"<FLOAT,[8,1280,16,16]>  ::Mul(%\"val_4725\", %\"val_4731\"{...})\n",
       "            1426 |  # node_group_norm_45\n",
       "                    %\"group_norm_45\"<FLOAT,[8,1280,16,16]>  ::Add(%\"val_4732\", %\"val_4733\"{...})\n",
       "            1427 |  # node_Sigmoid_4650\n",
       "                    %\"val_4734\"<FLOAT,[8,1280,16,16]>  ::Sigmoid(%\"group_norm_45\")\n",
       "            1428 |  # node_silu_52\n",
       "                    %\"silu_52\"<FLOAT,[8,1280,16,16]>  ::Mul(%\"group_norm_45\", %\"val_4734\")\n",
       "            1429 |  # node_conv2d_71\n",
       "                    %\"conv2d_71\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_52\", %\"unet.up_blocks.2.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1430 |  # node_linear_153\n",
       "                    %\"linear_153\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1431 |  # node_Unsqueeze_7790\n",
       "                    %\"unsqueeze_37\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_153\", %\"val_6521\"{[2, 3]})\n",
       "            1432 |  # node_add_88\n",
       "                    %\"add_88\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_71\", %\"unsqueeze_37\")\n",
       "            1433 |  # node_Reshape_4656\n",
       "                    %\"val_4740\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_88\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1434 |  # node_InstanceNormalization_4663\n",
       "                    %\"val_4747\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4740\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1435 |  # node_Reshape_4665\n",
       "                    %\"val_4749\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4747\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1436 |  # node_Mul_4672\n",
       "                    %\"val_4756\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4749\", %\"val_4755\"{...})\n",
       "            1437 |  # node_group_norm_46\n",
       "                    %\"group_norm_46\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4756\", %\"val_4757\"{...})\n",
       "            1438 |  # node_Sigmoid_4674\n",
       "                    %\"val_4758\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_46\")\n",
       "            1439 |  # node_silu_54\n",
       "                    %\"silu_54\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_46\", %\"val_4758\")\n",
       "            1440 |  # node_conv2d_72\n",
       "                    %\"conv2d_72\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_54\", %\"unet.up_blocks.2.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1441 |  # node_conv2d_73\n",
       "                    %\"conv2d_73\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_12\", %\"unet.up_blocks.2.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1442 |  # node_add_89\n",
       "                    %\"add_89\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_73\", %\"conv2d_72\")\n",
       "            1443 |  # node_Reshape_4679\n",
       "                    %\"val_4763\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_89\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1444 |  # node_InstanceNormalization_4686\n",
       "                    %\"val_4770\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4763\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1445 |  # node_Reshape_4688\n",
       "                    %\"val_4772\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4770\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1446 |  # node_Mul_4695\n",
       "                    %\"val_4779\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4772\", %\"val_4778\"{...})\n",
       "            1447 |  # node_group_norm_47\n",
       "                    %\"group_norm_47\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4779\", %\"val_4780\"{...})\n",
       "            1448 |  # node_conv2d_74\n",
       "                    %\"conv2d_74\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_47\", %\"unet.up_blocks.2.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1449 |  # node_permute_23\n",
       "                    %\"permute_23\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_74\") {perm=(0, 2, 3, 1)}\n",
       "            1450 |  # node_view_127\n",
       "                    %\"view_127\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_23\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1451 |  # node_layer_norm_42\n",
       "                    %\"layer_norm_42\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_127\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1452 |  # node_linear_154\n",
       "                    %\"linear_154\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4788\"{...})\n",
       "            1453 |  # node_linear_155\n",
       "                    %\"linear_155\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4789\"{...})\n",
       "            1454 |  # node_linear_156\n",
       "                    %\"linear_156\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4790\"{...})\n",
       "            1455 |  # node_view_128\n",
       "                    %\"view_128\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_154\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1456 |  # node_transpose_104\n",
       "                    %\"transpose_104\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_128\") {perm=(0, 2, 1, 3)}\n",
       "            1457 |  # node_view_129\n",
       "                    %\"view_129\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_155\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1458 |  # node_transpose_105\n",
       "                    %\"transpose_105\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_129\") {perm=(0, 2, 1, 3)}\n",
       "            1459 |  # node_view_130\n",
       "                    %\"view_130\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_156\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1460 |  # node_transpose_106\n",
       "                    %\"transpose_106\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_130\") {perm=(0, 2, 1, 3)}\n",
       "            1461 |  # node_Reshape_4740\n",
       "                    %\"val_4826\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_105\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1462 |  # node_Transpose_4741\n",
       "                    %\"val_4827\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_4826\") {perm=(0, 2, 1)}\n",
       "            1463 |  # node_Reshape_4743\n",
       "                    %\"val_4829\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_4827\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1464 |  # node_Mul_4745\n",
       "                    %\"val_4831\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_104\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1465 |  # node_Mul_4748\n",
       "                    %\"val_4834\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_4829\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1466 |  # node_MatMul_4749\n",
       "                    %\"val_4835\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_4831\", %\"val_4834\")\n",
       "            1467 |  # node_Softmax_4750\n",
       "                    %\"val_4836\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_4835\") {axis=-1}\n",
       "            1468 |  # node_scaled_dot_product_attention_26\n",
       "                    %\"scaled_dot_product_attention_26\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4836\", %\"transpose_106\")\n",
       "            1469 |  # node_transpose_107\n",
       "                    %\"transpose_107\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_26\") {perm=(0, 2, 1, 3)}\n",
       "            1470 |  # node_view_131\n",
       "                    %\"view_131\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_107\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1471 |  # node_MatMul_4757\n",
       "                    %\"val_4843\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_131\", %\"val_4842\"{...})\n",
       "            1472 |  # node_linear_157\n",
       "                    %\"linear_157\"<FLOAT,[8,256,640]>  ::Add(%\"val_4843\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1473 |  # node_add_90\n",
       "                    %\"add_90\"<FLOAT,[8,256,640]>  ::Add(%\"linear_157\", %\"view_127\")\n",
       "            1474 |  # node_layer_norm_43\n",
       "                    %\"layer_norm_43\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_90\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1475 |  # node_linear_158\n",
       "                    %\"linear_158\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_43\", %\"val_4846\"{...})\n",
       "            1476 |  # node_linear_159\n",
       "                    %\"linear_159\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4847\"{...})\n",
       "            1477 |  # node_linear_160\n",
       "                    %\"linear_160\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4848\"{...})\n",
       "            1478 |  # node_view_132\n",
       "                    %\"view_132\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_158\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1479 |  # node_transpose_108\n",
       "                    %\"transpose_108\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_132\") {perm=(0, 2, 1, 3)}\n",
       "            1480 |  # node_view_133\n",
       "                    %\"view_133\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_159\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1481 |  # node_transpose_109\n",
       "                    %\"transpose_109\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_133\") {perm=(0, 2, 1, 3)}\n",
       "            1482 |  # node_view_134\n",
       "                    %\"view_134\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_160\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1483 |  # node_transpose_110\n",
       "                    %\"transpose_110\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_134\") {perm=(0, 2, 1, 3)}\n",
       "            1484 |  # node_Reshape_4796\n",
       "                    %\"val_4884\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_109\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1485 |  # node_Transpose_4797\n",
       "                    %\"val_4885\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_4884\") {perm=(0, 2, 1)}\n",
       "            1486 |  # node_Reshape_4799\n",
       "                    %\"val_4887\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_4885\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1487 |  # node_Mul_4801\n",
       "                    %\"val_4889\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_108\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1488 |  # node_Mul_4804\n",
       "                    %\"val_4892\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_4887\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1489 |  # node_MatMul_4805\n",
       "                    %\"val_4893\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_4889\", %\"val_4892\")\n",
       "            1490 |  # node_Softmax_4806\n",
       "                    %\"val_4894\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_4893\") {axis=-1}\n",
       "            1491 |  # node_scaled_dot_product_attention_27\n",
       "                    %\"scaled_dot_product_attention_27\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4894\", %\"transpose_110\")\n",
       "            1492 |  # node_transpose_111\n",
       "                    %\"transpose_111\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_27\") {perm=(0, 2, 1, 3)}\n",
       "            1493 |  # node_view_135\n",
       "                    %\"view_135\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_111\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1494 |  # node_MatMul_4813\n",
       "                    %\"val_4901\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_135\", %\"val_4900\"{...})\n",
       "            1495 |  # node_linear_161\n",
       "                    %\"linear_161\"<FLOAT,[8,256,640]>  ::Add(%\"val_4901\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1496 |  # node_add_91\n",
       "                    %\"add_91\"<FLOAT,[8,256,640]>  ::Add(%\"linear_161\", %\"add_90\")\n",
       "            1497 |  # node_layer_norm_44\n",
       "                    %\"layer_norm_44\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_91\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1498 |  # node_MatMul_4815\n",
       "                    %\"val_4905\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_44\", %\"val_4904\"{...})\n",
       "            1499 |  # node_linear_162\n",
       "                    %\"linear_162\"<FLOAT,[8,256,5120]>  ::Add(%\"val_4905\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1500 |  # node_Split_7414\n",
       "                    %\"split_11_split_0\"<FLOAT,[8,256,2560]>, %\"split_11_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_162\") {axis=2, num_outputs=2}\n",
       "            1501 |  # node_gelu_17\n",
       "                    %\"gelu_17\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_11_split_1\") {approximate='none'}\n",
       "            1502 |  # node_mul_14\n",
       "                    %\"mul_14\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_11_split_0\", %\"gelu_17\")\n",
       "            1503 |  # node_MatMul_4817\n",
       "                    %\"val_4907\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_14\", %\"val_4906\"{...})\n",
       "            1504 |  # node_linear_163\n",
       "                    %\"linear_163\"<FLOAT,[8,256,640]>  ::Add(%\"val_4907\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1505 |  # node_add_92\n",
       "                    %\"add_92\"<FLOAT,[8,256,640]>  ::Add(%\"linear_163\", %\"add_91\")\n",
       "            1506 |  # node_view_136\n",
       "                    %\"view_136\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_92\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1507 |  # node_permute_24\n",
       "                    %\"permute_24\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_136\") {perm=(0, 3, 1, 2)}\n",
       "            1508 |  # node_conv2d_75\n",
       "                    %\"conv2d_75\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_24\", %\"unet.up_blocks.2.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1509 |  # node_add_93\n",
       "                    %\"add_93\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_75\", %\"add_89\")\n",
       "            1510 |  # node_cat_13\n",
       "                    %\"cat_13\"<FLOAT,[8,960,16,16]>  ::Concat(%\"add_93\", %\"conv2d_9\") {axis=1}\n",
       "            1511 |  # node_Reshape_4828\n",
       "                    %\"val_4918\"<FLOAT,[8,32,7680]>  ::Reshape(%\"cat_13\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1512 |  # node_InstanceNormalization_4835\n",
       "                    %\"val_4925\"<FLOAT,[8,32,7680]>  ::InstanceNormalization(%\"val_4918\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1513 |  # node_Reshape_4837\n",
       "                    %\"val_4927\"<FLOAT,[8,960,16,16]>  ::Reshape(%\"val_4925\", %\"val_4926\"{[8, 960, 16, 16]}) {allowzero=0}\n",
       "            1514 |  # node_Mul_4844\n",
       "                    %\"val_4934\"<FLOAT,[8,960,16,16]>  ::Mul(%\"val_4927\", %\"val_4933\"{...})\n",
       "            1515 |  # node_group_norm_48\n",
       "                    %\"group_norm_48\"<FLOAT,[8,960,16,16]>  ::Add(%\"val_4934\", %\"val_4935\"{...})\n",
       "            1516 |  # node_Sigmoid_4846\n",
       "                    %\"val_4936\"<FLOAT,[8,960,16,16]>  ::Sigmoid(%\"group_norm_48\")\n",
       "            1517 |  # node_silu_55\n",
       "                    %\"silu_55\"<FLOAT,[8,960,16,16]>  ::Mul(%\"group_norm_48\", %\"val_4936\")\n",
       "            1518 |  # node_conv2d_76\n",
       "                    %\"conv2d_76\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_55\", %\"unet.up_blocks.2.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1519 |  # node_linear_164\n",
       "                    %\"linear_164\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1520 |  # node_Unsqueeze_7795\n",
       "                    %\"unsqueeze_39\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_164\", %\"val_6521\"{[2, 3]})\n",
       "            1521 |  # node_add_94\n",
       "                    %\"add_94\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_76\", %\"unsqueeze_39\")\n",
       "            1522 |  # node_Reshape_4852\n",
       "                    %\"val_4942\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_94\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1523 |  # node_InstanceNormalization_4859\n",
       "                    %\"val_4949\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4942\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1524 |  # node_Reshape_4861\n",
       "                    %\"val_4951\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4949\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1525 |  # node_Mul_4868\n",
       "                    %\"val_4958\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4951\", %\"val_4957\"{...})\n",
       "            1526 |  # node_group_norm_49\n",
       "                    %\"group_norm_49\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4958\", %\"val_4959\"{...})\n",
       "            1527 |  # node_Sigmoid_4870\n",
       "                    %\"val_4960\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_49\")\n",
       "            1528 |  # node_silu_57\n",
       "                    %\"silu_57\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_49\", %\"val_4960\")\n",
       "            1529 |  # node_conv2d_77\n",
       "                    %\"conv2d_77\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_57\", %\"unet.up_blocks.2.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1530 |  # node_conv2d_78\n",
       "                    %\"conv2d_78\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_13\", %\"unet.up_blocks.2.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1531 |  # node_add_95\n",
       "                    %\"add_95\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_78\", %\"conv2d_77\")\n",
       "            1532 |  # node_Reshape_4875\n",
       "                    %\"val_4965\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_95\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1533 |  # node_InstanceNormalization_4882\n",
       "                    %\"val_4972\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4965\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1534 |  # node_Reshape_4884\n",
       "                    %\"val_4974\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4972\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1535 |  # node_Mul_4891\n",
       "                    %\"val_4981\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4974\", %\"val_4980\"{...})\n",
       "            1536 |  # node_group_norm_50\n",
       "                    %\"group_norm_50\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4981\", %\"val_4982\"{...})\n",
       "            1537 |  # node_conv2d_79\n",
       "                    %\"conv2d_79\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_50\", %\"unet.up_blocks.2.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1538 |  # node_permute_25\n",
       "                    %\"permute_25\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_79\") {perm=(0, 2, 3, 1)}\n",
       "            1539 |  # node_view_137\n",
       "                    %\"view_137\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_25\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1540 |  # node_layer_norm_45\n",
       "                    %\"layer_norm_45\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_137\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1541 |  # node_linear_165\n",
       "                    %\"linear_165\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4990\"{...})\n",
       "            1542 |  # node_linear_166\n",
       "                    %\"linear_166\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4991\"{...})\n",
       "            1543 |  # node_linear_167\n",
       "                    %\"linear_167\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4992\"{...})\n",
       "            1544 |  # node_view_138\n",
       "                    %\"view_138\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_165\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1545 |  # node_transpose_112\n",
       "                    %\"transpose_112\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_138\") {perm=(0, 2, 1, 3)}\n",
       "            1546 |  # node_view_139\n",
       "                    %\"view_139\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_166\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1547 |  # node_transpose_113\n",
       "                    %\"transpose_113\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_139\") {perm=(0, 2, 1, 3)}\n",
       "            1548 |  # node_view_140\n",
       "                    %\"view_140\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_167\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1549 |  # node_transpose_114\n",
       "                    %\"transpose_114\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_140\") {perm=(0, 2, 1, 3)}\n",
       "            1550 |  # node_Reshape_4936\n",
       "                    %\"val_5028\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_113\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1551 |  # node_Transpose_4937\n",
       "                    %\"val_5029\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_5028\") {perm=(0, 2, 1)}\n",
       "            1552 |  # node_Reshape_4939\n",
       "                    %\"val_5031\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_5029\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1553 |  # node_Mul_4941\n",
       "                    %\"val_5033\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_112\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1554 |  # node_Mul_4944\n",
       "                    %\"val_5036\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_5031\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1555 |  # node_MatMul_4945\n",
       "                    %\"val_5037\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_5033\", %\"val_5036\")\n",
       "            1556 |  # node_Softmax_4946\n",
       "                    %\"val_5038\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_5037\") {axis=-1}\n",
       "            1557 |  # node_scaled_dot_product_attention_28\n",
       "                    %\"scaled_dot_product_attention_28\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_5038\", %\"transpose_114\")\n",
       "            1558 |  # node_transpose_115\n",
       "                    %\"transpose_115\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_28\") {perm=(0, 2, 1, 3)}\n",
       "            1559 |  # node_view_141\n",
       "                    %\"view_141\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_115\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1560 |  # node_MatMul_4953\n",
       "                    %\"val_5045\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_141\", %\"val_5044\"{...})\n",
       "            1561 |  # node_linear_168\n",
       "                    %\"linear_168\"<FLOAT,[8,256,640]>  ::Add(%\"val_5045\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1562 |  # node_add_96\n",
       "                    %\"add_96\"<FLOAT,[8,256,640]>  ::Add(%\"linear_168\", %\"view_137\")\n",
       "            1563 |  # node_layer_norm_46\n",
       "                    %\"layer_norm_46\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_96\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1564 |  # node_linear_169\n",
       "                    %\"linear_169\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_46\", %\"val_5048\"{...})\n",
       "            1565 |  # node_linear_170\n",
       "                    %\"linear_170\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_5049\"{...})\n",
       "            1566 |  # node_linear_171\n",
       "                    %\"linear_171\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_5050\"{...})\n",
       "            1567 |  # node_view_142\n",
       "                    %\"view_142\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_169\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1568 |  # node_transpose_116\n",
       "                    %\"transpose_116\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_142\") {perm=(0, 2, 1, 3)}\n",
       "            1569 |  # node_view_143\n",
       "                    %\"view_143\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_170\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1570 |  # node_transpose_117\n",
       "                    %\"transpose_117\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_143\") {perm=(0, 2, 1, 3)}\n",
       "            1571 |  # node_view_144\n",
       "                    %\"view_144\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_171\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1572 |  # node_transpose_118\n",
       "                    %\"transpose_118\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_144\") {perm=(0, 2, 1, 3)}\n",
       "            1573 |  # node_Reshape_4992\n",
       "                    %\"val_5086\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_117\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1574 |  # node_Transpose_4993\n",
       "                    %\"val_5087\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_5086\") {perm=(0, 2, 1)}\n",
       "            1575 |  # node_Reshape_4995\n",
       "                    %\"val_5089\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_5087\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1576 |  # node_Mul_4997\n",
       "                    %\"val_5091\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_116\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1577 |  # node_Mul_5000\n",
       "                    %\"val_5094\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_5089\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1578 |  # node_MatMul_5001\n",
       "                    %\"val_5095\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_5091\", %\"val_5094\")\n",
       "            1579 |  # node_Softmax_5002\n",
       "                    %\"val_5096\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_5095\") {axis=-1}\n",
       "            1580 |  # node_scaled_dot_product_attention_29\n",
       "                    %\"scaled_dot_product_attention_29\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_5096\", %\"transpose_118\")\n",
       "            1581 |  # node_transpose_119\n",
       "                    %\"transpose_119\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_29\") {perm=(0, 2, 1, 3)}\n",
       "            1582 |  # node_view_145\n",
       "                    %\"view_145\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_119\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1583 |  # node_MatMul_5009\n",
       "                    %\"val_5103\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_145\", %\"val_5102\"{...})\n",
       "            1584 |  # node_linear_172\n",
       "                    %\"linear_172\"<FLOAT,[8,256,640]>  ::Add(%\"val_5103\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1585 |  # node_add_97\n",
       "                    %\"add_97\"<FLOAT,[8,256,640]>  ::Add(%\"linear_172\", %\"add_96\")\n",
       "            1586 |  # node_layer_norm_47\n",
       "                    %\"layer_norm_47\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_97\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1587 |  # node_MatMul_5011\n",
       "                    %\"val_5107\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_47\", %\"val_5106\"{...})\n",
       "            1588 |  # node_linear_173\n",
       "                    %\"linear_173\"<FLOAT,[8,256,5120]>  ::Add(%\"val_5107\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1589 |  # node_Split_7447\n",
       "                    %\"split_12_split_0\"<FLOAT,[8,256,2560]>, %\"split_12_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_173\") {axis=2, num_outputs=2}\n",
       "            1590 |  # node_gelu_18\n",
       "                    %\"gelu_18\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_12_split_1\") {approximate='none'}\n",
       "            1591 |  # node_mul_15\n",
       "                    %\"mul_15\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_12_split_0\", %\"gelu_18\")\n",
       "            1592 |  # node_MatMul_5013\n",
       "                    %\"val_5109\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_15\", %\"val_5108\"{...})\n",
       "            1593 |  # node_linear_174\n",
       "                    %\"linear_174\"<FLOAT,[8,256,640]>  ::Add(%\"val_5109\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1594 |  # node_add_98\n",
       "                    %\"add_98\"<FLOAT,[8,256,640]>  ::Add(%\"linear_174\", %\"add_97\")\n",
       "            1595 |  # node_view_146\n",
       "                    %\"view_146\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_98\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1596 |  # node_permute_26\n",
       "                    %\"permute_26\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_146\") {perm=(0, 3, 1, 2)}\n",
       "            1597 |  # node_conv2d_80\n",
       "                    %\"conv2d_80\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_26\", %\"unet.up_blocks.2.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1598 |  # node_add_99\n",
       "                    %\"add_99\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_80\", %\"add_95\")\n",
       "            1599 |  # node_upsample_nearest2d_2\n",
       "                    %\"upsample_nearest2d_2\"<FLOAT,[8,640,32,32]>  ::Resize(%\"add_99\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1600 |  # node_conv2d_81\n",
       "                    %\"conv2d_81\"<FLOAT,[8,640,32,32]>  ::Conv(%\"upsample_nearest2d_2\", %\"unet.up_blocks.2.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.2.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1601 |  # node_cat_14\n",
       "                    %\"cat_14\"<FLOAT,[8,960,32,32]>  ::Concat(%\"conv2d_81\", %\"add_21\") {axis=1}\n",
       "            1602 |  # node_Reshape_5025\n",
       "                    %\"val_5121\"<FLOAT,[8,32,30720]>  ::Reshape(%\"cat_14\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1603 |  # node_InstanceNormalization_5032\n",
       "                    %\"val_5128\"<FLOAT,[8,32,30720]>  ::InstanceNormalization(%\"val_5121\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1604 |  # node_Reshape_5034\n",
       "                    %\"val_5130\"<FLOAT,[8,960,32,32]>  ::Reshape(%\"val_5128\", %\"val_5129\"{[8, 960, 32, 32]}) {allowzero=0}\n",
       "            1605 |  # node_Mul_5041\n",
       "                    %\"val_5137\"<FLOAT,[8,960,32,32]>  ::Mul(%\"val_5130\", %\"val_5136\"{...})\n",
       "            1606 |  # node_group_norm_51\n",
       "                    %\"group_norm_51\"<FLOAT,[8,960,32,32]>  ::Add(%\"val_5137\", %\"val_5138\"{...})\n",
       "            1607 |  # node_Sigmoid_5043\n",
       "                    %\"val_5139\"<FLOAT,[8,960,32,32]>  ::Sigmoid(%\"group_norm_51\")\n",
       "            1608 |  # node_silu_58\n",
       "                    %\"silu_58\"<FLOAT,[8,960,32,32]>  ::Mul(%\"group_norm_51\", %\"val_5139\")\n",
       "            1609 |  # node_conv2d_82\n",
       "                    %\"conv2d_82\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_58\", %\"unet.up_blocks.3.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1610 |  # node_linear_175\n",
       "                    %\"linear_175\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1611 |  # node_Unsqueeze_7800\n",
       "                    %\"unsqueeze_41\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_175\", %\"val_6521\"{[2, 3]})\n",
       "            1612 |  # node_add_100\n",
       "                    %\"add_100\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_82\", %\"unsqueeze_41\")\n",
       "            1613 |  # node_Reshape_5049\n",
       "                    %\"val_5145\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_100\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1614 |  # node_InstanceNormalization_5056\n",
       "                    %\"val_5152\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5145\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1615 |  # node_Reshape_5058\n",
       "                    %\"val_5154\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5152\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1616 |  # node_Mul_5065\n",
       "                    %\"val_5161\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5154\", %\"val_5160\"{...})\n",
       "            1617 |  # node_group_norm_52\n",
       "                    %\"group_norm_52\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5161\", %\"val_5162\"{...})\n",
       "            1618 |  # node_Sigmoid_5067\n",
       "                    %\"val_5163\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_52\")\n",
       "            1619 |  # node_silu_60\n",
       "                    %\"silu_60\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_52\", %\"val_5163\")\n",
       "            1620 |  # node_conv2d_83\n",
       "                    %\"conv2d_83\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_60\", %\"unet.up_blocks.3.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1621 |  # node_conv2d_84\n",
       "                    %\"conv2d_84\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_14\", %\"unet.up_blocks.3.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1622 |  # node_add_101\n",
       "                    %\"add_101\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_84\", %\"conv2d_83\")\n",
       "            1623 |  # node_Reshape_5072\n",
       "                    %\"val_5168\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_101\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1624 |  # node_InstanceNormalization_5079\n",
       "                    %\"val_5175\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5168\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1625 |  # node_Reshape_5081\n",
       "                    %\"val_5177\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5175\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1626 |  # node_Mul_5088\n",
       "                    %\"val_5184\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5177\", %\"val_5183\"{...})\n",
       "            1627 |  # node_group_norm_53\n",
       "                    %\"group_norm_53\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5184\", %\"val_5185\"{...})\n",
       "            1628 |  # node_conv2d_85\n",
       "                    %\"conv2d_85\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_53\", %\"unet.up_blocks.3.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1629 |  # node_permute_27\n",
       "                    %\"permute_27\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_85\") {perm=(0, 2, 3, 1)}\n",
       "            1630 |  # node_view_147\n",
       "                    %\"view_147\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_27\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1631 |  # node_layer_norm_48\n",
       "                    %\"layer_norm_48\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_147\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1632 |  # node_linear_176\n",
       "                    %\"linear_176\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5193\"{...})\n",
       "            1633 |  # node_linear_177\n",
       "                    %\"linear_177\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5194\"{...})\n",
       "            1634 |  # node_linear_178\n",
       "                    %\"linear_178\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5195\"{...})\n",
       "            1635 |  # node_view_148\n",
       "                    %\"view_148\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_176\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1636 |  # node_transpose_120\n",
       "                    %\"transpose_120\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_148\") {perm=(0, 2, 1, 3)}\n",
       "            1637 |  # node_view_149\n",
       "                    %\"view_149\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_177\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1638 |  # node_transpose_121\n",
       "                    %\"transpose_121\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_149\") {perm=(0, 2, 1, 3)}\n",
       "            1639 |  # node_view_150\n",
       "                    %\"view_150\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_178\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1640 |  # node_transpose_122\n",
       "                    %\"transpose_122\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_150\") {perm=(0, 2, 1, 3)}\n",
       "            1641 |  # node_Reshape_5133\n",
       "                    %\"val_5231\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_121\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1642 |  # node_Transpose_5134\n",
       "                    %\"val_5232\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5231\") {perm=(0, 2, 1)}\n",
       "            1643 |  # node_Reshape_5136\n",
       "                    %\"val_5234\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5232\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1644 |  # node_Mul_5138\n",
       "                    %\"val_5236\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_120\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1645 |  # node_Mul_5141\n",
       "                    %\"val_5239\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5234\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1646 |  # node_MatMul_5142\n",
       "                    %\"val_5240\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5236\", %\"val_5239\")\n",
       "            1647 |  # node_Softmax_5143\n",
       "                    %\"val_5241\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5240\") {axis=-1}\n",
       "            1648 |  # node_scaled_dot_product_attention_30\n",
       "                    %\"scaled_dot_product_attention_30\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5241\", %\"transpose_122\")\n",
       "            1649 |  # node_transpose_123\n",
       "                    %\"transpose_123\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_30\") {perm=(0, 2, 1, 3)}\n",
       "            1650 |  # node_view_151\n",
       "                    %\"view_151\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_123\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1651 |  # node_MatMul_5150\n",
       "                    %\"val_5248\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_151\", %\"val_5247\"{...})\n",
       "            1652 |  # node_linear_179\n",
       "                    %\"linear_179\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5248\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1653 |  # node_add_102\n",
       "                    %\"add_102\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_179\", %\"view_147\")\n",
       "            1654 |  # node_layer_norm_49\n",
       "                    %\"layer_norm_49\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_102\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1655 |  # node_linear_180\n",
       "                    %\"linear_180\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_49\", %\"val_5251\"{...})\n",
       "            1656 |  # node_linear_181\n",
       "                    %\"linear_181\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5252\"{...})\n",
       "            1657 |  # node_linear_182\n",
       "                    %\"linear_182\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5253\"{...})\n",
       "            1658 |  # node_view_152\n",
       "                    %\"view_152\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_180\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1659 |  # node_transpose_124\n",
       "                    %\"transpose_124\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_152\") {perm=(0, 2, 1, 3)}\n",
       "            1660 |  # node_view_153\n",
       "                    %\"view_153\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_181\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1661 |  # node_transpose_125\n",
       "                    %\"transpose_125\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_153\") {perm=(0, 2, 1, 3)}\n",
       "            1662 |  # node_view_154\n",
       "                    %\"view_154\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_182\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1663 |  # node_transpose_126\n",
       "                    %\"transpose_126\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_154\") {perm=(0, 2, 1, 3)}\n",
       "            1664 |  # node_Reshape_5189\n",
       "                    %\"val_5289\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_125\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1665 |  # node_Transpose_5190\n",
       "                    %\"val_5290\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5289\") {perm=(0, 2, 1)}\n",
       "            1666 |  # node_Reshape_5192\n",
       "                    %\"val_5292\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5290\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1667 |  # node_Mul_5194\n",
       "                    %\"val_5294\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_124\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1668 |  # node_Mul_5197\n",
       "                    %\"val_5297\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5292\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1669 |  # node_MatMul_5198\n",
       "                    %\"val_5298\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5294\", %\"val_5297\")\n",
       "            1670 |  # node_Softmax_5199\n",
       "                    %\"val_5299\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5298\") {axis=-1}\n",
       "            1671 |  # node_scaled_dot_product_attention_31\n",
       "                    %\"scaled_dot_product_attention_31\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5299\", %\"transpose_126\")\n",
       "            1672 |  # node_transpose_127\n",
       "                    %\"transpose_127\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_31\") {perm=(0, 2, 1, 3)}\n",
       "            1673 |  # node_view_155\n",
       "                    %\"view_155\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_127\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1674 |  # node_MatMul_5206\n",
       "                    %\"val_5306\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_155\", %\"val_5305\"{...})\n",
       "            1675 |  # node_linear_183\n",
       "                    %\"linear_183\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5306\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1676 |  # node_add_103\n",
       "                    %\"add_103\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_183\", %\"add_102\")\n",
       "            1677 |  # node_layer_norm_50\n",
       "                    %\"layer_norm_50\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_103\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1678 |  # node_MatMul_5208\n",
       "                    %\"val_5310\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_50\", %\"val_5309\"{...})\n",
       "            1679 |  # node_linear_184\n",
       "                    %\"linear_184\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5310\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1680 |  # node_Split_7480\n",
       "                    %\"split_13_split_0\"<FLOAT,[8,1024,1280]>, %\"split_13_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_184\") {axis=2, num_outputs=2}\n",
       "            1681 |  # node_gelu_19\n",
       "                    %\"gelu_19\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_13_split_1\") {approximate='none'}\n",
       "            1682 |  # node_mul_16\n",
       "                    %\"mul_16\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_13_split_0\", %\"gelu_19\")\n",
       "            1683 |  # node_MatMul_5210\n",
       "                    %\"val_5312\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_16\", %\"val_5311\"{...})\n",
       "            1684 |  # node_linear_185\n",
       "                    %\"linear_185\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5312\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1685 |  # node_add_104\n",
       "                    %\"add_104\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_185\", %\"add_103\")\n",
       "            1686 |  # node_view_156\n",
       "                    %\"view_156\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_104\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1687 |  # node_permute_28\n",
       "                    %\"permute_28\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_156\") {perm=(0, 3, 1, 2)}\n",
       "            1688 |  # node_conv2d_86\n",
       "                    %\"conv2d_86\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_28\", %\"unet.up_blocks.3.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1689 |  # node_add_105\n",
       "                    %\"add_105\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_86\", %\"add_101\")\n",
       "            1690 |  # node_cat_15\n",
       "                    %\"cat_15\"<FLOAT,[8,640,32,32]>  ::Concat(%\"add_105\", %\"add_15\") {axis=1}\n",
       "            1691 |  # node_Reshape_5221\n",
       "                    %\"val_5323\"<FLOAT,[8,32,20480]>  ::Reshape(%\"cat_15\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1692 |  # node_InstanceNormalization_5228\n",
       "                    %\"val_5330\"<FLOAT,[8,32,20480]>  ::InstanceNormalization(%\"val_5323\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1693 |  # node_Reshape_5230\n",
       "                    %\"val_5332\"<FLOAT,[8,640,32,32]>  ::Reshape(%\"val_5330\", %\"val_5331\"{[8, 640, 32, 32]}) {allowzero=0}\n",
       "            1694 |  # node_Mul_5237\n",
       "                    %\"val_5339\"<FLOAT,[8,640,32,32]>  ::Mul(%\"val_5332\", %\"val_5338\"{...})\n",
       "            1695 |  # node_group_norm_54\n",
       "                    %\"group_norm_54\"<FLOAT,[8,640,32,32]>  ::Add(%\"val_5339\", %\"val_5340\"{...})\n",
       "            1696 |  # node_Sigmoid_5239\n",
       "                    %\"val_5341\"<FLOAT,[8,640,32,32]>  ::Sigmoid(%\"group_norm_54\")\n",
       "            1697 |  # node_silu_61\n",
       "                    %\"silu_61\"<FLOAT,[8,640,32,32]>  ::Mul(%\"group_norm_54\", %\"val_5341\")\n",
       "            1698 |  # node_conv2d_87\n",
       "                    %\"conv2d_87\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_61\", %\"unet.up_blocks.3.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1699 |  # node_linear_186\n",
       "                    %\"linear_186\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1700 |  # node_Unsqueeze_7805\n",
       "                    %\"unsqueeze_43\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_186\", %\"val_6521\"{[2, 3]})\n",
       "            1701 |  # node_add_106\n",
       "                    %\"add_106\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_87\", %\"unsqueeze_43\")\n",
       "            1702 |  # node_Reshape_5245\n",
       "                    %\"val_5347\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_106\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1703 |  # node_InstanceNormalization_5252\n",
       "                    %\"val_5354\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5347\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1704 |  # node_Reshape_5254\n",
       "                    %\"val_5356\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5354\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1705 |  # node_Mul_5261\n",
       "                    %\"val_5363\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5356\", %\"val_5362\"{...})\n",
       "            1706 |  # node_group_norm_55\n",
       "                    %\"group_norm_55\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5363\", %\"val_5364\"{...})\n",
       "            1707 |  # node_Sigmoid_5263\n",
       "                    %\"val_5365\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_55\")\n",
       "            1708 |  # node_silu_63\n",
       "                    %\"silu_63\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_55\", %\"val_5365\")\n",
       "            1709 |  # node_conv2d_88\n",
       "                    %\"conv2d_88\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_63\", %\"unet.up_blocks.3.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1710 |  # node_conv2d_89\n",
       "                    %\"conv2d_89\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_15\", %\"unet.up_blocks.3.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1711 |  # node_add_107\n",
       "                    %\"add_107\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_89\", %\"conv2d_88\")\n",
       "            1712 |  # node_Reshape_5268\n",
       "                    %\"val_5370\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_107\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1713 |  # node_InstanceNormalization_5275\n",
       "                    %\"val_5377\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5370\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1714 |  # node_Reshape_5277\n",
       "                    %\"val_5379\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5377\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1715 |  # node_Mul_5284\n",
       "                    %\"val_5386\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5379\", %\"val_5385\"{...})\n",
       "            1716 |  # node_group_norm_56\n",
       "                    %\"group_norm_56\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5386\", %\"val_5387\"{...})\n",
       "            1717 |  # node_conv2d_90\n",
       "                    %\"conv2d_90\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_56\", %\"unet.up_blocks.3.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1718 |  # node_permute_29\n",
       "                    %\"permute_29\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_90\") {perm=(0, 2, 3, 1)}\n",
       "            1719 |  # node_view_157\n",
       "                    %\"view_157\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_29\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1720 |  # node_layer_norm_51\n",
       "                    %\"layer_norm_51\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_157\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1721 |  # node_linear_187\n",
       "                    %\"linear_187\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5395\"{...})\n",
       "            1722 |  # node_linear_188\n",
       "                    %\"linear_188\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5396\"{...})\n",
       "            1723 |  # node_linear_189\n",
       "                    %\"linear_189\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5397\"{...})\n",
       "            1724 |  # node_view_158\n",
       "                    %\"view_158\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_187\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1725 |  # node_transpose_128\n",
       "                    %\"transpose_128\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_158\") {perm=(0, 2, 1, 3)}\n",
       "            1726 |  # node_view_159\n",
       "                    %\"view_159\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_188\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1727 |  # node_transpose_129\n",
       "                    %\"transpose_129\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_159\") {perm=(0, 2, 1, 3)}\n",
       "            1728 |  # node_view_160\n",
       "                    %\"view_160\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_189\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1729 |  # node_transpose_130\n",
       "                    %\"transpose_130\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_160\") {perm=(0, 2, 1, 3)}\n",
       "            1730 |  # node_Reshape_5329\n",
       "                    %\"val_5433\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_129\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1731 |  # node_Transpose_5330\n",
       "                    %\"val_5434\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5433\") {perm=(0, 2, 1)}\n",
       "            1732 |  # node_Reshape_5332\n",
       "                    %\"val_5436\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5434\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1733 |  # node_Mul_5334\n",
       "                    %\"val_5438\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_128\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1734 |  # node_Mul_5337\n",
       "                    %\"val_5441\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5436\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1735 |  # node_MatMul_5338\n",
       "                    %\"val_5442\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5438\", %\"val_5441\")\n",
       "            1736 |  # node_Softmax_5339\n",
       "                    %\"val_5443\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5442\") {axis=-1}\n",
       "            1737 |  # node_scaled_dot_product_attention_32\n",
       "                    %\"scaled_dot_product_attention_32\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5443\", %\"transpose_130\")\n",
       "            1738 |  # node_transpose_131\n",
       "                    %\"transpose_131\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_32\") {perm=(0, 2, 1, 3)}\n",
       "            1739 |  # node_view_161\n",
       "                    %\"view_161\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_131\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1740 |  # node_MatMul_5346\n",
       "                    %\"val_5450\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_161\", %\"val_5449\"{...})\n",
       "            1741 |  # node_linear_190\n",
       "                    %\"linear_190\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5450\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1742 |  # node_add_108\n",
       "                    %\"add_108\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_190\", %\"view_157\")\n",
       "            1743 |  # node_layer_norm_52\n",
       "                    %\"layer_norm_52\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_108\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1744 |  # node_linear_191\n",
       "                    %\"linear_191\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_52\", %\"val_5453\"{...})\n",
       "            1745 |  # node_linear_192\n",
       "                    %\"linear_192\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5454\"{...})\n",
       "            1746 |  # node_linear_193\n",
       "                    %\"linear_193\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5455\"{...})\n",
       "            1747 |  # node_view_162\n",
       "                    %\"view_162\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_191\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1748 |  # node_transpose_132\n",
       "                    %\"transpose_132\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_162\") {perm=(0, 2, 1, 3)}\n",
       "            1749 |  # node_view_163\n",
       "                    %\"view_163\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_192\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1750 |  # node_transpose_133\n",
       "                    %\"transpose_133\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_163\") {perm=(0, 2, 1, 3)}\n",
       "            1751 |  # node_view_164\n",
       "                    %\"view_164\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_193\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1752 |  # node_transpose_134\n",
       "                    %\"transpose_134\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_164\") {perm=(0, 2, 1, 3)}\n",
       "            1753 |  # node_Reshape_5385\n",
       "                    %\"val_5491\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_133\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1754 |  # node_Transpose_5386\n",
       "                    %\"val_5492\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5491\") {perm=(0, 2, 1)}\n",
       "            1755 |  # node_Reshape_5388\n",
       "                    %\"val_5494\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5492\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1756 |  # node_Mul_5390\n",
       "                    %\"val_5496\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_132\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1757 |  # node_Mul_5393\n",
       "                    %\"val_5499\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5494\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1758 |  # node_MatMul_5394\n",
       "                    %\"val_5500\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5496\", %\"val_5499\")\n",
       "            1759 |  # node_Softmax_5395\n",
       "                    %\"val_5501\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5500\") {axis=-1}\n",
       "            1760 |  # node_scaled_dot_product_attention_33\n",
       "                    %\"scaled_dot_product_attention_33\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5501\", %\"transpose_134\")\n",
       "            1761 |  # node_transpose_135\n",
       "                    %\"transpose_135\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_33\") {perm=(0, 2, 1, 3)}\n",
       "            1762 |  # node_view_165\n",
       "                    %\"view_165\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_135\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1763 |  # node_MatMul_5402\n",
       "                    %\"val_5508\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_165\", %\"val_5507\"{...})\n",
       "            1764 |  # node_linear_194\n",
       "                    %\"linear_194\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5508\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1765 |  # node_add_109\n",
       "                    %\"add_109\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_194\", %\"add_108\")\n",
       "            1766 |  # node_layer_norm_53\n",
       "                    %\"layer_norm_53\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_109\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1767 |  # node_MatMul_5404\n",
       "                    %\"val_5512\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_53\", %\"val_5511\"{...})\n",
       "            1768 |  # node_linear_195\n",
       "                    %\"linear_195\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5512\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1769 |  # node_Split_7513\n",
       "                    %\"split_14_split_0\"<FLOAT,[8,1024,1280]>, %\"split_14_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_195\") {axis=2, num_outputs=2}\n",
       "            1770 |  # node_gelu_20\n",
       "                    %\"gelu_20\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_14_split_1\") {approximate='none'}\n",
       "            1771 |  # node_mul_17\n",
       "                    %\"mul_17\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_14_split_0\", %\"gelu_20\")\n",
       "            1772 |  # node_MatMul_5406\n",
       "                    %\"val_5514\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_17\", %\"val_5513\"{...})\n",
       "            1773 |  # node_linear_196\n",
       "                    %\"linear_196\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5514\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1774 |  # node_add_110\n",
       "                    %\"add_110\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_196\", %\"add_109\")\n",
       "            1775 |  # node_view_166\n",
       "                    %\"view_166\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_110\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1776 |  # node_permute_30\n",
       "                    %\"permute_30\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_166\") {perm=(0, 3, 1, 2)}\n",
       "            1777 |  # node_conv2d_91\n",
       "                    %\"conv2d_91\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_30\", %\"unet.up_blocks.3.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1778 |  # node_add_111\n",
       "                    %\"add_111\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_91\", %\"add_107\")\n",
       "            1779 |  # node_cat_16\n",
       "                    %\"cat_16\"<FLOAT,[8,640,32,32]>  ::Concat(%\"add_111\", %\"conv2d\") {axis=1}\n",
       "            1780 |  # node_Reshape_5417\n",
       "                    %\"val_5525\"<FLOAT,[8,32,20480]>  ::Reshape(%\"cat_16\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1781 |  # node_InstanceNormalization_5424\n",
       "                    %\"val_5532\"<FLOAT,[8,32,20480]>  ::InstanceNormalization(%\"val_5525\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1782 |  # node_Reshape_5426\n",
       "                    %\"val_5534\"<FLOAT,[8,640,32,32]>  ::Reshape(%\"val_5532\", %\"val_5331\"{[8, 640, 32, 32]}) {allowzero=0}\n",
       "            1783 |  # node_Mul_5433\n",
       "                    %\"val_5541\"<FLOAT,[8,640,32,32]>  ::Mul(%\"val_5534\", %\"val_5540\"{...})\n",
       "            1784 |  # node_group_norm_57\n",
       "                    %\"group_norm_57\"<FLOAT,[8,640,32,32]>  ::Add(%\"val_5541\", %\"val_5542\"{...})\n",
       "            1785 |  # node_Sigmoid_5435\n",
       "                    %\"val_5543\"<FLOAT,[8,640,32,32]>  ::Sigmoid(%\"group_norm_57\")\n",
       "            1786 |  # node_silu_64\n",
       "                    %\"silu_64\"<FLOAT,[8,640,32,32]>  ::Mul(%\"group_norm_57\", %\"val_5543\")\n",
       "            1787 |  # node_conv2d_92\n",
       "                    %\"conv2d_92\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_64\", %\"unet.up_blocks.3.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1788 |  # node_linear_197\n",
       "                    %\"linear_197\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1789 |  # node_Unsqueeze_7810\n",
       "                    %\"unsqueeze_45\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_197\", %\"val_6521\"{[2, 3]})\n",
       "            1790 |  # node_add_112\n",
       "                    %\"add_112\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_92\", %\"unsqueeze_45\")\n",
       "            1791 |  # node_Reshape_5441\n",
       "                    %\"val_5549\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_112\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1792 |  # node_InstanceNormalization_5448\n",
       "                    %\"val_5556\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5549\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1793 |  # node_Reshape_5450\n",
       "                    %\"val_5558\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5556\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1794 |  # node_Mul_5457\n",
       "                    %\"val_5565\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5558\", %\"val_5564\"{...})\n",
       "            1795 |  # node_group_norm_58\n",
       "                    %\"group_norm_58\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5565\", %\"val_5566\"{...})\n",
       "            1796 |  # node_Sigmoid_5459\n",
       "                    %\"val_5567\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_58\")\n",
       "            1797 |  # node_silu_66\n",
       "                    %\"silu_66\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_58\", %\"val_5567\")\n",
       "            1798 |  # node_conv2d_93\n",
       "                    %\"conv2d_93\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_66\", %\"unet.up_blocks.3.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1799 |  # node_conv2d_94\n",
       "                    %\"conv2d_94\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_16\", %\"unet.up_blocks.3.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1800 |  # node_add_113\n",
       "                    %\"add_113\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_94\", %\"conv2d_93\")\n",
       "            1801 |  # node_Reshape_5464\n",
       "                    %\"val_5572\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_113\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1802 |  # node_InstanceNormalization_5471\n",
       "                    %\"val_5579\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5572\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1803 |  # node_Reshape_5473\n",
       "                    %\"val_5581\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5579\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1804 |  # node_Mul_5480\n",
       "                    %\"val_5588\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5581\", %\"val_5587\"{...})\n",
       "            1805 |  # node_group_norm_59\n",
       "                    %\"group_norm_59\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5588\", %\"val_5589\"{...})\n",
       "            1806 |  # node_conv2d_95\n",
       "                    %\"conv2d_95\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_59\", %\"unet.up_blocks.3.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1807 |  # node_permute_31\n",
       "                    %\"permute_31\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_95\") {perm=(0, 2, 3, 1)}\n",
       "            1808 |  # node_view_167\n",
       "                    %\"view_167\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_31\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1809 |  # node_layer_norm_54\n",
       "                    %\"layer_norm_54\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_167\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1810 |  # node_linear_198\n",
       "                    %\"linear_198\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5597\"{...})\n",
       "            1811 |  # node_linear_199\n",
       "                    %\"linear_199\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5598\"{...})\n",
       "            1812 |  # node_linear_200\n",
       "                    %\"linear_200\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5599\"{...})\n",
       "            1813 |  # node_view_168\n",
       "                    %\"view_168\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_198\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1814 |  # node_transpose_136\n",
       "                    %\"transpose_136\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_168\") {perm=(0, 2, 1, 3)}\n",
       "            1815 |  # node_view_169\n",
       "                    %\"view_169\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_199\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1816 |  # node_transpose_137\n",
       "                    %\"transpose_137\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_169\") {perm=(0, 2, 1, 3)}\n",
       "            1817 |  # node_view_170\n",
       "                    %\"view_170\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_200\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1818 |  # node_transpose_138\n",
       "                    %\"transpose_138\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_170\") {perm=(0, 2, 1, 3)}\n",
       "            1819 |  # node_Reshape_5525\n",
       "                    %\"val_5635\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_137\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1820 |  # node_Transpose_5526\n",
       "                    %\"val_5636\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5635\") {perm=(0, 2, 1)}\n",
       "            1821 |  # node_Reshape_5528\n",
       "                    %\"val_5638\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5636\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1822 |  # node_Mul_5530\n",
       "                    %\"val_5640\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_136\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1823 |  # node_Mul_5533\n",
       "                    %\"val_5643\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5638\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1824 |  # node_MatMul_5534\n",
       "                    %\"val_5644\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5640\", %\"val_5643\")\n",
       "            1825 |  # node_Softmax_5535\n",
       "                    %\"val_5645\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5644\") {axis=-1}\n",
       "            1826 |  # node_scaled_dot_product_attention_34\n",
       "                    %\"scaled_dot_product_attention_34\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5645\", %\"transpose_138\")\n",
       "            1827 |  # node_transpose_139\n",
       "                    %\"transpose_139\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_34\") {perm=(0, 2, 1, 3)}\n",
       "            1828 |  # node_view_171\n",
       "                    %\"view_171\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_139\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1829 |  # node_MatMul_5542\n",
       "                    %\"val_5652\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_171\", %\"val_5651\"{...})\n",
       "            1830 |  # node_linear_201\n",
       "                    %\"linear_201\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5652\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1831 |  # node_add_114\n",
       "                    %\"add_114\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_201\", %\"view_167\")\n",
       "            1832 |  # node_layer_norm_55\n",
       "                    %\"layer_norm_55\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_114\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1833 |  # node_linear_202\n",
       "                    %\"linear_202\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_55\", %\"val_5655\"{...})\n",
       "            1834 |  # node_linear_203\n",
       "                    %\"linear_203\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5656\"{...})\n",
       "            1835 |  # node_linear_204\n",
       "                    %\"linear_204\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5657\"{...})\n",
       "            1836 |  # node_view_172\n",
       "                    %\"view_172\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_202\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1837 |  # node_transpose_140\n",
       "                    %\"transpose_140\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_172\") {perm=(0, 2, 1, 3)}\n",
       "            1838 |  # node_view_173\n",
       "                    %\"view_173\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_203\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1839 |  # node_transpose_141\n",
       "                    %\"transpose_141\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_173\") {perm=(0, 2, 1, 3)}\n",
       "            1840 |  # node_view_174\n",
       "                    %\"view_174\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_204\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1841 |  # node_transpose_142\n",
       "                    %\"transpose_142\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_174\") {perm=(0, 2, 1, 3)}\n",
       "            1842 |  # node_Reshape_5581\n",
       "                    %\"val_5693\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_141\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1843 |  # node_Transpose_5582\n",
       "                    %\"val_5694\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5693\") {perm=(0, 2, 1)}\n",
       "            1844 |  # node_Reshape_5584\n",
       "                    %\"val_5696\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5694\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1845 |  # node_Mul_5586\n",
       "                    %\"val_5698\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_140\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1846 |  # node_Mul_5589\n",
       "                    %\"val_5701\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5696\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1847 |  # node_MatMul_5590\n",
       "                    %\"val_5702\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5698\", %\"val_5701\")\n",
       "            1848 |  # node_Softmax_5591\n",
       "                    %\"val_5703\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5702\") {axis=-1}\n",
       "            1849 |  # node_scaled_dot_product_attention_35\n",
       "                    %\"scaled_dot_product_attention_35\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5703\", %\"transpose_142\")\n",
       "            1850 |  # node_transpose_143\n",
       "                    %\"transpose_143\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_35\") {perm=(0, 2, 1, 3)}\n",
       "            1851 |  # node_view_175\n",
       "                    %\"view_175\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_143\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1852 |  # node_MatMul_5598\n",
       "                    %\"val_5710\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_175\", %\"val_5709\"{...})\n",
       "            1853 |  # node_linear_205\n",
       "                    %\"linear_205\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5710\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1854 |  # node_add_115\n",
       "                    %\"add_115\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_205\", %\"add_114\")\n",
       "            1855 |  # node_layer_norm_56\n",
       "                    %\"layer_norm_56\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_115\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1856 |  # node_MatMul_5600\n",
       "                    %\"val_5714\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_56\", %\"val_5713\"{...})\n",
       "            1857 |  # node_linear_206\n",
       "                    %\"linear_206\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5714\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1858 |  # node_Split_7546\n",
       "                    %\"split_15_split_0\"<FLOAT,[8,1024,1280]>, %\"split_15_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_206\") {axis=2, num_outputs=2}\n",
       "            1859 |  # node_gelu_21\n",
       "                    %\"gelu_21\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_15_split_1\") {approximate='none'}\n",
       "            1860 |  # node_mul_18\n",
       "                    %\"mul_18\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_15_split_0\", %\"gelu_21\")\n",
       "            1861 |  # node_MatMul_5602\n",
       "                    %\"val_5716\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_18\", %\"val_5715\"{...})\n",
       "            1862 |  # node_linear_207\n",
       "                    %\"linear_207\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5716\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1863 |  # node_add_116\n",
       "                    %\"add_116\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_207\", %\"add_115\")\n",
       "            1864 |  # node_view_176\n",
       "                    %\"view_176\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_116\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1865 |  # node_permute_32\n",
       "                    %\"permute_32\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_176\") {perm=(0, 3, 1, 2)}\n",
       "            1866 |  # node_conv2d_96\n",
       "                    %\"conv2d_96\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_32\", %\"unet.up_blocks.3.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1867 |  # node_add_117\n",
       "                    %\"add_117\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_96\", %\"add_113\")\n",
       "            1868 |  # node_Reshape_5613\n",
       "                    %\"val_5727\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_117\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1869 |  # node_InstanceNormalization_5620\n",
       "                    %\"val_5734\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5727\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1870 |  # node_Reshape_5622\n",
       "                    %\"val_5736\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5734\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1871 |  # node_Mul_5629\n",
       "                    %\"val_5743\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5736\", %\"val_5742\"{...})\n",
       "            1872 |  # node_group_norm_60\n",
       "                    %\"group_norm_60\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5743\", %\"val_5744\"{...})\n",
       "            1873 |  # node_Sigmoid_5631\n",
       "                    %\"val_5745\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_60\")\n",
       "            1874 |  # node_silu_67\n",
       "                    %\"silu_67\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_60\", %\"val_5745\")\n",
       "            1875 |  # node_conv2d_97\n",
       "                    %\"conv2d_97\"<FLOAT,[8,4,32,32]>  ::Conv(%\"silu_67\", %\"unet.conv_out.weight\"{...}, %\"unet.conv_out.bias\"{[-0.006085909903049469, -0.00038231629878282547, 0.006092327646911144, -0.01816507801413536]}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1876 |  # node_mul_19\n",
       "                    %\"mul_19\"<FLOAT,[8,4,32,32]>  ::Mul(%\"conv2d_97\", %\"val_5746\"{5.489980697631836})\n",
       "            1877 |  # node_conv2d_98\n",
       "                    %\"conv2d_98\"<FLOAT,[8,4,32,32]>  ::Conv(%\"mul_19\", %\"vae.post_quant_conv.weight\"{...}, %\"vae.post_quant_conv.bias\"{[0.032071199268102646, -0.08428296446800232, -0.24323134124279022, 0.13154643774032593]}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1878 |  # node_conv2d_99\n",
       "                    %\"conv2d_99\"<FLOAT,[8,512,32,32]>  ::Conv(%\"conv2d_98\", %\"vae.decoder.conv_in.weight\"{...}, %\"vae.decoder.conv_in.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1879 |  # node_Reshape_5637\n",
       "                    %\"val_5751\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_99\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1880 |  # node_InstanceNormalization_5644\n",
       "                    %\"val_5758\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5751\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1881 |  # node_Reshape_5646\n",
       "                    %\"val_5760\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5758\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1882 |  # node_Mul_5653\n",
       "                    %\"val_5767\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5760\", %\"val_5766\"{...})\n",
       "            1883 |  # node_group_norm_61\n",
       "                    %\"group_norm_61\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5767\", %\"val_5768\"{...})\n",
       "            1884 |  # node_Sigmoid_5655\n",
       "                    %\"val_5769\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_61\")\n",
       "            1885 |  # node_silu_68\n",
       "                    %\"silu_68\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_61\", %\"val_5769\")\n",
       "            1886 |  # node_conv2d_100\n",
       "                    %\"conv2d_100\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_68\", %\"vae.decoder.mid_block.resnets.0.conv1.weight\"{...}, %\"vae.decoder.mid_block.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1887 |  # node_Reshape_5660\n",
       "                    %\"val_5774\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_100\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1888 |  # node_InstanceNormalization_5667\n",
       "                    %\"val_5781\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5774\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1889 |  # node_Reshape_5669\n",
       "                    %\"val_5783\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5781\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1890 |  # node_Mul_5676\n",
       "                    %\"val_5790\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5783\", %\"val_5789\"{...})\n",
       "            1891 |  # node_group_norm_62\n",
       "                    %\"group_norm_62\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5790\", %\"val_5791\"{...})\n",
       "            1892 |  # node_Sigmoid_5678\n",
       "                    %\"val_5792\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_62\")\n",
       "            1893 |  # node_silu_69\n",
       "                    %\"silu_69\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_62\", %\"val_5792\")\n",
       "            1894 |  # node_conv2d_101\n",
       "                    %\"conv2d_101\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_69\", %\"vae.decoder.mid_block.resnets.0.conv2.weight\"{...}, %\"vae.decoder.mid_block.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1895 |  # node_add_118\n",
       "                    %\"add_118\"<FLOAT,[8,512,32,32]>  ::Add(%\"conv2d_99\", %\"conv2d_101\")\n",
       "            1896 |  # node_view_177\n",
       "                    %\"view_177\"<FLOAT,[8,512,1024]>  ::Reshape(%\"add_118\", %\"val_5797\"{[8, 512, 1024]}) {allowzero=1}\n",
       "            1897 |  # node_Reshape_5688\n",
       "                    %\"val_5802\"<FLOAT,[8,32,16384]>  ::Reshape(%\"view_177\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1898 |  # node_InstanceNormalization_5695\n",
       "                    %\"val_5809\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5802\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1899 |  # node_Reshape_5697\n",
       "                    %\"val_5811\"<FLOAT,[8,512,1024]>  ::Reshape(%\"val_5809\", %\"val_5797\"{[8, 512, 1024]}) {allowzero=0}\n",
       "            1900 |  # node_Mul_5705\n",
       "                    %\"val_5819\"<FLOAT,[8,512,1024]>  ::Mul(%\"val_5811\", %\"val_5818\"{...})\n",
       "            1901 |  # node_group_norm_63\n",
       "                    %\"group_norm_63\"<FLOAT,[8,512,1024]>  ::Add(%\"val_5819\", %\"val_5820\"{...})\n",
       "            1902 |  # node_transpose_146\n",
       "                    %\"transpose_146\"<FLOAT,[8,1024,512]>  ::Transpose(%\"group_norm_63\") {perm=(0, 2, 1)}\n",
       "            1903 |  # node_MatMul_5708\n",
       "                    %\"val_5822\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5821\"{...})\n",
       "            1904 |  # node_linear_208\n",
       "                    %\"linear_208\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5822\", %\"vae.decoder.mid_block.attentions.0.to_q.bias\"{...})\n",
       "            1905 |  # node_MatMul_5710\n",
       "                    %\"val_5824\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5823\"{...})\n",
       "            1906 |  # node_linear_209\n",
       "                    %\"linear_209\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5824\", %\"vae.decoder.mid_block.attentions.0.to_k.bias\"{...})\n",
       "            1907 |  # node_MatMul_5712\n",
       "                    %\"val_5826\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5825\"{...})\n",
       "            1908 |  # node_linear_210\n",
       "                    %\"linear_210\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5826\", %\"vae.decoder.mid_block.attentions.0.to_v.bias\"{...})\n",
       "            1909 |  # node_view_178\n",
       "                    %\"view_178\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_208\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1910 |  # node_transpose_147\n",
       "                    %\"transpose_147\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_178\") {perm=(0, 2, 1, 3)}\n",
       "            1911 |  # node_view_179\n",
       "                    %\"view_179\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_209\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1912 |  # node_transpose_148\n",
       "                    %\"transpose_148\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_179\") {perm=(0, 2, 1, 3)}\n",
       "            1913 |  # node_view_180\n",
       "                    %\"view_180\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_210\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1914 |  # node_transpose_149\n",
       "                    %\"transpose_149\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_180\") {perm=(0, 2, 1, 3)}\n",
       "            1915 |  # node_Reshape_5748\n",
       "                    %\"val_5862\"<FLOAT,[8,1024,512]>  ::Reshape(%\"transpose_148\", %\"val_5861\"{[-1, 1024, 512]}) {allowzero=0}\n",
       "            1916 |  # node_Transpose_5749\n",
       "                    %\"val_5863\"<FLOAT,[8,512,1024]>  ::Transpose(%\"val_5862\") {perm=(0, 2, 1)}\n",
       "            1917 |  # node_Reshape_5751\n",
       "                    %\"val_5865\"<FLOAT,[8,1,512,1024]>  ::Reshape(%\"val_5863\", %\"val_5864\"{[8, 1, 512, 1024]}) {allowzero=0}\n",
       "            1918 |  # node_Mul_5753\n",
       "                    %\"val_5867\"<FLOAT,[8,1,1024,512]>  ::Mul(%\"transpose_147\", %\"val_5866\"{[0.21022410690784454]})\n",
       "            1919 |  # node_Mul_5756\n",
       "                    %\"val_5870\"<FLOAT,[8,1,512,1024]>  ::Mul(%\"val_5865\", %\"val_5866\"{[0.21022410690784454]})\n",
       "            1920 |  # node_MatMul_5757\n",
       "                    %\"val_5871\"<FLOAT,[8,1,1024,1024]>  ::MatMul(%\"val_5867\", %\"val_5870\")\n",
       "            1921 |  # node_Softmax_5758\n",
       "                    %\"val_5872\"<FLOAT,[8,1,1024,1024]>  ::Softmax(%\"val_5871\") {axis=-1}\n",
       "            1922 |  # node_scaled_dot_product_attention_36\n",
       "                    %\"scaled_dot_product_attention_36\"<FLOAT,[8,1,1024,512]>  ::MatMul(%\"val_5872\", %\"transpose_149\")\n",
       "            1923 |  # node_transpose_150\n",
       "                    %\"transpose_150\"<FLOAT,[8,1024,1,512]>  ::Transpose(%\"scaled_dot_product_attention_36\") {perm=(0, 2, 1, 3)}\n",
       "            1924 |  # node_view_181\n",
       "                    %\"view_181\"<FLOAT,[8,1024,512]>  ::Reshape(%\"transpose_150\", %\"val_5877\"{[8, -1, 512]}) {allowzero=1}\n",
       "            1925 |  # node_MatMul_5765\n",
       "                    %\"val_5879\"<FLOAT,[8,1024,512]>  ::MatMul(%\"view_181\", %\"val_5878\"{...})\n",
       "            1926 |  # node_linear_211\n",
       "                    %\"linear_211\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5879\", %\"vae.decoder.mid_block.attentions.0.to_out.0.bias\"{...})\n",
       "            1927 |  # node_transpose_151\n",
       "                    %\"transpose_151\"<FLOAT,[8,512,1024]>  ::Transpose(%\"linear_211\") {perm=(0, 2, 1)}\n",
       "            1928 |  # node_view_182\n",
       "                    %\"view_182\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"transpose_151\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=1}\n",
       "            1929 |  # node_add_119\n",
       "                    %\"add_119\"<FLOAT,[8,512,32,32]>  ::Add(%\"view_182\", %\"add_118\")\n",
       "            1930 |  # node_Reshape_5776\n",
       "                    %\"val_5890\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_119\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1931 |  # node_InstanceNormalization_5783\n",
       "                    %\"val_5897\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5890\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1932 |  # node_Reshape_5785\n",
       "                    %\"val_5899\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5897\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1933 |  # node_Mul_5792\n",
       "                    %\"val_5906\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5899\", %\"val_5905\"{...})\n",
       "            1934 |  # node_group_norm_64\n",
       "                    %\"group_norm_64\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5906\", %\"val_5907\"{...})\n",
       "            1935 |  # node_Sigmoid_5794\n",
       "                    %\"val_5908\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_64\")\n",
       "            1936 |  # node_silu_70\n",
       "                    %\"silu_70\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_64\", %\"val_5908\")\n",
       "            1937 |  # node_conv2d_102\n",
       "                    %\"conv2d_102\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_70\", %\"vae.decoder.mid_block.resnets.1.conv1.weight\"{...}, %\"vae.decoder.mid_block.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1938 |  # node_Reshape_5799\n",
       "                    %\"val_5913\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_102\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1939 |  # node_InstanceNormalization_5806\n",
       "                    %\"val_5920\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5913\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1940 |  # node_Reshape_5808\n",
       "                    %\"val_5922\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5920\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1941 |  # node_Mul_5815\n",
       "                    %\"val_5929\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5922\", %\"val_5928\"{...})\n",
       "            1942 |  # node_group_norm_65\n",
       "                    %\"group_norm_65\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5929\", %\"val_5930\"{...})\n",
       "            1943 |  # node_Sigmoid_5817\n",
       "                    %\"val_5931\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_65\")\n",
       "            1944 |  # node_silu_71\n",
       "                    %\"silu_71\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_65\", %\"val_5931\")\n",
       "            1945 |  # node_conv2d_103\n",
       "                    %\"conv2d_103\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_71\", %\"vae.decoder.mid_block.resnets.1.conv2.weight\"{...}, %\"vae.decoder.mid_block.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1946 |  # node_add_120\n",
       "                    %\"add_120\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_119\", %\"conv2d_103\")\n",
       "            1947 |  # node_Reshape_5822\n",
       "                    %\"val_5936\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_120\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1948 |  # node_InstanceNormalization_5829\n",
       "                    %\"val_5943\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5936\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1949 |  # node_Reshape_5831\n",
       "                    %\"val_5945\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5943\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1950 |  # node_Mul_5838\n",
       "                    %\"val_5952\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5945\", %\"val_5951\"{...})\n",
       "            1951 |  # node_group_norm_66\n",
       "                    %\"group_norm_66\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5952\", %\"val_5953\"{...})\n",
       "            1952 |  # node_Sigmoid_5840\n",
       "                    %\"val_5954\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_66\")\n",
       "            1953 |  # node_silu_72\n",
       "                    %\"silu_72\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_66\", %\"val_5954\")\n",
       "            1954 |  # node_conv2d_104\n",
       "                    %\"conv2d_104\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_72\", %\"vae.decoder.up_blocks.0.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1955 |  # node_Reshape_5845\n",
       "                    %\"val_5959\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_104\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1956 |  # node_InstanceNormalization_5852\n",
       "                    %\"val_5966\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5959\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1957 |  # node_Reshape_5854\n",
       "                    %\"val_5968\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5966\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1958 |  # node_Mul_5861\n",
       "                    %\"val_5975\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5968\", %\"val_5974\"{...})\n",
       "            1959 |  # node_group_norm_67\n",
       "                    %\"group_norm_67\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5975\", %\"val_5976\"{...})\n",
       "            1960 |  # node_Sigmoid_5863\n",
       "                    %\"val_5977\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_67\")\n",
       "            1961 |  # node_silu_73\n",
       "                    %\"silu_73\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_67\", %\"val_5977\")\n",
       "            1962 |  # node_conv2d_105\n",
       "                    %\"conv2d_105\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_73\", %\"vae.decoder.up_blocks.0.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1963 |  # node_add_121\n",
       "                    %\"add_121\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_120\", %\"conv2d_105\")\n",
       "            1964 |  # node_Reshape_5868\n",
       "                    %\"val_5982\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_121\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1965 |  # node_InstanceNormalization_5875\n",
       "                    %\"val_5989\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5982\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1966 |  # node_Reshape_5877\n",
       "                    %\"val_5991\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5989\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1967 |  # node_Mul_5884\n",
       "                    %\"val_5998\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5991\", %\"val_5997\"{...})\n",
       "            1968 |  # node_group_norm_68\n",
       "                    %\"group_norm_68\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5998\", %\"val_5999\"{...})\n",
       "            1969 |  # node_Sigmoid_5886\n",
       "                    %\"val_6000\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_68\")\n",
       "            1970 |  # node_silu_74\n",
       "                    %\"silu_74\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_68\", %\"val_6000\")\n",
       "            1971 |  # node_conv2d_106\n",
       "                    %\"conv2d_106\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_74\", %\"vae.decoder.up_blocks.0.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1972 |  # node_Reshape_5891\n",
       "                    %\"val_6005\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_106\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1973 |  # node_InstanceNormalization_5898\n",
       "                    %\"val_6012\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6005\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1974 |  # node_Reshape_5900\n",
       "                    %\"val_6014\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6012\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1975 |  # node_Mul_5907\n",
       "                    %\"val_6021\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6014\", %\"val_6020\"{...})\n",
       "            1976 |  # node_group_norm_69\n",
       "                    %\"group_norm_69\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6021\", %\"val_6022\"{...})\n",
       "            1977 |  # node_Sigmoid_5909\n",
       "                    %\"val_6023\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_69\")\n",
       "            1978 |  # node_silu_75\n",
       "                    %\"silu_75\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_69\", %\"val_6023\")\n",
       "            1979 |  # node_conv2d_107\n",
       "                    %\"conv2d_107\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_75\", %\"vae.decoder.up_blocks.0.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1980 |  # node_add_122\n",
       "                    %\"add_122\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_121\", %\"conv2d_107\")\n",
       "            1981 |  # node_Reshape_5914\n",
       "                    %\"val_6028\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_122\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1982 |  # node_InstanceNormalization_5921\n",
       "                    %\"val_6035\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6028\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1983 |  # node_Reshape_5923\n",
       "                    %\"val_6037\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6035\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1984 |  # node_Mul_5930\n",
       "                    %\"val_6044\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6037\", %\"val_6043\"{...})\n",
       "            1985 |  # node_group_norm_70\n",
       "                    %\"group_norm_70\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6044\", %\"val_6045\"{...})\n",
       "            1986 |  # node_Sigmoid_5932\n",
       "                    %\"val_6046\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_70\")\n",
       "            1987 |  # node_silu_76\n",
       "                    %\"silu_76\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_70\", %\"val_6046\")\n",
       "            1988 |  # node_conv2d_108\n",
       "                    %\"conv2d_108\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_76\", %\"vae.decoder.up_blocks.0.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1989 |  # node_Reshape_5937\n",
       "                    %\"val_6051\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_108\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1990 |  # node_InstanceNormalization_5944\n",
       "                    %\"val_6058\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6051\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1991 |  # node_Reshape_5946\n",
       "                    %\"val_6060\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6058\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1992 |  # node_Mul_5953\n",
       "                    %\"val_6067\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6060\", %\"val_6066\"{...})\n",
       "            1993 |  # node_group_norm_71\n",
       "                    %\"group_norm_71\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6067\", %\"val_6068\"{...})\n",
       "            1994 |  # node_Sigmoid_5955\n",
       "                    %\"val_6069\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_71\")\n",
       "            1995 |  # node_silu_77\n",
       "                    %\"silu_77\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_71\", %\"val_6069\")\n",
       "            1996 |  # node_conv2d_109\n",
       "                    %\"conv2d_109\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_77\", %\"vae.decoder.up_blocks.0.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1997 |  # node_add_123\n",
       "                    %\"add_123\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_122\", %\"conv2d_109\")\n",
       "            1998 |  # node_upsample_nearest2d_3\n",
       "                    %\"upsample_nearest2d_3\"<FLOAT,[8,512,64,64]>  ::Resize(%\"add_123\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1999 |  # node_conv2d_110\n",
       "                    %\"conv2d_110\"<FLOAT,[8,512,64,64]>  ::Conv(%\"upsample_nearest2d_3\", %\"vae.decoder.up_blocks.0.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.0.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2000 |  # node_Reshape_5961\n",
       "                    %\"val_6075\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_110\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2001 |  # node_InstanceNormalization_5968\n",
       "                    %\"val_6082\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6075\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2002 |  # node_Reshape_5970\n",
       "                    %\"val_6084\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6082\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2003 |  # node_Mul_5977\n",
       "                    %\"val_6091\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6084\", %\"val_6090\"{...})\n",
       "            2004 |  # node_group_norm_72\n",
       "                    %\"group_norm_72\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6091\", %\"val_6092\"{...})\n",
       "            2005 |  # node_Sigmoid_5979\n",
       "                    %\"val_6093\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_72\")\n",
       "            2006 |  # node_silu_78\n",
       "                    %\"silu_78\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_72\", %\"val_6093\")\n",
       "            2007 |  # node_conv2d_111\n",
       "                    %\"conv2d_111\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_78\", %\"vae.decoder.up_blocks.1.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2008 |  # node_Reshape_5984\n",
       "                    %\"val_6098\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_111\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2009 |  # node_InstanceNormalization_5991\n",
       "                    %\"val_6105\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6098\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2010 |  # node_Reshape_5993\n",
       "                    %\"val_6107\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6105\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2011 |  # node_Mul_6000\n",
       "                    %\"val_6114\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6107\", %\"val_6113\"{...})\n",
       "            2012 |  # node_group_norm_73\n",
       "                    %\"group_norm_73\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6114\", %\"val_6115\"{...})\n",
       "            2013 |  # node_Sigmoid_6002\n",
       "                    %\"val_6116\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_73\")\n",
       "            2014 |  # node_silu_79\n",
       "                    %\"silu_79\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_73\", %\"val_6116\")\n",
       "            2015 |  # node_conv2d_112\n",
       "                    %\"conv2d_112\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_79\", %\"vae.decoder.up_blocks.1.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2016 |  # node_add_124\n",
       "                    %\"add_124\"<FLOAT,[8,512,64,64]>  ::Add(%\"conv2d_110\", %\"conv2d_112\")\n",
       "            2017 |  # node_Reshape_6007\n",
       "                    %\"val_6121\"<FLOAT,[8,32,65536]>  ::Reshape(%\"add_124\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2018 |  # node_InstanceNormalization_6014\n",
       "                    %\"val_6128\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6121\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2019 |  # node_Reshape_6016\n",
       "                    %\"val_6130\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6128\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2020 |  # node_Mul_6023\n",
       "                    %\"val_6137\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6130\", %\"val_6136\"{...})\n",
       "            2021 |  # node_group_norm_74\n",
       "                    %\"group_norm_74\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6137\", %\"val_6138\"{...})\n",
       "            2022 |  # node_Sigmoid_6025\n",
       "                    %\"val_6139\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_74\")\n",
       "            2023 |  # node_silu_80\n",
       "                    %\"silu_80\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_74\", %\"val_6139\")\n",
       "            2024 |  # node_conv2d_113\n",
       "                    %\"conv2d_113\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_80\", %\"vae.decoder.up_blocks.1.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2025 |  # node_Reshape_6030\n",
       "                    %\"val_6144\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_113\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2026 |  # node_InstanceNormalization_6037\n",
       "                    %\"val_6151\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6144\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2027 |  # node_Reshape_6039\n",
       "                    %\"val_6153\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6151\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2028 |  # node_Mul_6046\n",
       "                    %\"val_6160\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6153\", %\"val_6159\"{...})\n",
       "            2029 |  # node_group_norm_75\n",
       "                    %\"group_norm_75\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6160\", %\"val_6161\"{...})\n",
       "            2030 |  # node_Sigmoid_6048\n",
       "                    %\"val_6162\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_75\")\n",
       "            2031 |  # node_silu_81\n",
       "                    %\"silu_81\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_75\", %\"val_6162\")\n",
       "            2032 |  # node_conv2d_114\n",
       "                    %\"conv2d_114\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_81\", %\"vae.decoder.up_blocks.1.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2033 |  # node_add_125\n",
       "                    %\"add_125\"<FLOAT,[8,512,64,64]>  ::Add(%\"add_124\", %\"conv2d_114\")\n",
       "            2034 |  # node_Reshape_6053\n",
       "                    %\"val_6167\"<FLOAT,[8,32,65536]>  ::Reshape(%\"add_125\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2035 |  # node_InstanceNormalization_6060\n",
       "                    %\"val_6174\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6167\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2036 |  # node_Reshape_6062\n",
       "                    %\"val_6176\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6174\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2037 |  # node_Mul_6069\n",
       "                    %\"val_6183\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6176\", %\"val_6182\"{...})\n",
       "            2038 |  # node_group_norm_76\n",
       "                    %\"group_norm_76\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6183\", %\"val_6184\"{...})\n",
       "            2039 |  # node_Sigmoid_6071\n",
       "                    %\"val_6185\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_76\")\n",
       "            2040 |  # node_silu_82\n",
       "                    %\"silu_82\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_76\", %\"val_6185\")\n",
       "            2041 |  # node_conv2d_115\n",
       "                    %\"conv2d_115\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_82\", %\"vae.decoder.up_blocks.1.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2042 |  # node_Reshape_6076\n",
       "                    %\"val_6190\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_115\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2043 |  # node_InstanceNormalization_6083\n",
       "                    %\"val_6197\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6190\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2044 |  # node_Reshape_6085\n",
       "                    %\"val_6199\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6197\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2045 |  # node_Mul_6092\n",
       "                    %\"val_6206\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6199\", %\"val_6205\"{...})\n",
       "            2046 |  # node_group_norm_77\n",
       "                    %\"group_norm_77\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6206\", %\"val_6207\"{...})\n",
       "            2047 |  # node_Sigmoid_6094\n",
       "                    %\"val_6208\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_77\")\n",
       "            2048 |  # node_silu_83\n",
       "                    %\"silu_83\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_77\", %\"val_6208\")\n",
       "            2049 |  # node_conv2d_116\n",
       "                    %\"conv2d_116\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_83\", %\"vae.decoder.up_blocks.1.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2050 |  # node_add_126\n",
       "                    %\"add_126\"<FLOAT,[8,512,64,64]>  ::Add(%\"add_125\", %\"conv2d_116\")\n",
       "            2051 |  # node_upsample_nearest2d_4\n",
       "                    %\"upsample_nearest2d_4\"<FLOAT,[8,512,128,128]>  ::Resize(%\"add_126\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            2052 |  # node_conv2d_117\n",
       "                    %\"conv2d_117\"<FLOAT,[8,512,128,128]>  ::Conv(%\"upsample_nearest2d_4\", %\"vae.decoder.up_blocks.1.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.1.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2053 |  # node_Reshape_6100\n",
       "                    %\"val_6214\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_117\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2054 |  # node_InstanceNormalization_6107\n",
       "                    %\"val_6221\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6214\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2055 |  # node_Reshape_6109\n",
       "                    %\"val_6223\"<FLOAT,[8,512,128,128]>  ::Reshape(%\"val_6221\", %\"val_6222\"{[8, 512, 128, 128]}) {allowzero=0}\n",
       "            2056 |  # node_Mul_6116\n",
       "                    %\"val_6230\"<FLOAT,[8,512,128,128]>  ::Mul(%\"val_6223\", %\"val_6229\"{...})\n",
       "            2057 |  # node_group_norm_78\n",
       "                    %\"group_norm_78\"<FLOAT,[8,512,128,128]>  ::Add(%\"val_6230\", %\"val_6231\"{...})\n",
       "            2058 |  # node_Sigmoid_6118\n",
       "                    %\"val_6232\"<FLOAT,[8,512,128,128]>  ::Sigmoid(%\"group_norm_78\")\n",
       "            2059 |  # node_silu_84\n",
       "                    %\"silu_84\"<FLOAT,[8,512,128,128]>  ::Mul(%\"group_norm_78\", %\"val_6232\")\n",
       "            2060 |  # node_conv2d_118\n",
       "                    %\"conv2d_118\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_84\", %\"vae.decoder.up_blocks.2.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2061 |  # node_Reshape_6123\n",
       "                    %\"val_6237\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_118\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2062 |  # node_InstanceNormalization_6130\n",
       "                    %\"val_6244\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6237\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2063 |  # node_Reshape_6132\n",
       "                    %\"val_6246\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6244\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2064 |  # node_Mul_6139\n",
       "                    %\"val_6253\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6246\", %\"val_6252\"{...})\n",
       "            2065 |  # node_group_norm_79\n",
       "                    %\"group_norm_79\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6253\", %\"val_6254\"{...})\n",
       "            2066 |  # node_Sigmoid_6141\n",
       "                    %\"val_6255\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_79\")\n",
       "            2067 |  # node_silu_85\n",
       "                    %\"silu_85\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_79\", %\"val_6255\")\n",
       "            2068 |  # node_conv2d_119\n",
       "                    %\"conv2d_119\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_85\", %\"vae.decoder.up_blocks.2.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2069 |  # node_conv2d_120\n",
       "                    %\"conv2d_120\"<FLOAT,[8,256,128,128]>  ::Conv(%\"conv2d_117\", %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2070 |  # node_add_127\n",
       "                    %\"add_127\"<FLOAT,[8,256,128,128]>  ::Add(%\"conv2d_120\", %\"conv2d_119\")\n",
       "            2071 |  # node_Reshape_6146\n",
       "                    %\"val_6260\"<FLOAT,[8,32,131072]>  ::Reshape(%\"add_127\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2072 |  # node_InstanceNormalization_6153\n",
       "                    %\"val_6267\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6260\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2073 |  # node_Reshape_6155\n",
       "                    %\"val_6269\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6267\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2074 |  # node_Mul_6162\n",
       "                    %\"val_6276\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6269\", %\"val_6275\"{...})\n",
       "            2075 |  # node_group_norm_80\n",
       "                    %\"group_norm_80\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6276\", %\"val_6277\"{...})\n",
       "            2076 |  # node_Sigmoid_6164\n",
       "                    %\"val_6278\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_80\")\n",
       "            2077 |  # node_silu_86\n",
       "                    %\"silu_86\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_80\", %\"val_6278\")\n",
       "            2078 |  # node_conv2d_121\n",
       "                    %\"conv2d_121\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_86\", %\"vae.decoder.up_blocks.2.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2079 |  # node_Reshape_6169\n",
       "                    %\"val_6283\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_121\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2080 |  # node_InstanceNormalization_6176\n",
       "                    %\"val_6290\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6283\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2081 |  # node_Reshape_6178\n",
       "                    %\"val_6292\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6290\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2082 |  # node_Mul_6185\n",
       "                    %\"val_6299\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6292\", %\"val_6298\"{...})\n",
       "            2083 |  # node_group_norm_81\n",
       "                    %\"group_norm_81\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6299\", %\"val_6300\"{...})\n",
       "            2084 |  # node_Sigmoid_6187\n",
       "                    %\"val_6301\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_81\")\n",
       "            2085 |  # node_silu_87\n",
       "                    %\"silu_87\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_81\", %\"val_6301\")\n",
       "            2086 |  # node_conv2d_122\n",
       "                    %\"conv2d_122\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_87\", %\"vae.decoder.up_blocks.2.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2087 |  # node_add_128\n",
       "                    %\"add_128\"<FLOAT,[8,256,128,128]>  ::Add(%\"add_127\", %\"conv2d_122\")\n",
       "            2088 |  # node_Reshape_6192\n",
       "                    %\"val_6306\"<FLOAT,[8,32,131072]>  ::Reshape(%\"add_128\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2089 |  # node_InstanceNormalization_6199\n",
       "                    %\"val_6313\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6306\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2090 |  # node_Reshape_6201\n",
       "                    %\"val_6315\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6313\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2091 |  # node_Mul_6208\n",
       "                    %\"val_6322\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6315\", %\"val_6321\"{...})\n",
       "            2092 |  # node_group_norm_82\n",
       "                    %\"group_norm_82\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6322\", %\"val_6323\"{...})\n",
       "            2093 |  # node_Sigmoid_6210\n",
       "                    %\"val_6324\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_82\")\n",
       "            2094 |  # node_silu_88\n",
       "                    %\"silu_88\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_82\", %\"val_6324\")\n",
       "            2095 |  # node_conv2d_123\n",
       "                    %\"conv2d_123\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_88\", %\"vae.decoder.up_blocks.2.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2096 |  # node_Reshape_6215\n",
       "                    %\"val_6329\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_123\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2097 |  # node_InstanceNormalization_6222\n",
       "                    %\"val_6336\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6329\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2098 |  # node_Reshape_6224\n",
       "                    %\"val_6338\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6336\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2099 |  # node_Mul_6231\n",
       "                    %\"val_6345\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6338\", %\"val_6344\"{...})\n",
       "            2100 |  # node_group_norm_83\n",
       "                    %\"group_norm_83\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6345\", %\"val_6346\"{...})\n",
       "            2101 |  # node_Sigmoid_6233\n",
       "                    %\"val_6347\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_83\")\n",
       "            2102 |  # node_silu_89\n",
       "                    %\"silu_89\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_83\", %\"val_6347\")\n",
       "            2103 |  # node_conv2d_124\n",
       "                    %\"conv2d_124\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_89\", %\"vae.decoder.up_blocks.2.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2104 |  # node_add_129\n",
       "                    %\"add_129\"<FLOAT,[8,256,128,128]>  ::Add(%\"add_128\", %\"conv2d_124\")\n",
       "            2105 |  # node_upsample_nearest2d_5\n",
       "                    %\"upsample_nearest2d_5\"<FLOAT,[8,256,256,256]>  ::Resize(%\"add_129\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            2106 |  # node_conv2d_125\n",
       "                    %\"conv2d_125\"<FLOAT,[8,256,256,256]>  ::Conv(%\"upsample_nearest2d_5\", %\"vae.decoder.up_blocks.2.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.2.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2107 |  # node_Reshape_6239\n",
       "                    %\"val_6353\"<FLOAT,[8,32,524288]>  ::Reshape(%\"conv2d_125\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2108 |  # node_InstanceNormalization_6246\n",
       "                    %\"val_6360\"<FLOAT,[8,32,524288]>  ::InstanceNormalization(%\"val_6353\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2109 |  # node_Reshape_6248\n",
       "                    %\"val_6362\"<FLOAT,[8,256,256,256]>  ::Reshape(%\"val_6360\", %\"val_6361\"{[8, 256, 256, 256]}) {allowzero=0}\n",
       "            2110 |  # node_Mul_6255\n",
       "                    %\"val_6369\"<FLOAT,[8,256,256,256]>  ::Mul(%\"val_6362\", %\"val_6368\"{...})\n",
       "            2111 |  # node_group_norm_84\n",
       "                    %\"group_norm_84\"<FLOAT,[8,256,256,256]>  ::Add(%\"val_6369\", %\"val_6370\"{...})\n",
       "            2112 |  # node_Sigmoid_6257\n",
       "                    %\"val_6371\"<FLOAT,[8,256,256,256]>  ::Sigmoid(%\"group_norm_84\")\n",
       "            2113 |  # node_silu_90\n",
       "                    %\"silu_90\"<FLOAT,[8,256,256,256]>  ::Mul(%\"group_norm_84\", %\"val_6371\")\n",
       "            2114 |  # node_conv2d_126\n",
       "                    %\"conv2d_126\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_90\", %\"vae.decoder.up_blocks.3.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2115 |  # node_Reshape_6262\n",
       "                    %\"val_6376\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_126\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2116 |  # node_InstanceNormalization_6269\n",
       "                    %\"val_6383\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6376\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2117 |  # node_Reshape_6271\n",
       "                    %\"val_6385\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6383\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2118 |  # node_Mul_6278\n",
       "                    %\"val_6392\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6385\", %\"val_6391\"{...})\n",
       "            2119 |  # node_group_norm_85\n",
       "                    %\"group_norm_85\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6392\", %\"val_6393\"{...})\n",
       "            2120 |  # node_Sigmoid_6280\n",
       "                    %\"val_6394\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_85\")\n",
       "            2121 |  # node_silu_91\n",
       "                    %\"silu_91\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_85\", %\"val_6394\")\n",
       "            2122 |  # node_conv2d_127\n",
       "                    %\"conv2d_127\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_91\", %\"vae.decoder.up_blocks.3.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2123 |  # node_conv2d_128\n",
       "                    %\"conv2d_128\"<FLOAT,[8,128,256,256]>  ::Conv(%\"conv2d_125\", %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2124 |  # node_add_130\n",
       "                    %\"add_130\"<FLOAT,[8,128,256,256]>  ::Add(%\"conv2d_128\", %\"conv2d_127\")\n",
       "            2125 |  # node_Reshape_6285\n",
       "                    %\"val_6399\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_130\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2126 |  # node_InstanceNormalization_6292\n",
       "                    %\"val_6406\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6399\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2127 |  # node_Reshape_6294\n",
       "                    %\"val_6408\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6406\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2128 |  # node_Mul_6301\n",
       "                    %\"val_6415\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6408\", %\"val_6414\"{...})\n",
       "            2129 |  # node_group_norm_86\n",
       "                    %\"group_norm_86\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6415\", %\"val_6416\"{...})\n",
       "            2130 |  # node_Sigmoid_6303\n",
       "                    %\"val_6417\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_86\")\n",
       "            2131 |  # node_silu_92\n",
       "                    %\"silu_92\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_86\", %\"val_6417\")\n",
       "            2132 |  # node_conv2d_129\n",
       "                    %\"conv2d_129\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_92\", %\"vae.decoder.up_blocks.3.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2133 |  # node_Reshape_6308\n",
       "                    %\"val_6422\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_129\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2134 |  # node_InstanceNormalization_6315\n",
       "                    %\"val_6429\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6422\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2135 |  # node_Reshape_6317\n",
       "                    %\"val_6431\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6429\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2136 |  # node_Mul_6324\n",
       "                    %\"val_6438\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6431\", %\"val_6437\"{...})\n",
       "            2137 |  # node_group_norm_87\n",
       "                    %\"group_norm_87\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6438\", %\"val_6439\"{...})\n",
       "            2138 |  # node_Sigmoid_6326\n",
       "                    %\"val_6440\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_87\")\n",
       "            2139 |  # node_silu_93\n",
       "                    %\"silu_93\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_87\", %\"val_6440\")\n",
       "            2140 |  # node_conv2d_130\n",
       "                    %\"conv2d_130\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_93\", %\"vae.decoder.up_blocks.3.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2141 |  # node_add_131\n",
       "                    %\"add_131\"<FLOAT,[8,128,256,256]>  ::Add(%\"add_130\", %\"conv2d_130\")\n",
       "            2142 |  # node_Reshape_6331\n",
       "                    %\"val_6445\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_131\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2143 |  # node_InstanceNormalization_6338\n",
       "                    %\"val_6452\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6445\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2144 |  # node_Reshape_6340\n",
       "                    %\"val_6454\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6452\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2145 |  # node_Mul_6347\n",
       "                    %\"val_6461\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6454\", %\"val_6460\"{...})\n",
       "            2146 |  # node_group_norm_88\n",
       "                    %\"group_norm_88\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6461\", %\"val_6462\"{...})\n",
       "            2147 |  # node_Sigmoid_6349\n",
       "                    %\"val_6463\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_88\")\n",
       "            2148 |  # node_silu_94\n",
       "                    %\"silu_94\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_88\", %\"val_6463\")\n",
       "            2149 |  # node_conv2d_131\n",
       "                    %\"conv2d_131\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_94\", %\"vae.decoder.up_blocks.3.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2150 |  # node_Reshape_6354\n",
       "                    %\"val_6468\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_131\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2151 |  # node_InstanceNormalization_6361\n",
       "                    %\"val_6475\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6468\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2152 |  # node_Reshape_6363\n",
       "                    %\"val_6477\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6475\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2153 |  # node_Mul_6370\n",
       "                    %\"val_6484\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6477\", %\"val_6483\"{...})\n",
       "            2154 |  # node_group_norm_89\n",
       "                    %\"group_norm_89\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6484\", %\"val_6485\"{...})\n",
       "            2155 |  # node_Sigmoid_6372\n",
       "                    %\"val_6486\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_89\")\n",
       "            2156 |  # node_silu_95\n",
       "                    %\"silu_95\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_89\", %\"val_6486\")\n",
       "            2157 |  # node_conv2d_132\n",
       "                    %\"conv2d_132\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_95\", %\"vae.decoder.up_blocks.3.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2158 |  # node_add_132\n",
       "                    %\"add_132\"<FLOAT,[8,128,256,256]>  ::Add(%\"add_131\", %\"conv2d_132\")\n",
       "            2159 |  # node_Reshape_6377\n",
       "                    %\"val_6491\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_132\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2160 |  # node_InstanceNormalization_6384\n",
       "                    %\"val_6498\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6491\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2161 |  # node_Reshape_6386\n",
       "                    %\"val_6500\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6498\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2162 |  # node_Mul_6393\n",
       "                    %\"val_6507\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6500\", %\"val_6506\"{...})\n",
       "            2163 |  # node_group_norm_90\n",
       "                    %\"group_norm_90\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6507\", %\"val_6508\"{...})\n",
       "            2164 |  # node_Sigmoid_6395\n",
       "                    %\"val_6509\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_90\")\n",
       "            2165 |  # node_silu_96\n",
       "                    %\"silu_96\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_90\", %\"val_6509\")\n",
       "            2166 |  # node_conv2d_133\n",
       "                    %\"conv2d_133\"<FLOAT,[8,3,256,256]>  ::Conv(%\"silu_96\", %\"vae.decoder.conv_out.weight\"{...}, %\"vae.decoder.conv_out.bias\"{[0.015753211453557014, -0.02031349577009678, -0.046358201652765274]}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2167 |  # node_div_70\n",
       "                    %\"div_70\"<FLOAT,[8,3,256,256]>  ::Div(%\"conv2d_133\", %\"scalar_tensor_default_7\"{2.0})\n",
       "            2168 |  # node_add_133\n",
       "                    %\"add_133\"<FLOAT,[8,3,256,256]>  ::Add(%\"div_70\", %\"val_6510\"{0.5})\n",
       "            2169 |  # node_Clip_7830\n",
       "                    %\"clamp\"<FLOAT,[8,3,256,256]>  ::Clip(%\"add_133\", %\"add_133_min\"{0.0}, %\"add_133_max\"{1.0})\n",
       "            2170 |  # node_permute_33\n",
       "                    %\"permute_33\"<FLOAT,[8,256,256,3]>  ::Transpose(%\"clamp\") {perm=(0, 2, 3, 1)}\n",
       "            2171 |  # node_multiply\n",
       "                    %\"multiply\"<FLOAT,[8,256,256,3]>  ::Mul(%\"permute_33\", %\"clone_126\"{255.0})\n",
       "            2172 |  # n6\n",
       "                    %\"rounded\"<FLOAT,[8,256,256,3]>  ::Round(%\"multiply\")\n",
       "            2173 |  # node__to_copy_5\n",
       "                    %\"_to_copy_5\"<UINT8,[8,256,256,3]>  ::Cast(%\"rounded\") {to=2}\n",
       "            2174 |  # node_flip\n",
       "                    %\"alias\"<UINT8,[8,256,256,3]>  ::Slice(%\"_to_copy_5\", %\"val_6518\"{[-1]}, %\"val_6519\"{[-9223372036854775808]}, %\"val_289\"{[1]}, %\"val_6518\"{[-1]})\n",
       "            return %\"alias\"<UINT8,[8,256,256,3]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_whisper_encoder_embed_positions_weight: \"f32[1500, 384]\", p_whisper_encoder_conv1_weight: \"f32[384, 80, 3]\", p_whisper_encoder_conv1_bias: \"f32[384]\", p_whisper_encoder_conv2_weight: \"f32[384, 384, 3]\", p_whisper_encoder_conv2_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_0_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_0_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_0_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_0_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_0_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_1_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_1_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_1_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_1_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_1_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_2_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_2_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_2_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_2_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_2_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_3_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_3_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_3_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_3_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_3_fc2_bias: \"f32[384]\", p_whisper_encoder_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layer_norm_bias: \"f32[384]\", p_unet_time_embedding_linear_1_weight: \"f32[1280, 320]\", p_unet_time_embedding_linear_1_bias: \"f32[1280]\", p_unet_time_embedding_linear_2_weight: \"f32[1280, 1280]\", p_unet_time_embedding_linear_2_bias: \"f32[1280]\", p_unet_conv_in_weight: \"f32[320, 8, 3, 3]\", p_unet_conv_in_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_conv1_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_0_conv1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_resnets_0_time_emb_proj_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_0_conv2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_conv1_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_1_conv1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_resnets_1_time_emb_proj_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_1_conv2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_norm_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_norm_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_0_proj_in_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_0_proj_out_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_norm_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_norm_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_1_proj_in_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_1_proj_out_bias: \"f32[320]\", p_unet_down_blocks_0_downsamplers_0_conv_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_downsamplers_0_conv_bias: \"f32[320]\", p_unet_down_blocks_1_resnets_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_1_resnets_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_1_resnets_0_conv1_weight: \"f32[640, 320, 3, 3]\", p_unet_down_blocks_1_resnets_0_conv1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_down_blocks_1_resnets_0_time_emb_proj_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_0_conv2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_conv_shortcut_weight: \"f32[640, 320, 1, 1]\", p_unet_down_blocks_1_resnets_0_conv_shortcut_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_conv1_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_1_conv1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_down_blocks_1_resnets_1_time_emb_proj_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_1_conv2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_norm_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_norm_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_0_proj_in_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_0_proj_out_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_norm_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_norm_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_1_proj_in_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_1_proj_out_bias: \"f32[640]\", p_unet_down_blocks_1_downsamplers_0_conv_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_downsamplers_0_conv_bias: \"f32[640]\", p_unet_down_blocks_2_resnets_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_2_resnets_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_2_resnets_0_conv1_weight: \"f32[1280, 640, 3, 3]\", p_unet_down_blocks_2_resnets_0_conv1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_0_conv2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_conv_shortcut_weight: \"f32[1280, 640, 1, 1]\", p_unet_down_blocks_2_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_1_conv1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_1_conv2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_norm_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_norm_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_norm_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_norm_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_1_proj_in_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_1_proj_out_bias: \"f32[1280]\", p_unet_down_blocks_2_downsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_downsamplers_0_conv_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_0_conv1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_3_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_0_conv2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm1_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_1_conv1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_3_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm2_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_1_conv2_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_norm1_weight: \"f32[1280]\", p_unet_mid_block_resnets_0_norm1_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_0_conv1_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_mid_block_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_norm2_weight: \"f32[1280]\", p_unet_mid_block_resnets_0_norm2_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_0_conv2_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_norm1_weight: \"f32[1280]\", p_unet_mid_block_resnets_1_norm1_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_1_conv1_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_mid_block_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_norm2_weight: \"f32[1280]\", p_unet_mid_block_resnets_1_norm2_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_1_conv2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_norm_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_norm_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_mid_block_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_mid_block_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_0_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_0_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_0_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_0_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_1_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_1_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_1_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_1_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_1_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_2_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_2_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_2_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_2_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_2_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_2_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_upsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_upsamplers_0_conv_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm1_weight: \"f32[2560]\", p_unet_up_blocks_1_resnets_0_norm1_bias: \"f32[2560]\", p_unet_up_blocks_1_resnets_0_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_1_resnets_0_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_0_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_1_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm1_weight: \"f32[2560]\", p_unet_up_blocks_1_resnets_1_norm1_bias: \"f32[2560]\", p_unet_up_blocks_1_resnets_1_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_1_resnets_1_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_1_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_1_resnets_1_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm1_weight: \"f32[1920]\", p_unet_up_blocks_1_resnets_2_norm1_bias: \"f32[1920]\", p_unet_up_blocks_1_resnets_2_conv1_weight: \"f32[1280, 1920, 3, 3]\", p_unet_up_blocks_1_resnets_2_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_2_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_2_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_conv_shortcut_weight: \"f32[1280, 1920, 1, 1]\", p_unet_up_blocks_1_resnets_2_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_1_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_1_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_2_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_2_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_upsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_upsamplers_0_conv_bias: \"f32[1280]\", p_unet_up_blocks_2_resnets_0_norm1_weight: \"f32[1920]\", p_unet_up_blocks_2_resnets_0_norm1_bias: \"f32[1920]\", p_unet_up_blocks_2_resnets_0_conv1_weight: \"f32[640, 1920, 3, 3]\", p_unet_up_blocks_2_resnets_0_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_0_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_0_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_conv_shortcut_weight: \"f32[640, 1920, 1, 1]\", p_unet_up_blocks_2_resnets_0_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm1_weight: \"f32[1280]\", p_unet_up_blocks_2_resnets_1_norm1_bias: \"f32[1280]\", p_unet_up_blocks_2_resnets_1_conv1_weight: \"f32[640, 1280, 3, 3]\", p_unet_up_blocks_2_resnets_1_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_1_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_1_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_conv_shortcut_weight: \"f32[640, 1280, 1, 1]\", p_unet_up_blocks_2_resnets_1_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm1_weight: \"f32[960]\", p_unet_up_blocks_2_resnets_2_norm1_bias: \"f32[960]\", p_unet_up_blocks_2_resnets_2_conv1_weight: \"f32[640, 960, 3, 3]\", p_unet_up_blocks_2_resnets_2_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_2_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_2_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_conv_shortcut_weight: \"f32[640, 960, 1, 1]\", p_unet_up_blocks_2_resnets_2_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_0_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_0_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_1_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_1_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_2_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_2_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_upsamplers_0_conv_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_upsamplers_0_conv_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_0_norm1_weight: \"f32[960]\", p_unet_up_blocks_3_resnets_0_norm1_bias: \"f32[960]\", p_unet_up_blocks_3_resnets_0_conv1_weight: \"f32[320, 960, 3, 3]\", p_unet_up_blocks_3_resnets_0_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_0_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_0_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_conv_shortcut_weight: \"f32[320, 960, 1, 1]\", p_unet_up_blocks_3_resnets_0_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm1_weight: \"f32[640]\", p_unet_up_blocks_3_resnets_1_norm1_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_1_conv1_weight: \"f32[320, 640, 3, 3]\", p_unet_up_blocks_3_resnets_1_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_1_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_1_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_conv_shortcut_weight: \"f32[320, 640, 1, 1]\", p_unet_up_blocks_3_resnets_1_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm1_weight: \"f32[640]\", p_unet_up_blocks_3_resnets_2_norm1_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_2_conv1_weight: \"f32[320, 640, 3, 3]\", p_unet_up_blocks_3_resnets_2_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_2_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_2_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_conv_shortcut_weight: \"f32[320, 640, 1, 1]\", p_unet_up_blocks_3_resnets_2_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_0_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_0_proj_out_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_1_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_1_proj_out_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_2_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_2_proj_out_bias: \"f32[320]\", p_unet_conv_norm_out_weight: \"f32[320]\", p_unet_conv_norm_out_bias: \"f32[320]\", p_unet_conv_out_weight: \"f32[4, 320, 3, 3]\", p_unet_conv_out_bias: \"f32[4]\", p_vae_post_quant_conv_weight: \"f32[4, 4, 1, 1]\", p_vae_post_quant_conv_bias: \"f32[4]\", p_vae_decoder_conv_in_weight: \"f32[512, 4, 3, 3]\", p_vae_decoder_conv_in_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_group_norm_weight: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_group_norm_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_q_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_q_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_k_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_k_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_v_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_v_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_out_0_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_out_0_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_2_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_2_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_2_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_2_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_conv1_weight: \"f32[256, 512, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_0_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_0_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: \"f32[256, 512, 1, 1]\", p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_conv1_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_1_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_1_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_conv1_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_2_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_2_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_conv1_weight: \"f32[128, 256, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_0_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_0_conv2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: \"f32[128, 256, 1, 1]\", p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm1_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_conv1_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_1_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_1_conv2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm1_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_conv1_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_2_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_2_conv2_bias: \"f32[128]\", p_vae_decoder_conv_norm_out_weight: \"f32[128]\", p_vae_decoder_conv_norm_out_bias: \"f32[128]\", p_vae_decoder_conv_out_weight: \"f32[3, 128, 3, 3]\", p_vae_decoder_conv_out_bias: \"f32[3]\", c_timesteps: \"i64[1]\", c_lifted_tensor_0: \"f32[]\", b_pe_pe: \"f32[1, 5000, 384]\", whisper_input_features_0: \"f32[1, 80, 3000]\", latent_inputs: \"f32[8, 8, 32, 32]\", frame_idx, librosa_length):\n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:68 in get_whisper_chunk, code: input_feature = input_feature.to(device).to(weight_dtype)\n",
       "                    _to_copy: \"f32[1, 80, 3000]\" = torch.ops.aten._to_copy.default(whisper_input_features_0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  whisper_input_features_0 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1175 in forward, code: inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
       "                    conv1d: \"f32[1, 384, 3000]\" = torch.ops.aten.conv1d.default(_to_copy, p_whisper_encoder_conv1_weight, p_whisper_encoder_conv1_bias, [1], [1]);  _to_copy = p_whisper_encoder_conv1_weight = p_whisper_encoder_conv1_bias = None\n",
       "                    gelu: \"f32[1, 384, 3000]\" = torch.ops.aten.gelu.default(conv1d);  conv1d = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1176 in forward, code: inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
       "                    conv1d_1: \"f32[1, 384, 1500]\" = torch.ops.aten.conv1d.default(gelu, p_whisper_encoder_conv2_weight, p_whisper_encoder_conv2_bias, [2], [1]);  gelu = p_whisper_encoder_conv2_weight = p_whisper_encoder_conv2_bias = None\n",
       "                    gelu_1: \"f32[1, 384, 1500]\" = torch.ops.aten.gelu.default(conv1d_1);  conv1d_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1178 in forward, code: inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
       "                    permute: \"f32[1, 1500, 384]\" = torch.ops.aten.permute.default(gelu_1, [0, 2, 1]);  gelu_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1181 in forward, code: hidden_states = inputs_embeds + embed_pos\n",
       "                    add: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(permute, p_whisper_encoder_embed_positions_weight);  permute = p_whisper_encoder_embed_positions_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1182 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(add);  add = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(clone, [384], p_whisper_encoder_layers_0_self_attn_layer_norm_weight, p_whisper_encoder_layers_0_self_attn_layer_norm_bias);  p_whisper_encoder_layers_0_self_attn_layer_norm_weight = p_whisper_encoder_layers_0_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_q_proj_weight, p_whisper_encoder_layers_0_self_attn_q_proj_bias);  p_whisper_encoder_layers_0_self_attn_q_proj_weight = p_whisper_encoder_layers_0_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_1: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_k_proj_weight);  p_whisper_encoder_layers_0_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_1, [1, -1, 6, 64]);  linear_1 = None\n",
       "                    transpose: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "                    clone_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose, memory_format = torch.contiguous_format);  transpose = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_2: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_v_proj_weight, p_whisper_encoder_layers_0_self_attn_v_proj_bias);  layer_norm = p_whisper_encoder_layers_0_self_attn_v_proj_weight = p_whisper_encoder_layers_0_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_1: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_2, [1, -1, 6, 64]);  linear_2 = None\n",
       "                    transpose_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
       "                    clone_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_2: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear, [1, 1500, 6, 64]);  linear = None\n",
       "                    transpose_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
       "                    clone_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_3, clone_1, clone_2);  clone_3 = clone_1 = clone_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_3: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_3, [1, 1500, 384]);  transpose_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_3: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_3, p_whisper_encoder_layers_0_self_attn_out_proj_weight, p_whisper_encoder_layers_0_self_attn_out_proj_bias);  view_3 = p_whisper_encoder_layers_0_self_attn_out_proj_weight = p_whisper_encoder_layers_0_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_4: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(clone, clone_4);  clone_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_1: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_1, [384], p_whisper_encoder_layers_0_final_layer_norm_weight, p_whisper_encoder_layers_0_final_layer_norm_bias);  p_whisper_encoder_layers_0_final_layer_norm_weight = p_whisper_encoder_layers_0_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_1, p_whisper_encoder_layers_0_fc1_weight, p_whisper_encoder_layers_0_fc1_bias);  layer_norm_1 = p_whisper_encoder_layers_0_fc1_weight = p_whisper_encoder_layers_0_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_2);  gelu_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_5: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_5, p_whisper_encoder_layers_0_fc2_weight, p_whisper_encoder_layers_0_fc2_bias);  clone_5 = p_whisper_encoder_layers_0_fc2_weight = p_whisper_encoder_layers_0_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_6: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_2: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_1, clone_6);  add_1 = clone_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_2: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_2, [384], p_whisper_encoder_layers_1_self_attn_layer_norm_weight, p_whisper_encoder_layers_1_self_attn_layer_norm_bias);  p_whisper_encoder_layers_1_self_attn_layer_norm_weight = p_whisper_encoder_layers_1_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_6: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_q_proj_weight, p_whisper_encoder_layers_1_self_attn_q_proj_bias);  p_whisper_encoder_layers_1_self_attn_q_proj_weight = p_whisper_encoder_layers_1_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_7: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_k_proj_weight);  p_whisper_encoder_layers_1_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_4: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_7, [1, -1, 6, 64]);  linear_7 = None\n",
       "                    transpose_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
       "                    clone_7: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_8: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_v_proj_weight, p_whisper_encoder_layers_1_self_attn_v_proj_bias);  layer_norm_2 = p_whisper_encoder_layers_1_self_attn_v_proj_weight = p_whisper_encoder_layers_1_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_5: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_8, [1, -1, 6, 64]);  linear_8 = None\n",
       "                    transpose_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
       "                    clone_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_6: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_6, [1, 1500, 6, 64]);  linear_6 = None\n",
       "                    transpose_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
       "                    clone_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_9, clone_7, clone_8);  clone_9 = clone_7 = clone_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_7: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_7, [1, 1500, 384]);  transpose_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_9: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_7, p_whisper_encoder_layers_1_self_attn_out_proj_weight, p_whisper_encoder_layers_1_self_attn_out_proj_bias);  view_7 = p_whisper_encoder_layers_1_self_attn_out_proj_weight = p_whisper_encoder_layers_1_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_10: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_3: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_2, clone_10);  clone_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_3: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_3, [384], p_whisper_encoder_layers_1_final_layer_norm_weight, p_whisper_encoder_layers_1_final_layer_norm_bias);  p_whisper_encoder_layers_1_final_layer_norm_weight = p_whisper_encoder_layers_1_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_10: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_3, p_whisper_encoder_layers_1_fc1_weight, p_whisper_encoder_layers_1_fc1_bias);  layer_norm_3 = p_whisper_encoder_layers_1_fc1_weight = p_whisper_encoder_layers_1_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_3: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_11: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_3);  gelu_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_11: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_11, p_whisper_encoder_layers_1_fc2_weight, p_whisper_encoder_layers_1_fc2_bias);  clone_11 = p_whisper_encoder_layers_1_fc2_weight = p_whisper_encoder_layers_1_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_12: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_4: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_3, clone_12);  add_3 = clone_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_4: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_4, [384], p_whisper_encoder_layers_2_self_attn_layer_norm_weight, p_whisper_encoder_layers_2_self_attn_layer_norm_bias);  p_whisper_encoder_layers_2_self_attn_layer_norm_weight = p_whisper_encoder_layers_2_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_12: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_q_proj_weight, p_whisper_encoder_layers_2_self_attn_q_proj_bias);  p_whisper_encoder_layers_2_self_attn_q_proj_weight = p_whisper_encoder_layers_2_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_13: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_k_proj_weight);  p_whisper_encoder_layers_2_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_8: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_13, [1, -1, 6, 64]);  linear_13 = None\n",
       "                    transpose_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
       "                    clone_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_8, memory_format = torch.contiguous_format);  transpose_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_14: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_v_proj_weight, p_whisper_encoder_layers_2_self_attn_v_proj_bias);  layer_norm_4 = p_whisper_encoder_layers_2_self_attn_v_proj_weight = p_whisper_encoder_layers_2_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_9: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_14, [1, -1, 6, 64]);  linear_14 = None\n",
       "                    transpose_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
       "                    clone_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_10: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_12, [1, 1500, 6, 64]);  linear_12 = None\n",
       "                    transpose_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
       "                    clone_15: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_15, clone_13, clone_14);  clone_15 = clone_13 = clone_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_11: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_11, [1, 1500, 384]);  transpose_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_15: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_11, p_whisper_encoder_layers_2_self_attn_out_proj_weight, p_whisper_encoder_layers_2_self_attn_out_proj_bias);  view_11 = p_whisper_encoder_layers_2_self_attn_out_proj_weight = p_whisper_encoder_layers_2_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_16: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_5: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_4, clone_16);  clone_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_5: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_5, [384], p_whisper_encoder_layers_2_final_layer_norm_weight, p_whisper_encoder_layers_2_final_layer_norm_bias);  p_whisper_encoder_layers_2_final_layer_norm_weight = p_whisper_encoder_layers_2_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_16: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_5, p_whisper_encoder_layers_2_fc1_weight, p_whisper_encoder_layers_2_fc1_bias);  layer_norm_5 = p_whisper_encoder_layers_2_fc1_weight = p_whisper_encoder_layers_2_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_17: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_4);  gelu_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_17: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_17, p_whisper_encoder_layers_2_fc2_weight, p_whisper_encoder_layers_2_fc2_bias);  clone_17 = p_whisper_encoder_layers_2_fc2_weight = p_whisper_encoder_layers_2_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_18: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_6: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_5, clone_18);  add_5 = clone_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_6: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], p_whisper_encoder_layers_3_self_attn_layer_norm_weight, p_whisper_encoder_layers_3_self_attn_layer_norm_bias);  p_whisper_encoder_layers_3_self_attn_layer_norm_weight = p_whisper_encoder_layers_3_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_18: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_q_proj_weight, p_whisper_encoder_layers_3_self_attn_q_proj_bias);  p_whisper_encoder_layers_3_self_attn_q_proj_weight = p_whisper_encoder_layers_3_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_19: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_k_proj_weight);  p_whisper_encoder_layers_3_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_12: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_19, [1, -1, 6, 64]);  linear_19 = None\n",
       "                    transpose_12: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_12, 1, 2);  view_12 = None\n",
       "                    clone_19: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_20: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_v_proj_weight, p_whisper_encoder_layers_3_self_attn_v_proj_bias);  layer_norm_6 = p_whisper_encoder_layers_3_self_attn_v_proj_weight = p_whisper_encoder_layers_3_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_13: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_20, [1, -1, 6, 64]);  linear_20 = None\n",
       "                    transpose_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
       "                    clone_20: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_13, memory_format = torch.contiguous_format);  transpose_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_14: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_18, [1, 1500, 6, 64]);  linear_18 = None\n",
       "                    transpose_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_14, 1, 2);  view_14 = None\n",
       "                    clone_21: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_21, clone_19, clone_20);  clone_21 = clone_19 = clone_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_15: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_15: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_15, [1, 1500, 384]);  transpose_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_21: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_15, p_whisper_encoder_layers_3_self_attn_out_proj_weight, p_whisper_encoder_layers_3_self_attn_out_proj_bias);  view_15 = p_whisper_encoder_layers_3_self_attn_out_proj_weight = p_whisper_encoder_layers_3_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_22: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_7: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_6, clone_22);  clone_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_7: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_7, [384], p_whisper_encoder_layers_3_final_layer_norm_weight, p_whisper_encoder_layers_3_final_layer_norm_bias);  p_whisper_encoder_layers_3_final_layer_norm_weight = p_whisper_encoder_layers_3_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_22: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_7, p_whisper_encoder_layers_3_fc1_weight, p_whisper_encoder_layers_3_fc1_bias);  layer_norm_7 = p_whisper_encoder_layers_3_fc1_weight = p_whisper_encoder_layers_3_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_23: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_5);  gelu_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_23: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_23, p_whisper_encoder_layers_3_fc2_weight, p_whisper_encoder_layers_3_fc2_bias);  clone_23 = p_whisper_encoder_layers_3_fc2_weight = p_whisper_encoder_layers_3_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_24: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_8: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_7, clone_24);  add_7 = clone_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1227 in forward, code: hidden_states = self.layer_norm(hidden_states)\n",
       "                    layer_norm_8: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_8, [384], p_whisper_encoder_layer_norm_weight, p_whisper_encoder_layer_norm_bias);  add_8 = p_whisper_encoder_layer_norm_weight = p_whisper_encoder_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:70 in get_whisper_chunk, code: audio_feats = torch.stack(audio_feats, dim=2)\n",
       "                    stack: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.stack.default([clone, add_2, add_4, add_6, layer_norm_8], 2);  clone = add_2 = add_4 = add_6 = layer_norm_8 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:73 in get_whisper_chunk, code: whisper_feature = torch.cat(whisper_feature, dim=1)\n",
       "                    cat: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.cat.default([stack], 1);  stack = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:81 in get_whisper_chunk, code: whisper_feature = whisper_feature[:,:actual_length,...]\n",
       "                    slice_1: \"f32[1, 332, 5, 384]\" = torch.ops.aten.slice.Tensor(cat, 1, 0, 332);  cat = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:87 in get_whisper_chunk, code: torch.zeros_like(whisper_feature[:, :padding_nums * self.audio_padding_length_left]),\n",
       "                    slice_2: \"f32[1, 4, 5, 384]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 4)\n",
       "                    zeros_like: \"f32[1, 4, 5, 384]\" = torch.ops.aten.zeros_like.default(slice_2, pin_memory = False);  slice_2 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:90 in get_whisper_chunk, code: torch.zeros_like(whisper_feature[:, :padding_nums * 3 * self.audio_padding_length_right])\n",
       "                    slice_3: \"f32[1, 12, 5, 384]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 12)\n",
       "                    zeros_like_1: \"f32[1, 12, 5, 384]\" = torch.ops.aten.zeros_like.default(slice_3, pin_memory = False);  slice_3 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:86 in get_whisper_chunk, code: whisper_feature = torch.cat([\n",
       "                    cat_1: \"f32[1, 348, 5, 384]\" = torch.ops.aten.cat.default([zeros_like, slice_1, zeros_like_1], 1);  zeros_like = slice_1 = zeros_like_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:96 in get_whisper_chunk, code: audio_clip = whisper_feature[:, audio_index: audio_index + audio_feature_length_per_frame]\n",
       "                    slice_4: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 0, 10)\n",
       "                    slice_5: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 2, 12)\n",
       "                    slice_6: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 4, 14)\n",
       "                    slice_7: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 6, 16)\n",
       "                    slice_8: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 8, 18)\n",
       "                    slice_9: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 10, 20)\n",
       "                    slice_10: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 12, 22)\n",
       "                    slice_11: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 14, 24)\n",
       "                    slice_12: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 16, 26)\n",
       "                    slice_13: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 18, 28)\n",
       "                    slice_14: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 20, 30)\n",
       "                    slice_15: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 22, 32)\n",
       "                    slice_16: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 24, 34)\n",
       "                    slice_17: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 26, 36)\n",
       "                    slice_18: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 28, 38)\n",
       "                    slice_19: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 30, 40)\n",
       "                    slice_20: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 32, 42)\n",
       "                    slice_21: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 34, 44)\n",
       "                    slice_22: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 36, 46)\n",
       "                    slice_23: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 38, 48)\n",
       "                    slice_24: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 40, 50)\n",
       "                    slice_25: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 42, 52)\n",
       "                    slice_26: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 44, 54)\n",
       "                    slice_27: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 46, 56)\n",
       "                    slice_28: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 48, 58)\n",
       "                    slice_29: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 50, 60)\n",
       "                    slice_30: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 52, 62)\n",
       "                    slice_31: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 54, 64)\n",
       "                    slice_32: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 56, 66)\n",
       "                    slice_33: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 58, 68)\n",
       "                    slice_34: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 60, 70)\n",
       "                    slice_35: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 62, 72)\n",
       "                    slice_36: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 64, 74)\n",
       "                    slice_37: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 66, 76)\n",
       "                    slice_38: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 68, 78)\n",
       "                    slice_39: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 70, 80)\n",
       "                    slice_40: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 72, 82)\n",
       "                    slice_41: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 74, 84)\n",
       "                    slice_42: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 76, 86)\n",
       "                    slice_43: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 78, 88)\n",
       "                    slice_44: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 80, 90)\n",
       "                    slice_45: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 82, 92)\n",
       "                    slice_46: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 84, 94)\n",
       "                    slice_47: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 86, 96)\n",
       "                    slice_48: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 88, 98)\n",
       "                    slice_49: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 90, 100)\n",
       "                    slice_50: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 92, 102)\n",
       "                    slice_51: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 94, 104)\n",
       "                    slice_52: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 96, 106)\n",
       "                    slice_53: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 98, 108)\n",
       "                    slice_54: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 100, 110)\n",
       "                    slice_55: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 102, 112)\n",
       "                    slice_56: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 104, 114)\n",
       "                    slice_57: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 106, 116)\n",
       "                    slice_58: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 108, 118)\n",
       "                    slice_59: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 110, 120)\n",
       "                    slice_60: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 112, 122)\n",
       "                    slice_61: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 114, 124)\n",
       "                    slice_62: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 116, 126)\n",
       "                    slice_63: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 118, 128)\n",
       "                    slice_64: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 120, 130)\n",
       "                    slice_65: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 122, 132)\n",
       "                    slice_66: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 124, 134)\n",
       "                    slice_67: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 126, 136)\n",
       "                    slice_68: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 128, 138)\n",
       "                    slice_69: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 130, 140)\n",
       "                    slice_70: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 132, 142)\n",
       "                    slice_71: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 134, 144)\n",
       "                    slice_72: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 136, 146)\n",
       "                    slice_73: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 138, 148)\n",
       "                    slice_74: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 140, 150)\n",
       "                    slice_75: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 142, 152)\n",
       "                    slice_76: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 144, 154)\n",
       "                    slice_77: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 146, 156)\n",
       "                    slice_78: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 148, 158)\n",
       "                    slice_79: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 150, 160)\n",
       "                    slice_80: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 152, 162)\n",
       "                    slice_81: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 154, 164)\n",
       "                    slice_82: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 156, 166)\n",
       "                    slice_83: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 158, 168)\n",
       "                    slice_84: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 160, 170)\n",
       "                    slice_85: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 162, 172)\n",
       "                    slice_86: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 164, 174)\n",
       "                    slice_87: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 166, 176)\n",
       "                    slice_88: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 168, 178)\n",
       "                    slice_89: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 170, 180)\n",
       "                    slice_90: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 172, 182)\n",
       "                    slice_91: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 174, 184)\n",
       "                    slice_92: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 176, 186)\n",
       "                    slice_93: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 178, 188)\n",
       "                    slice_94: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 180, 190)\n",
       "                    slice_95: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 182, 192)\n",
       "                    slice_96: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 184, 194)\n",
       "                    slice_97: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 186, 196)\n",
       "                    slice_98: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 188, 198)\n",
       "                    slice_99: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 190, 200)\n",
       "                    slice_100: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 192, 202)\n",
       "                    slice_101: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 194, 204)\n",
       "                    slice_102: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 196, 206)\n",
       "                    slice_103: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 198, 208)\n",
       "                    slice_104: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 200, 210)\n",
       "                    slice_105: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 202, 212)\n",
       "                    slice_106: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 204, 214)\n",
       "                    slice_107: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 206, 216)\n",
       "                    slice_108: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 208, 218)\n",
       "                    slice_109: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 210, 220)\n",
       "                    slice_110: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 212, 222)\n",
       "                    slice_111: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 214, 224)\n",
       "                    slice_112: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 216, 226)\n",
       "                    slice_113: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 218, 228)\n",
       "                    slice_114: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 220, 230)\n",
       "                    slice_115: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 222, 232)\n",
       "                    slice_116: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 224, 234)\n",
       "                    slice_117: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 226, 236)\n",
       "                    slice_118: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 228, 238)\n",
       "                    slice_119: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 230, 240)\n",
       "                    slice_120: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 232, 242)\n",
       "                    slice_121: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 234, 244)\n",
       "                    slice_122: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 236, 246)\n",
       "                    slice_123: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 238, 248)\n",
       "                    slice_124: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 240, 250)\n",
       "                    slice_125: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 242, 252)\n",
       "                    slice_126: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 244, 254)\n",
       "                    slice_127: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 246, 256)\n",
       "                    slice_128: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 248, 258)\n",
       "                    slice_129: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 250, 260)\n",
       "                    slice_130: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 252, 262)\n",
       "                    slice_131: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 254, 264)\n",
       "                    slice_132: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 256, 266)\n",
       "                    slice_133: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 258, 268)\n",
       "                    slice_134: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 260, 270)\n",
       "                    slice_135: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 262, 272)\n",
       "                    slice_136: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 264, 274)\n",
       "                    slice_137: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 266, 276)\n",
       "                    slice_138: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 268, 278)\n",
       "                    slice_139: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 270, 280)\n",
       "                    slice_140: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 272, 282)\n",
       "                    slice_141: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 274, 284)\n",
       "                    slice_142: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 276, 286)\n",
       "                    slice_143: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 278, 288)\n",
       "                    slice_144: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 280, 290)\n",
       "                    slice_145: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 282, 292)\n",
       "                    slice_146: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 284, 294)\n",
       "                    slice_147: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 286, 296)\n",
       "                    slice_148: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 288, 298)\n",
       "                    slice_149: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 290, 300)\n",
       "                    slice_150: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 292, 302)\n",
       "                    slice_151: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 294, 304)\n",
       "                    slice_152: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 296, 306)\n",
       "                    slice_153: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 298, 308)\n",
       "                    slice_154: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 300, 310)\n",
       "                    slice_155: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 302, 312)\n",
       "                    slice_156: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 304, 314)\n",
       "                    slice_157: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 306, 316)\n",
       "                    slice_158: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 308, 318)\n",
       "                    slice_159: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 310, 320)\n",
       "                    slice_160: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 312, 322)\n",
       "                    slice_161: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 314, 324)\n",
       "                    slice_162: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 316, 326)\n",
       "                    slice_163: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 318, 328)\n",
       "                    slice_164: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 320, 330)\n",
       "                    slice_165: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 322, 332)\n",
       "                    slice_166: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 324, 334)\n",
       "                    slice_167: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 326, 336)\n",
       "                    slice_168: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 328, 338)\n",
       "                    slice_169: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 330, 340);  cat_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:99 in get_whisper_chunk, code: audio_prompts = torch.cat(audio_prompts, dim=0)  # T, 10, 5, 384\n",
       "                    cat_2: \"f32[166, 10, 5, 384]\" = torch.ops.aten.cat.default([slice_4, slice_5, slice_6, slice_7, slice_8, slice_9, slice_10, slice_11, slice_12, slice_13, slice_14, slice_15, slice_16, slice_17, slice_18, slice_19, slice_20, slice_21, slice_22, slice_23, slice_24, slice_25, slice_26, slice_27, slice_28, slice_29, slice_30, slice_31, slice_32, slice_33, slice_34, slice_35, slice_36, slice_37, slice_38, slice_39, slice_40, slice_41, slice_42, slice_43, slice_44, slice_45, slice_46, slice_47, slice_48, slice_49, slice_50, slice_51, slice_52, slice_53, slice_54, slice_55, slice_56, slice_57, slice_58, slice_59, slice_60, slice_61, slice_62, slice_63, slice_64, slice_65, slice_66, slice_67, slice_68, slice_69, slice_70, slice_71, slice_72, slice_73, slice_74, slice_75, slice_76, slice_77, slice_78, slice_79, slice_80, slice_81, slice_82, slice_83, slice_84, slice_85, slice_86, slice_87, slice_88, slice_89, slice_90, slice_91, slice_92, slice_93, slice_94, slice_95, slice_96, slice_97, slice_98, slice_99, slice_100, slice_101, slice_102, slice_103, slice_104, slice_105, slice_106, slice_107, slice_108, slice_109, slice_110, slice_111, slice_112, slice_113, slice_114, slice_115, slice_116, slice_117, slice_118, slice_119, slice_120, slice_121, slice_122, slice_123, slice_124, slice_125, slice_126, slice_127, slice_128, slice_129, slice_130, slice_131, slice_132, slice_133, slice_134, slice_135, slice_136, slice_137, slice_138, slice_139, slice_140, slice_141, slice_142, slice_143, slice_144, slice_145, slice_146, slice_147, slice_148, slice_149, slice_150, slice_151, slice_152, slice_153, slice_154, slice_155, slice_156, slice_157, slice_158, slice_159, slice_160, slice_161, slice_162, slice_163, slice_164, slice_165, slice_166, slice_167, slice_168, slice_169]);  slice_4 = slice_5 = slice_6 = slice_7 = slice_8 = slice_9 = slice_10 = slice_11 = slice_12 = slice_13 = slice_14 = slice_15 = slice_16 = slice_17 = slice_18 = slice_19 = slice_20 = slice_21 = slice_22 = slice_23 = slice_24 = slice_25 = slice_26 = slice_27 = slice_28 = slice_29 = slice_30 = slice_31 = slice_32 = slice_33 = slice_34 = slice_35 = slice_36 = slice_37 = slice_38 = slice_39 = slice_40 = slice_41 = slice_42 = slice_43 = slice_44 = slice_45 = slice_46 = slice_47 = slice_48 = slice_49 = slice_50 = slice_51 = slice_52 = slice_53 = slice_54 = slice_55 = slice_56 = slice_57 = slice_58 = slice_59 = slice_60 = slice_61 = slice_62 = slice_63 = slice_64 = slice_65 = slice_66 = slice_67 = slice_68 = slice_69 = slice_70 = slice_71 = slice_72 = slice_73 = slice_74 = slice_75 = slice_76 = slice_77 = slice_78 = slice_79 = slice_80 = slice_81 = slice_82 = slice_83 = slice_84 = slice_85 = slice_86 = slice_87 = slice_88 = slice_89 = slice_90 = slice_91 = slice_92 = slice_93 = slice_94 = slice_95 = slice_96 = slice_97 = slice_98 = slice_99 = slice_100 = slice_101 = slice_102 = slice_103 = slice_104 = slice_105 = slice_106 = slice_107 = slice_108 = slice_109 = slice_110 = slice_111 = slice_112 = slice_113 = slice_114 = slice_115 = slice_116 = slice_117 = slice_118 = slice_119 = slice_120 = slice_121 = slice_122 = slice_123 = slice_124 = slice_125 = slice_126 = slice_127 = slice_128 = slice_129 = slice_130 = slice_131 = slice_132 = slice_133 = slice_134 = slice_135 = slice_136 = slice_137 = slice_138 = slice_139 = slice_140 = slice_141 = slice_142 = slice_143 = slice_144 = slice_145 = slice_146 = slice_147 = slice_148 = slice_149 = slice_150 = slice_151 = slice_152 = slice_153 = slice_154 = slice_155 = slice_156 = slice_157 = slice_158 = slice_159 = slice_160 = slice_161 = slice_162 = slice_163 = slice_164 = slice_165 = slice_166 = slice_167 = slice_168 = slice_169 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:100 in get_whisper_chunk, code: audio_prompts = rearrange(audio_prompts, 'b c h w -> b (c h) w')\n",
       "                    view_16: \"f32[166, 50, 384]\" = torch.ops.aten.view.default(cat_2, [166, 50, 384]);  cat_2 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:119 in forward, code: context_whisper_chunk = whisper_chunks[frame_idx: frame_idx+batch_size]  # shape [batch_size, 50, 384]\n",
       "                    slice_170: \"f32[8, 50, 384]\" = torch.ops.aten.slice.Tensor(view_16, 0, 0, 8);  view_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/musetalk/models/unet.py:25 in forward, code: pe = self.pe[:, :seq_len, :]\n",
       "                    slice_171: \"f32[1, 50, 384]\" = torch.ops.aten.slice.Tensor(b_pe_pe, 1, 0, 50);  b_pe_pe = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/musetalk/models/unet.py:26 in forward, code: x = x + pe.to(x.device)\n",
       "                    _to_copy_1: \"f32[1, 50, 384]\" = torch.ops.aten._to_copy.default(slice_171, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  slice_171 = None\n",
       "                    add_9: \"f32[8, 50, 384]\" = torch.ops.aten.add.Tensor(slice_170, _to_copy_1);  slice_170 = _to_copy_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:122 in forward, code: latent_batch = latent_inputs.to(device=self.unet_device, dtype=self.unet.dtype)\n",
       "                    _to_copy_2: \"f32[8, 8, 32, 32]\" = torch.ops.aten._to_copy.default(latent_inputs, dtype = torch.float32, device = device(type='cuda', index=0));  latent_inputs = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:926 in get_time_embed, code: timesteps = timesteps.expand(sample.shape[0])\n",
       "                    expand: \"i64[8]\" = torch.ops.aten.expand.default(c_timesteps, [8]);  c_timesteps = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:57 in get_timestep_embedding, code: exponent = -math.log(max_period) * torch.arange(\n",
       "                    arange: \"f32[160]\" = torch.ops.aten.arange.start(0, 160, dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)\n",
       "                    mul: \"f32[160]\" = torch.ops.aten.mul.Tensor(arange, -9.210340371976184);  arange = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:60 in get_timestep_embedding, code: exponent = exponent / (half_dim - downscale_freq_shift)\n",
       "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(160, dtype = torch.float32)\n",
       "                    div: \"f32[160]\" = torch.ops.aten.div.Tensor(mul, scalar_tensor_default);  mul = scalar_tensor_default = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:62 in get_timestep_embedding, code: emb = torch.exp(exponent)\n",
       "                    exp: \"f32[160]\" = torch.ops.aten.exp.default(div);  div = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:63 in get_timestep_embedding, code: emb = timesteps[:, None].float() * emb[None, :]\n",
       "                    unsqueeze: \"i64[8, 1]\" = torch.ops.aten.unsqueeze.default(expand, 1);  expand = None\n",
       "                    _to_copy_3: \"f32[8, 1]\" = torch.ops.aten._to_copy.default(unsqueeze, dtype = torch.float32);  unsqueeze = None\n",
       "                    unsqueeze_1: \"f32[1, 160]\" = torch.ops.aten.unsqueeze.default(exp, 0);  exp = None\n",
       "                    mul_1: \"f32[8, 160]\" = torch.ops.aten.mul.Tensor(_to_copy_3, unsqueeze_1);  _to_copy_3 = unsqueeze_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:66 in get_timestep_embedding, code: emb = scale * emb\n",
       "                    scalar_tensor_default_1: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    mul_2: \"f32[8, 160]\" = torch.ops.aten.mul.Tensor(mul_1, scalar_tensor_default_1);  mul_1 = scalar_tensor_default_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:69 in get_timestep_embedding, code: emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
       "                    sin: \"f32[8, 160]\" = torch.ops.aten.sin.default(mul_2)\n",
       "                    cos: \"f32[8, 160]\" = torch.ops.aten.cos.default(mul_2);  mul_2 = None\n",
       "                    cat_3: \"f32[8, 320]\" = torch.ops.aten.cat.default([sin, cos], -1);  sin = cos = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:73 in get_timestep_embedding, code: emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n",
       "                    slice_172: \"f32[8, 160]\" = torch.ops.aten.slice.Tensor(cat_3, 1, 160, 9223372036854775807)\n",
       "                    slice_173: \"f32[8, 160]\" = torch.ops.aten.slice.Tensor(cat_3, 1, 0, 160);  cat_3 = None\n",
       "                    cat_4: \"f32[8, 320]\" = torch.ops.aten.cat.default([slice_172, slice_173], -1);  slice_172 = slice_173 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:663 in forward, code: sample = self.linear_1(sample)\n",
       "                    linear_24: \"f32[8, 1280]\" = torch.ops.aten.linear.default(cat_4, p_unet_time_embedding_linear_1_weight, p_unet_time_embedding_linear_1_bias);  cat_4 = p_unet_time_embedding_linear_1_weight = p_unet_time_embedding_linear_1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:666 in forward, code: sample = self.act(sample)\n",
       "                    silu: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_24);  linear_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:668 in forward, code: sample = self.linear_2(sample)\n",
       "                    linear_25: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu, p_unet_time_embedding_linear_2_weight, p_unet_time_embedding_linear_2_bias);  silu = p_unet_time_embedding_linear_2_weight = p_unet_time_embedding_linear_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1169 in forward, code: sample = self.conv_in(sample)\n",
       "                    conv2d: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(_to_copy_2, p_unet_conv_in_weight, p_unet_conv_in_bias, [1, 1], [1, 1]);  _to_copy_2 = p_unet_conv_in_weight = p_unet_conv_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d, 32, p_unet_down_blocks_0_resnets_0_norm1_weight, p_unet_down_blocks_0_resnets_0_norm1_bias);  p_unet_down_blocks_0_resnets_0_norm1_weight = p_unet_down_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm);  group_norm = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_1, p_unet_down_blocks_0_resnets_0_conv1_weight, p_unet_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_1 = p_unet_down_blocks_0_resnets_0_conv1_weight = p_unet_down_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_2: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_26: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_2, p_unet_down_blocks_0_resnets_0_time_emb_proj_weight, p_unet_down_blocks_0_resnets_0_time_emb_proj_bias);  silu_2 = p_unet_down_blocks_0_resnets_0_time_emb_proj_weight = p_unet_down_blocks_0_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_2: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_26, 2);  linear_26 = None\n",
       "                    unsqueeze_3: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_2, 3);  unsqueeze_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_10: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_1, unsqueeze_3);  conv2d_1 = unsqueeze_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_10, 32, p_unet_down_blocks_0_resnets_0_norm2_weight, p_unet_down_blocks_0_resnets_0_norm2_bias);  add_10 = p_unet_down_blocks_0_resnets_0_norm2_weight = p_unet_down_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_1);  group_norm_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_25: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_3);  silu_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_25, p_unet_down_blocks_0_resnets_0_conv2_weight, p_unet_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_25 = p_unet_down_blocks_0_resnets_0_conv2_weight = p_unet_down_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_11: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d, conv2d_2);  conv2d_2 = None\n",
       "                    div_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_11, 1.0);  add_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_1, 32, p_unet_down_blocks_0_attentions_0_norm_weight, p_unet_down_blocks_0_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_0_attentions_0_norm_weight = p_unet_down_blocks_0_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_2, p_unet_down_blocks_0_attentions_0_proj_in_weight, p_unet_down_blocks_0_attentions_0_proj_in_bias);  group_norm_2 = p_unet_down_blocks_0_attentions_0_proj_in_weight = p_unet_down_blocks_0_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_1: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_3, [0, 2, 3, 1]);  conv2d_3 = None\n",
       "                    view_17: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_1, [8, 1024, 320]);  permute_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_9: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_17, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_27: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_28: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_29: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_9 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_18: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_27, [8, -1, 8, 40]);  linear_27 = None\n",
       "                    transpose_16: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_19: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_28, [8, -1, 8, 40]);  linear_28 = None\n",
       "                    transpose_17: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_19, 1, 2);  view_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_20: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_29, [8, -1, 8, 40]);  linear_29 = None\n",
       "                    transpose_18: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_4: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_16, transpose_17, transpose_18);  transpose_16 = transpose_17 = transpose_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_19: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
       "                    view_21: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_19, [8, -1, 320]);  transpose_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_30: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_21, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_21 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_26: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_30);  linear_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_2: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_26, 1.0);  clone_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_12: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_2, view_17);  div_2 = view_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_10: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_12, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_31: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_10, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_10 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_32: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_33: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_22: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_31, [8, -1, 8, 40]);  linear_31 = None\n",
       "                    transpose_20: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_23: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_32, [8, -1, 8, 40]);  linear_32 = None\n",
       "                    transpose_21: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_23, 1, 2);  view_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_24: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_33, [8, -1, 8, 40]);  linear_33 = None\n",
       "                    transpose_22: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_24, 1, 2);  view_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_5: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_20, transpose_21, transpose_22);  transpose_20 = transpose_21 = transpose_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_23: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
       "                    view_25: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_23, [8, -1, 320]);  transpose_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_34: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_25, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_25 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_27: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_3: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_27, 1.0);  clone_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_13: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_3, add_12);  div_3 = add_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_11: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_13, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_35: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_11, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_11 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split = torch.ops.aten.split.Tensor(linear_35, 1280, -1);  linear_35 = None\n",
       "                    getitem: \"f32[8, 1024, 1280]\" = split[0]\n",
       "                    getitem_1: \"f32[8, 1024, 1280]\" = split[1];  split = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_6: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_1);  getitem_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_3: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem, gelu_6);  getitem = gelu_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_28: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_3);  mul_3 = None\n",
       "                    linear_36: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_28, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_28 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_14: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_36, add_13);  linear_36 = add_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_26: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_14, [8, 32, 32, 320]);  add_14 = None\n",
       "                    permute_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_26, [0, 3, 1, 2]);  view_26 = None\n",
       "                    clone_29: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_29, p_unet_down_blocks_0_attentions_0_proj_out_weight, p_unet_down_blocks_0_attentions_0_proj_out_bias);  clone_29 = p_unet_down_blocks_0_attentions_0_proj_out_weight = p_unet_down_blocks_0_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_15: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_4, div_1);  conv2d_4 = div_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_15, 32, p_unet_down_blocks_0_resnets_1_norm1_weight, p_unet_down_blocks_0_resnets_1_norm1_bias);  p_unet_down_blocks_0_resnets_1_norm1_weight = p_unet_down_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_3);  group_norm_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_5: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_4, p_unet_down_blocks_0_resnets_1_conv1_weight, p_unet_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_4 = p_unet_down_blocks_0_resnets_1_conv1_weight = p_unet_down_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_5: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_37: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_5, p_unet_down_blocks_0_resnets_1_time_emb_proj_weight, p_unet_down_blocks_0_resnets_1_time_emb_proj_bias);  silu_5 = p_unet_down_blocks_0_resnets_1_time_emb_proj_weight = p_unet_down_blocks_0_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_4: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_37, 2);  linear_37 = None\n",
       "                    unsqueeze_5: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_16: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_5, unsqueeze_5);  conv2d_5 = unsqueeze_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_16, 32, p_unet_down_blocks_0_resnets_1_norm2_weight, p_unet_down_blocks_0_resnets_1_norm2_bias);  add_16 = p_unet_down_blocks_0_resnets_1_norm2_weight = p_unet_down_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_6: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_4);  group_norm_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_30: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_6);  silu_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_6: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_30, p_unet_down_blocks_0_resnets_1_conv2_weight, p_unet_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_30 = p_unet_down_blocks_0_resnets_1_conv2_weight = p_unet_down_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_17: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(add_15, conv2d_6);  conv2d_6 = None\n",
       "                    div_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_17, 1.0);  add_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_5: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_4, 32, p_unet_down_blocks_0_attentions_1_norm_weight, p_unet_down_blocks_0_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_0_attentions_1_norm_weight = p_unet_down_blocks_0_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_7: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_5, p_unet_down_blocks_0_attentions_1_proj_in_weight, p_unet_down_blocks_0_attentions_1_proj_in_bias);  group_norm_5 = p_unet_down_blocks_0_attentions_1_proj_in_weight = p_unet_down_blocks_0_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_3: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_7, [0, 2, 3, 1]);  conv2d_7 = None\n",
       "                    view_27: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_3, [8, 1024, 320]);  permute_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_12: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_27, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_38: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_39: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_40: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_12 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_28: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_38, [8, -1, 8, 40]);  linear_38 = None\n",
       "                    transpose_24: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_28, 1, 2);  view_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_29: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_39, [8, -1, 8, 40]);  linear_39 = None\n",
       "                    transpose_25: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_30: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_40, [8, -1, 8, 40]);  linear_40 = None\n",
       "                    transpose_26: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_6: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_24, transpose_25, transpose_26);  transpose_24 = transpose_25 = transpose_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_27: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
       "                    view_31: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_27, [8, -1, 320]);  transpose_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_41: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_31, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_31 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_31: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_5: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_31, 1.0);  clone_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_18: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_5, view_27);  div_5 = view_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_13: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_18, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_42: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_13, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_13 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_43: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_44: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_32: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_42, [8, -1, 8, 40]);  linear_42 = None\n",
       "                    transpose_28: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_33: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_43, [8, -1, 8, 40]);  linear_43 = None\n",
       "                    transpose_29: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_33, 1, 2);  view_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_34: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_44, [8, -1, 8, 40]);  linear_44 = None\n",
       "                    transpose_30: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_34, 1, 2);  view_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_7: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_28, transpose_29, transpose_30);  transpose_28 = transpose_29 = transpose_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_31: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
       "                    view_35: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_31, [8, -1, 320]);  transpose_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_45: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_35, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_35 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_32: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_6: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_32, 1.0);  clone_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_19: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_6, add_18);  div_6 = add_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_14: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_19, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_46: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_14, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_14 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_1 = torch.ops.aten.split.Tensor(linear_46, 1280, -1);  linear_46 = None\n",
       "                    getitem_2: \"f32[8, 1024, 1280]\" = split_1[0]\n",
       "                    getitem_3: \"f32[8, 1024, 1280]\" = split_1[1];  split_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_7: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_4: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_2, gelu_7);  getitem_2 = gelu_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_33: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_4);  mul_4 = None\n",
       "                    linear_47: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_33, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_33 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_20: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_47, add_19);  linear_47 = add_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_36: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_20, [8, 32, 32, 320]);  add_20 = None\n",
       "                    permute_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_36, [0, 3, 1, 2]);  view_36 = None\n",
       "                    clone_34: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_8: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_34, p_unet_down_blocks_0_attentions_1_proj_out_weight, p_unet_down_blocks_0_attentions_1_proj_out_bias);  clone_34 = p_unet_down_blocks_0_attentions_1_proj_out_weight = p_unet_down_blocks_0_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_21: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_8, div_4);  conv2d_8 = div_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_9: \"f32[8, 320, 16, 16]\" = torch.ops.aten.conv2d.default(add_21, p_unet_down_blocks_0_downsamplers_0_conv_weight, p_unet_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_0_downsamplers_0_conv_weight = p_unet_down_blocks_0_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_6: \"f32[8, 320, 16, 16]\" = torch.ops.aten.group_norm.default(conv2d_9, 32, p_unet_down_blocks_1_resnets_0_norm1_weight, p_unet_down_blocks_1_resnets_0_norm1_bias);  p_unet_down_blocks_1_resnets_0_norm1_weight = p_unet_down_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_7: \"f32[8, 320, 16, 16]\" = torch.ops.aten.silu.default(group_norm_6);  group_norm_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_7, p_unet_down_blocks_1_resnets_0_conv1_weight, p_unet_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_7 = p_unet_down_blocks_1_resnets_0_conv1_weight = p_unet_down_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_8: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_48: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_8, p_unet_down_blocks_1_resnets_0_time_emb_proj_weight, p_unet_down_blocks_1_resnets_0_time_emb_proj_bias);  silu_8 = p_unet_down_blocks_1_resnets_0_time_emb_proj_weight = p_unet_down_blocks_1_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_6: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_48, 2);  linear_48 = None\n",
       "                    unsqueeze_7: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 3);  unsqueeze_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_22: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_10, unsqueeze_7);  conv2d_10 = unsqueeze_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_7: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_22, 32, p_unet_down_blocks_1_resnets_0_norm2_weight, p_unet_down_blocks_1_resnets_0_norm2_bias);  add_22 = p_unet_down_blocks_1_resnets_0_norm2_weight = p_unet_down_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_9: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_7);  group_norm_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_35: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_9);  silu_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_11: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_35, p_unet_down_blocks_1_resnets_0_conv2_weight, p_unet_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_35 = p_unet_down_blocks_1_resnets_0_conv2_weight = p_unet_down_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_12: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(conv2d_9, p_unet_down_blocks_1_resnets_0_conv_shortcut_weight, p_unet_down_blocks_1_resnets_0_conv_shortcut_bias);  p_unet_down_blocks_1_resnets_0_conv_shortcut_weight = p_unet_down_blocks_1_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_23: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_12, conv2d_11);  conv2d_12 = conv2d_11 = None\n",
       "                    div_7: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_23, 1.0);  add_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_8: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_7, 32, p_unet_down_blocks_1_attentions_0_norm_weight, p_unet_down_blocks_1_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_1_attentions_0_norm_weight = p_unet_down_blocks_1_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_13: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_8, p_unet_down_blocks_1_attentions_0_proj_in_weight, p_unet_down_blocks_1_attentions_0_proj_in_bias);  group_norm_8 = p_unet_down_blocks_1_attentions_0_proj_in_weight = p_unet_down_blocks_1_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_5: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_13, [0, 2, 3, 1]);  conv2d_13 = None\n",
       "                    view_37: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_5, [8, 256, 640]);  permute_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_15: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_37, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_49: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_50: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_51: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_15 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_38: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_49, [8, -1, 8, 80]);  linear_49 = None\n",
       "                    transpose_32: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_38, 1, 2);  view_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_39: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_50, [8, -1, 8, 80]);  linear_50 = None\n",
       "                    transpose_33: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_39, 1, 2);  view_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_40: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_51, [8, -1, 8, 80]);  linear_51 = None\n",
       "                    transpose_34: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_40, 1, 2);  view_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_8: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_32, transpose_33, transpose_34);  transpose_32 = transpose_33 = transpose_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_35: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
       "                    view_41: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_35, [8, -1, 640]);  transpose_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_52: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_41, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_41 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_36: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_8: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_36, 1.0);  clone_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_24: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_8, view_37);  div_8 = view_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_16: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_24, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_53: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_16, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_16 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_54: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_55: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_42: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_53, [8, -1, 8, 80]);  linear_53 = None\n",
       "                    transpose_36: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_42, 1, 2);  view_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_43: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_54, [8, -1, 8, 80]);  linear_54 = None\n",
       "                    transpose_37: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_43, 1, 2);  view_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_44: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_55, [8, -1, 8, 80]);  linear_55 = None\n",
       "                    transpose_38: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_44, 1, 2);  view_44 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_9: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_36, transpose_37, transpose_38);  transpose_36 = transpose_37 = transpose_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_39: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
       "                    view_45: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_39, [8, -1, 640]);  transpose_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_56: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_45, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_45 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_37: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_56);  linear_56 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_9: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_37, 1.0);  clone_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_25: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_9, add_24);  div_9 = add_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_17: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_25, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_57: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_17, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_17 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_2 = torch.ops.aten.split.Tensor(linear_57, 2560, -1);  linear_57 = None\n",
       "                    getitem_4: \"f32[8, 256, 2560]\" = split_2[0]\n",
       "                    getitem_5: \"f32[8, 256, 2560]\" = split_2[1];  split_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_8: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_5);  getitem_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_5: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_4, gelu_8);  getitem_4 = gelu_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_38: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_5);  mul_5 = None\n",
       "                    linear_58: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_38, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_38 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_26: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_58, add_25);  linear_58 = add_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_46: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_26, [8, 16, 16, 640]);  add_26 = None\n",
       "                    permute_6: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_46, [0, 3, 1, 2]);  view_46 = None\n",
       "                    clone_39: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_14: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_39, p_unet_down_blocks_1_attentions_0_proj_out_weight, p_unet_down_blocks_1_attentions_0_proj_out_bias);  clone_39 = p_unet_down_blocks_1_attentions_0_proj_out_weight = p_unet_down_blocks_1_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_27: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_14, div_7);  conv2d_14 = div_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_9: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_27, 32, p_unet_down_blocks_1_resnets_1_norm1_weight, p_unet_down_blocks_1_resnets_1_norm1_bias);  p_unet_down_blocks_1_resnets_1_norm1_weight = p_unet_down_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_9);  group_norm_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_15: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_10, p_unet_down_blocks_1_resnets_1_conv1_weight, p_unet_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_10 = p_unet_down_blocks_1_resnets_1_conv1_weight = p_unet_down_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_11: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_59: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_11, p_unet_down_blocks_1_resnets_1_time_emb_proj_weight, p_unet_down_blocks_1_resnets_1_time_emb_proj_bias);  silu_11 = p_unet_down_blocks_1_resnets_1_time_emb_proj_weight = p_unet_down_blocks_1_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_8: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_59, 2);  linear_59 = None\n",
       "                    unsqueeze_9: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_8, 3);  unsqueeze_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_28: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_15, unsqueeze_9);  conv2d_15 = unsqueeze_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_28, 32, p_unet_down_blocks_1_resnets_1_norm2_weight, p_unet_down_blocks_1_resnets_1_norm2_bias);  add_28 = p_unet_down_blocks_1_resnets_1_norm2_weight = p_unet_down_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_12: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_10);  group_norm_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_40: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_12);  silu_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_16: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_40, p_unet_down_blocks_1_resnets_1_conv2_weight, p_unet_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_40 = p_unet_down_blocks_1_resnets_1_conv2_weight = p_unet_down_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_29: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(add_27, conv2d_16);  conv2d_16 = None\n",
       "                    div_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_29, 1.0);  add_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_11: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_10, 32, p_unet_down_blocks_1_attentions_1_norm_weight, p_unet_down_blocks_1_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_1_attentions_1_norm_weight = p_unet_down_blocks_1_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_17: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_11, p_unet_down_blocks_1_attentions_1_proj_in_weight, p_unet_down_blocks_1_attentions_1_proj_in_bias);  group_norm_11 = p_unet_down_blocks_1_attentions_1_proj_in_weight = p_unet_down_blocks_1_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_7: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_17, [0, 2, 3, 1]);  conv2d_17 = None\n",
       "                    view_47: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_7, [8, 256, 640]);  permute_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_18: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_47, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_60: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_61: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_62: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_18 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_48: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_60, [8, -1, 8, 80]);  linear_60 = None\n",
       "                    transpose_40: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_48, 1, 2);  view_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_49: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_61, [8, -1, 8, 80]);  linear_61 = None\n",
       "                    transpose_41: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_49, 1, 2);  view_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_50: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_62, [8, -1, 8, 80]);  linear_62 = None\n",
       "                    transpose_42: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_50, 1, 2);  view_50 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_10: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_40, transpose_41, transpose_42);  transpose_40 = transpose_41 = transpose_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_43: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
       "                    view_51: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_43, [8, -1, 640]);  transpose_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_63: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_51, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_51 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_41: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_11: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_41, 1.0);  clone_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_30: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_11, view_47);  div_11 = view_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_19: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_30, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_64: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_19, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_19 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_65: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_66: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_52: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_64, [8, -1, 8, 80]);  linear_64 = None\n",
       "                    transpose_44: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_52, 1, 2);  view_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_53: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_65, [8, -1, 8, 80]);  linear_65 = None\n",
       "                    transpose_45: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_53, 1, 2);  view_53 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_54: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_66, [8, -1, 8, 80]);  linear_66 = None\n",
       "                    transpose_46: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_54, 1, 2);  view_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_11: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_44, transpose_45, transpose_46);  transpose_44 = transpose_45 = transpose_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_47: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
       "                    view_55: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_47, [8, -1, 640]);  transpose_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_67: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_55, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_55 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_42: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_67);  linear_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_12: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_42, 1.0);  clone_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_31: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_12, add_30);  div_12 = add_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_20: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_31, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_68: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_20, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_20 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_3 = torch.ops.aten.split.Tensor(linear_68, 2560, -1);  linear_68 = None\n",
       "                    getitem_6: \"f32[8, 256, 2560]\" = split_3[0]\n",
       "                    getitem_7: \"f32[8, 256, 2560]\" = split_3[1];  split_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_9: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_7);  getitem_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_6: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_6, gelu_9);  getitem_6 = gelu_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_43: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_6);  mul_6 = None\n",
       "                    linear_69: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_43, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_43 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_32: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_69, add_31);  linear_69 = add_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_56: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_32, [8, 16, 16, 640]);  add_32 = None\n",
       "                    permute_8: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_56, [0, 3, 1, 2]);  view_56 = None\n",
       "                    clone_44: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_18: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_44, p_unet_down_blocks_1_attentions_1_proj_out_weight, p_unet_down_blocks_1_attentions_1_proj_out_bias);  clone_44 = p_unet_down_blocks_1_attentions_1_proj_out_weight = p_unet_down_blocks_1_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_33: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_18, div_10);  conv2d_18 = div_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_19: \"f32[8, 640, 8, 8]\" = torch.ops.aten.conv2d.default(add_33, p_unet_down_blocks_1_downsamplers_0_conv_weight, p_unet_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_1_downsamplers_0_conv_weight = p_unet_down_blocks_1_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_12: \"f32[8, 640, 8, 8]\" = torch.ops.aten.group_norm.default(conv2d_19, 32, p_unet_down_blocks_2_resnets_0_norm1_weight, p_unet_down_blocks_2_resnets_0_norm1_bias);  p_unet_down_blocks_2_resnets_0_norm1_weight = p_unet_down_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_13: \"f32[8, 640, 8, 8]\" = torch.ops.aten.silu.default(group_norm_12);  group_norm_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_20: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_13, p_unet_down_blocks_2_resnets_0_conv1_weight, p_unet_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_13 = p_unet_down_blocks_2_resnets_0_conv1_weight = p_unet_down_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_14: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_70: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_14, p_unet_down_blocks_2_resnets_0_time_emb_proj_weight, p_unet_down_blocks_2_resnets_0_time_emb_proj_bias);  silu_14 = p_unet_down_blocks_2_resnets_0_time_emb_proj_weight = p_unet_down_blocks_2_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_10: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_70, 2);  linear_70 = None\n",
       "                    unsqueeze_11: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_10, 3);  unsqueeze_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_20, unsqueeze_11);  conv2d_20 = unsqueeze_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_13: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_34, 32, p_unet_down_blocks_2_resnets_0_norm2_weight, p_unet_down_blocks_2_resnets_0_norm2_bias);  add_34 = p_unet_down_blocks_2_resnets_0_norm2_weight = p_unet_down_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_15: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_13);  group_norm_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_15);  silu_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_21: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_45, p_unet_down_blocks_2_resnets_0_conv2_weight, p_unet_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_45 = p_unet_down_blocks_2_resnets_0_conv2_weight = p_unet_down_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_22: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(conv2d_19, p_unet_down_blocks_2_resnets_0_conv_shortcut_weight, p_unet_down_blocks_2_resnets_0_conv_shortcut_bias);  p_unet_down_blocks_2_resnets_0_conv_shortcut_weight = p_unet_down_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_35: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_22, conv2d_21);  conv2d_22 = conv2d_21 = None\n",
       "                    div_13: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_35, 1.0);  add_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_14: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_13, 32, p_unet_down_blocks_2_attentions_0_norm_weight, p_unet_down_blocks_2_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_2_attentions_0_norm_weight = p_unet_down_blocks_2_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_23: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_14, p_unet_down_blocks_2_attentions_0_proj_in_weight, p_unet_down_blocks_2_attentions_0_proj_in_bias);  group_norm_14 = p_unet_down_blocks_2_attentions_0_proj_in_weight = p_unet_down_blocks_2_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_9: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_23, [0, 2, 3, 1]);  conv2d_23 = None\n",
       "                    view_57: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_9, [8, 64, 1280]);  permute_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_21: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_57, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_71: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_72: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_73: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_21 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_58: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_71, [8, -1, 8, 160]);  linear_71 = None\n",
       "                    transpose_48: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_58, 1, 2);  view_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_59: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_72, [8, -1, 8, 160]);  linear_72 = None\n",
       "                    transpose_49: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_59, 1, 2);  view_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_60: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_73, [8, -1, 8, 160]);  linear_73 = None\n",
       "                    transpose_50: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_60, 1, 2);  view_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_12: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_48, transpose_49, transpose_50);  transpose_48 = transpose_49 = transpose_50 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_51: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_12, 1, 2);  scaled_dot_product_attention_12 = None\n",
       "                    view_61: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_51, [8, -1, 1280]);  transpose_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_74: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_61, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_61 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_46: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_74);  linear_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_14: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_46, 1.0);  clone_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_36: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_14, view_57);  div_14 = view_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_22: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_36, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_75: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_22, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_22 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_76: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_77: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_62: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_75, [8, -1, 8, 160]);  linear_75 = None\n",
       "                    transpose_52: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_62, 1, 2);  view_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_63: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_76, [8, -1, 8, 160]);  linear_76 = None\n",
       "                    transpose_53: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_63, 1, 2);  view_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_64: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_77, [8, -1, 8, 160]);  linear_77 = None\n",
       "                    transpose_54: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_64, 1, 2);  view_64 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_13: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_52, transpose_53, transpose_54);  transpose_52 = transpose_53 = transpose_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_55: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_13, 1, 2);  scaled_dot_product_attention_13 = None\n",
       "                    view_65: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_55, [8, -1, 1280]);  transpose_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_78: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_65, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_65 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_47: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_78);  linear_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_15: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_47, 1.0);  clone_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_37: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_15, add_36);  div_15 = add_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_23: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_37, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_79: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_23, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_23 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_4 = torch.ops.aten.split.Tensor(linear_79, 5120, -1);  linear_79 = None\n",
       "                    getitem_8: \"f32[8, 64, 5120]\" = split_4[0]\n",
       "                    getitem_9: \"f32[8, 64, 5120]\" = split_4[1];  split_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_10: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_9);  getitem_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_7: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_8, gelu_10);  getitem_8 = gelu_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_48: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_7);  mul_7 = None\n",
       "                    linear_80: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_48, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_48 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_38: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_80, add_37);  linear_80 = add_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_66: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_38, [8, 8, 8, 1280]);  add_38 = None\n",
       "                    permute_10: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_66, [0, 3, 1, 2]);  view_66 = None\n",
       "                    clone_49: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_24: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_49, p_unet_down_blocks_2_attentions_0_proj_out_weight, p_unet_down_blocks_2_attentions_0_proj_out_bias);  clone_49 = p_unet_down_blocks_2_attentions_0_proj_out_weight = p_unet_down_blocks_2_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_39: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_24, div_13);  conv2d_24 = div_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_15: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_39, 32, p_unet_down_blocks_2_resnets_1_norm1_weight, p_unet_down_blocks_2_resnets_1_norm1_bias);  p_unet_down_blocks_2_resnets_1_norm1_weight = p_unet_down_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_15);  group_norm_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_25: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_16, p_unet_down_blocks_2_resnets_1_conv1_weight, p_unet_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_16 = p_unet_down_blocks_2_resnets_1_conv1_weight = p_unet_down_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_17: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_81: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_17, p_unet_down_blocks_2_resnets_1_time_emb_proj_weight, p_unet_down_blocks_2_resnets_1_time_emb_proj_bias);  silu_17 = p_unet_down_blocks_2_resnets_1_time_emb_proj_weight = p_unet_down_blocks_2_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_12: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_81, 2);  linear_81 = None\n",
       "                    unsqueeze_13: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 3);  unsqueeze_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_40: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_25, unsqueeze_13);  conv2d_25 = unsqueeze_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_40, 32, p_unet_down_blocks_2_resnets_1_norm2_weight, p_unet_down_blocks_2_resnets_1_norm2_bias);  add_40 = p_unet_down_blocks_2_resnets_1_norm2_weight = p_unet_down_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_18: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_16);  group_norm_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_50: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_18);  silu_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_26: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_50, p_unet_down_blocks_2_resnets_1_conv2_weight, p_unet_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_50 = p_unet_down_blocks_2_resnets_1_conv2_weight = p_unet_down_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_41: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(add_39, conv2d_26);  conv2d_26 = None\n",
       "                    div_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_41, 1.0);  add_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_17: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_16, 32, p_unet_down_blocks_2_attentions_1_norm_weight, p_unet_down_blocks_2_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_2_attentions_1_norm_weight = p_unet_down_blocks_2_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_27: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_17, p_unet_down_blocks_2_attentions_1_proj_in_weight, p_unet_down_blocks_2_attentions_1_proj_in_bias);  group_norm_17 = p_unet_down_blocks_2_attentions_1_proj_in_weight = p_unet_down_blocks_2_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_11: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_27, [0, 2, 3, 1]);  conv2d_27 = None\n",
       "                    view_67: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_11, [8, 64, 1280]);  permute_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_24: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_67, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_82: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_83: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_84: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_24 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_68: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_82, [8, -1, 8, 160]);  linear_82 = None\n",
       "                    transpose_56: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_69: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_83, [8, -1, 8, 160]);  linear_83 = None\n",
       "                    transpose_57: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_69, 1, 2);  view_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_70: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_84, [8, -1, 8, 160]);  linear_84 = None\n",
       "                    transpose_58: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_14: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_56, transpose_57, transpose_58);  transpose_56 = transpose_57 = transpose_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_59: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_14, 1, 2);  scaled_dot_product_attention_14 = None\n",
       "                    view_71: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_59, [8, -1, 1280]);  transpose_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_85: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_71, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_71 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_51: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_85);  linear_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_17: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_51, 1.0);  clone_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_42: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_17, view_67);  div_17 = view_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_25: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_42, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_86: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_25, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_25 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_87: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_88: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_72: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_86, [8, -1, 8, 160]);  linear_86 = None\n",
       "                    transpose_60: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_73: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_87, [8, -1, 8, 160]);  linear_87 = None\n",
       "                    transpose_61: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_73, 1, 2);  view_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_74: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_88, [8, -1, 8, 160]);  linear_88 = None\n",
       "                    transpose_62: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_15: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_60, transpose_61, transpose_62);  transpose_60 = transpose_61 = transpose_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_63: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_15, 1, 2);  scaled_dot_product_attention_15 = None\n",
       "                    view_75: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_63, [8, -1, 1280]);  transpose_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_89: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_75, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_75 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_52: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_89);  linear_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_18: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_52, 1.0);  clone_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_43: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_18, add_42);  div_18 = add_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_26: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_43, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_90: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_26, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_26 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_5 = torch.ops.aten.split.Tensor(linear_90, 5120, -1);  linear_90 = None\n",
       "                    getitem_10: \"f32[8, 64, 5120]\" = split_5[0]\n",
       "                    getitem_11: \"f32[8, 64, 5120]\" = split_5[1];  split_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_11: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_11);  getitem_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_8: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_10, gelu_11);  getitem_10 = gelu_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_53: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_8);  mul_8 = None\n",
       "                    linear_91: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_53, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_53 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_44: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_91, add_43);  linear_91 = add_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_76: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_44, [8, 8, 8, 1280]);  add_44 = None\n",
       "                    permute_12: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_76, [0, 3, 1, 2]);  view_76 = None\n",
       "                    clone_54: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_28: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_54, p_unet_down_blocks_2_attentions_1_proj_out_weight, p_unet_down_blocks_2_attentions_1_proj_out_bias);  clone_54 = p_unet_down_blocks_2_attentions_1_proj_out_weight = p_unet_down_blocks_2_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_28, div_16);  conv2d_28 = div_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_29: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(add_45, p_unet_down_blocks_2_downsamplers_0_conv_weight, p_unet_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_2_downsamplers_0_conv_weight = p_unet_down_blocks_2_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_18: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(conv2d_29, 32, p_unet_down_blocks_3_resnets_0_norm1_weight, p_unet_down_blocks_3_resnets_0_norm1_bias);  p_unet_down_blocks_3_resnets_0_norm1_weight = p_unet_down_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_18);  group_norm_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_19, p_unet_down_blocks_3_resnets_0_conv1_weight, p_unet_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_19 = p_unet_down_blocks_3_resnets_0_conv1_weight = p_unet_down_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_20: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_92: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_20, p_unet_down_blocks_3_resnets_0_time_emb_proj_weight, p_unet_down_blocks_3_resnets_0_time_emb_proj_bias);  silu_20 = p_unet_down_blocks_3_resnets_0_time_emb_proj_weight = p_unet_down_blocks_3_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_14: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_92, 2);  linear_92 = None\n",
       "                    unsqueeze_15: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_14, 3);  unsqueeze_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_46: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_30, unsqueeze_15);  conv2d_30 = unsqueeze_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_46, 32, p_unet_down_blocks_3_resnets_0_norm2_weight, p_unet_down_blocks_3_resnets_0_norm2_bias);  add_46 = p_unet_down_blocks_3_resnets_0_norm2_weight = p_unet_down_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_19);  group_norm_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_55: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_21);  silu_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_31: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_55, p_unet_down_blocks_3_resnets_0_conv2_weight, p_unet_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_55 = p_unet_down_blocks_3_resnets_0_conv2_weight = p_unet_down_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_47: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_29, conv2d_31);  conv2d_31 = None\n",
       "                    div_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_47, 1.0);  add_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_20: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_19, 32, p_unet_down_blocks_3_resnets_1_norm1_weight, p_unet_down_blocks_3_resnets_1_norm1_bias);  p_unet_down_blocks_3_resnets_1_norm1_weight = p_unet_down_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_22: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_20);  group_norm_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_32: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_22, p_unet_down_blocks_3_resnets_1_conv1_weight, p_unet_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_22 = p_unet_down_blocks_3_resnets_1_conv1_weight = p_unet_down_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_23: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_93: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_23, p_unet_down_blocks_3_resnets_1_time_emb_proj_weight, p_unet_down_blocks_3_resnets_1_time_emb_proj_bias);  silu_23 = p_unet_down_blocks_3_resnets_1_time_emb_proj_weight = p_unet_down_blocks_3_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_16: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_93, 2);  linear_93 = None\n",
       "                    unsqueeze_17: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_16, 3);  unsqueeze_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_48: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_32, unsqueeze_17);  conv2d_32 = unsqueeze_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_48, 32, p_unet_down_blocks_3_resnets_1_norm2_weight, p_unet_down_blocks_3_resnets_1_norm2_bias);  add_48 = p_unet_down_blocks_3_resnets_1_norm2_weight = p_unet_down_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_21);  group_norm_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_56: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_24);  silu_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_33: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_56, p_unet_down_blocks_3_resnets_1_conv2_weight, p_unet_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_56 = p_unet_down_blocks_3_resnets_1_conv2_weight = p_unet_down_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_49: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(div_19, conv2d_33);  conv2d_33 = None\n",
       "                    div_20: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_49, 1.0);  add_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_22: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_20, 32, p_unet_mid_block_resnets_0_norm1_weight, p_unet_mid_block_resnets_0_norm1_bias);  p_unet_mid_block_resnets_0_norm1_weight = p_unet_mid_block_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_22);  group_norm_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_34: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_25, p_unet_mid_block_resnets_0_conv1_weight, p_unet_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_25 = p_unet_mid_block_resnets_0_conv1_weight = p_unet_mid_block_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_26: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_94: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_26, p_unet_mid_block_resnets_0_time_emb_proj_weight, p_unet_mid_block_resnets_0_time_emb_proj_bias);  silu_26 = p_unet_mid_block_resnets_0_time_emb_proj_weight = p_unet_mid_block_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_18: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_94, 2);  linear_94 = None\n",
       "                    unsqueeze_19: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_18, 3);  unsqueeze_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_50: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_34, unsqueeze_19);  conv2d_34 = unsqueeze_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_23: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_50, 32, p_unet_mid_block_resnets_0_norm2_weight, p_unet_mid_block_resnets_0_norm2_bias);  add_50 = p_unet_mid_block_resnets_0_norm2_weight = p_unet_mid_block_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_27: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_23);  group_norm_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_57: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_27);  silu_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_35: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_57, p_unet_mid_block_resnets_0_conv2_weight, p_unet_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_57 = p_unet_mid_block_resnets_0_conv2_weight = p_unet_mid_block_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_51: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(div_20, conv2d_35);  conv2d_35 = None\n",
       "                    scalar_tensor_default_2: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_51, scalar_tensor_default_2);  add_51 = scalar_tensor_default_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_21, 32, p_unet_mid_block_attentions_0_norm_weight, p_unet_mid_block_attentions_0_norm_bias, 1e-06);  p_unet_mid_block_attentions_0_norm_weight = p_unet_mid_block_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_36: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(group_norm_24, p_unet_mid_block_attentions_0_proj_in_weight, p_unet_mid_block_attentions_0_proj_in_bias);  group_norm_24 = p_unet_mid_block_attentions_0_proj_in_weight = p_unet_mid_block_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_13: \"f32[8, 4, 4, 1280]\" = torch.ops.aten.permute.default(conv2d_36, [0, 2, 3, 1]);  conv2d_36 = None\n",
       "                    view_77: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(permute_13, [8, 16, 1280]);  permute_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_27: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(view_77, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_95: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_96: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_97: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_27 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_78: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_95, [8, -1, 8, 160]);  linear_95 = None\n",
       "                    transpose_64: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_79: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_96, [8, -1, 8, 160]);  linear_96 = None\n",
       "                    transpose_65: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_80: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_97, [8, -1, 8, 160]);  linear_97 = None\n",
       "                    transpose_66: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_16: \"f32[8, 8, 16, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_64, transpose_65, transpose_66);  transpose_64 = transpose_65 = transpose_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_67: \"f32[8, 16, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_16, 1, 2);  scaled_dot_product_attention_16 = None\n",
       "                    view_81: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(transpose_67, [8, -1, 1280]);  transpose_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_98: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(view_81, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_81 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_58: \"f32[8, 16, 1280]\" = torch.ops.aten.clone.default(linear_98);  linear_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_22: \"f32[8, 16, 1280]\" = torch.ops.aten.div.Tensor(clone_58, 1.0);  clone_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_52: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(div_22, view_77);  div_22 = view_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_28: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(add_52, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_99: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_28, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_28 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_100: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_101: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_82: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_99, [8, -1, 8, 160]);  linear_99 = None\n",
       "                    transpose_68: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_83: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_100, [8, -1, 8, 160]);  linear_100 = None\n",
       "                    transpose_69: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_84: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_101, [8, -1, 8, 160]);  linear_101 = None\n",
       "                    transpose_70: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_17: \"f32[8, 8, 16, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_68, transpose_69, transpose_70);  transpose_68 = transpose_69 = transpose_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_71: \"f32[8, 16, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_17, 1, 2);  scaled_dot_product_attention_17 = None\n",
       "                    view_85: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(transpose_71, [8, -1, 1280]);  transpose_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_102: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(view_85, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_85 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_59: \"f32[8, 16, 1280]\" = torch.ops.aten.clone.default(linear_102);  linear_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_23: \"f32[8, 16, 1280]\" = torch.ops.aten.div.Tensor(clone_59, 1.0);  clone_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_53: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(div_23, add_52);  div_23 = add_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_29: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(add_53, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_103: \"f32[8, 16, 10240]\" = torch.ops.aten.linear.default(layer_norm_29, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_29 = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_6 = torch.ops.aten.split.Tensor(linear_103, 5120, -1);  linear_103 = None\n",
       "                    getitem_12: \"f32[8, 16, 5120]\" = split_6[0]\n",
       "                    getitem_13: \"f32[8, 16, 5120]\" = split_6[1];  split_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_12: \"f32[8, 16, 5120]\" = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_9: \"f32[8, 16, 5120]\" = torch.ops.aten.mul.Tensor(getitem_12, gelu_12);  getitem_12 = gelu_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_60: \"f32[8, 16, 5120]\" = torch.ops.aten.clone.default(mul_9);  mul_9 = None\n",
       "                    linear_104: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(clone_60, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_60 = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_54: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(linear_104, add_53);  linear_104 = add_53 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_86: \"f32[8, 4, 4, 1280]\" = torch.ops.aten.view.default(add_54, [8, 4, 4, 1280]);  add_54 = None\n",
       "                    permute_14: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.permute.default(view_86, [0, 3, 1, 2]);  view_86 = None\n",
       "                    clone_61: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_37: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_61, p_unet_mid_block_attentions_0_proj_out_weight, p_unet_mid_block_attentions_0_proj_out_bias);  clone_61 = p_unet_mid_block_attentions_0_proj_out_weight = p_unet_mid_block_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_55: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_37, div_21);  conv2d_37 = div_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_55, 32, p_unet_mid_block_resnets_1_norm1_weight, p_unet_mid_block_resnets_1_norm1_bias);  p_unet_mid_block_resnets_1_norm1_weight = p_unet_mid_block_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_28: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_25);  group_norm_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_38: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_28, p_unet_mid_block_resnets_1_conv1_weight, p_unet_mid_block_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_28 = p_unet_mid_block_resnets_1_conv1_weight = p_unet_mid_block_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_29: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_105: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_29, p_unet_mid_block_resnets_1_time_emb_proj_weight, p_unet_mid_block_resnets_1_time_emb_proj_bias);  silu_29 = p_unet_mid_block_resnets_1_time_emb_proj_weight = p_unet_mid_block_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_20: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_105, 2);  linear_105 = None\n",
       "                    unsqueeze_21: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_20, 3);  unsqueeze_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_56: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_38, unsqueeze_21);  conv2d_38 = unsqueeze_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_26: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_56, 32, p_unet_mid_block_resnets_1_norm2_weight, p_unet_mid_block_resnets_1_norm2_bias);  add_56 = p_unet_mid_block_resnets_1_norm2_weight = p_unet_mid_block_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_26);  group_norm_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_62: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_30);  silu_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_39: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_62, p_unet_mid_block_resnets_1_conv2_weight, p_unet_mid_block_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_62 = p_unet_mid_block_resnets_1_conv2_weight = p_unet_mid_block_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_57: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(add_55, conv2d_39);  add_55 = conv2d_39 = None\n",
       "                    scalar_tensor_default_3: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_57, scalar_tensor_default_3);  add_57 = scalar_tensor_default_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_5: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_24, div_20], 1);  div_24 = div_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_27: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_5, 32, p_unet_up_blocks_0_resnets_0_norm1_weight, p_unet_up_blocks_0_resnets_0_norm1_bias);  p_unet_up_blocks_0_resnets_0_norm1_weight = p_unet_up_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_31: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_27);  group_norm_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_40: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_31, p_unet_up_blocks_0_resnets_0_conv1_weight, p_unet_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_31 = p_unet_up_blocks_0_resnets_0_conv1_weight = p_unet_up_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_32: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_106: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_32, p_unet_up_blocks_0_resnets_0_time_emb_proj_weight, p_unet_up_blocks_0_resnets_0_time_emb_proj_bias);  silu_32 = p_unet_up_blocks_0_resnets_0_time_emb_proj_weight = p_unet_up_blocks_0_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_22: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_106, 2);  linear_106 = None\n",
       "                    unsqueeze_23: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_22, 3);  unsqueeze_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_58: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_40, unsqueeze_23);  conv2d_40 = unsqueeze_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_28: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_58, 32, p_unet_up_blocks_0_resnets_0_norm2_weight, p_unet_up_blocks_0_resnets_0_norm2_bias);  add_58 = p_unet_up_blocks_0_resnets_0_norm2_weight = p_unet_up_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_33: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_28);  group_norm_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_63: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_33);  silu_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_41: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_63, p_unet_up_blocks_0_resnets_0_conv2_weight, p_unet_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_63 = p_unet_up_blocks_0_resnets_0_conv2_weight = p_unet_up_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_42: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_5, p_unet_up_blocks_0_resnets_0_conv_shortcut_weight, p_unet_up_blocks_0_resnets_0_conv_shortcut_bias);  cat_5 = p_unet_up_blocks_0_resnets_0_conv_shortcut_weight = p_unet_up_blocks_0_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_59: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_42, conv2d_41);  conv2d_42 = conv2d_41 = None\n",
       "                    div_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_59, 1.0);  add_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_6: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_25, div_19], 1);  div_25 = div_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_29: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_6, 32, p_unet_up_blocks_0_resnets_1_norm1_weight, p_unet_up_blocks_0_resnets_1_norm1_bias);  p_unet_up_blocks_0_resnets_1_norm1_weight = p_unet_up_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_34: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_29);  group_norm_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_43: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_34, p_unet_up_blocks_0_resnets_1_conv1_weight, p_unet_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_34 = p_unet_up_blocks_0_resnets_1_conv1_weight = p_unet_up_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_35: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_107: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_35, p_unet_up_blocks_0_resnets_1_time_emb_proj_weight, p_unet_up_blocks_0_resnets_1_time_emb_proj_bias);  silu_35 = p_unet_up_blocks_0_resnets_1_time_emb_proj_weight = p_unet_up_blocks_0_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_24: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_107, 2);  linear_107 = None\n",
       "                    unsqueeze_25: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_24, 3);  unsqueeze_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_60: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_43, unsqueeze_25);  conv2d_43 = unsqueeze_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_60, 32, p_unet_up_blocks_0_resnets_1_norm2_weight, p_unet_up_blocks_0_resnets_1_norm2_bias);  add_60 = p_unet_up_blocks_0_resnets_1_norm2_weight = p_unet_up_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_36: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_30);  group_norm_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_64: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_36);  silu_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_44: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_64, p_unet_up_blocks_0_resnets_1_conv2_weight, p_unet_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_64 = p_unet_up_blocks_0_resnets_1_conv2_weight = p_unet_up_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_45: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_6, p_unet_up_blocks_0_resnets_1_conv_shortcut_weight, p_unet_up_blocks_0_resnets_1_conv_shortcut_bias);  cat_6 = p_unet_up_blocks_0_resnets_1_conv_shortcut_weight = p_unet_up_blocks_0_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_61: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_45, conv2d_44);  conv2d_45 = conv2d_44 = None\n",
       "                    div_26: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_61, 1.0);  add_61 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_7: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_26, conv2d_29], 1);  div_26 = conv2d_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_31: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_7, 32, p_unet_up_blocks_0_resnets_2_norm1_weight, p_unet_up_blocks_0_resnets_2_norm1_bias);  p_unet_up_blocks_0_resnets_2_norm1_weight = p_unet_up_blocks_0_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_37: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_31);  group_norm_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_46: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_37, p_unet_up_blocks_0_resnets_2_conv1_weight, p_unet_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_37 = p_unet_up_blocks_0_resnets_2_conv1_weight = p_unet_up_blocks_0_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_38: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_108: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_38, p_unet_up_blocks_0_resnets_2_time_emb_proj_weight, p_unet_up_blocks_0_resnets_2_time_emb_proj_bias);  silu_38 = p_unet_up_blocks_0_resnets_2_time_emb_proj_weight = p_unet_up_blocks_0_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_26: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_108, 2);  linear_108 = None\n",
       "                    unsqueeze_27: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_26, 3);  unsqueeze_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_62: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_46, unsqueeze_27);  conv2d_46 = unsqueeze_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_32: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_62, 32, p_unet_up_blocks_0_resnets_2_norm2_weight, p_unet_up_blocks_0_resnets_2_norm2_bias);  add_62 = p_unet_up_blocks_0_resnets_2_norm2_weight = p_unet_up_blocks_0_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_39: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_32);  group_norm_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_65: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_39);  silu_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_47: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_65, p_unet_up_blocks_0_resnets_2_conv2_weight, p_unet_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_65 = p_unet_up_blocks_0_resnets_2_conv2_weight = p_unet_up_blocks_0_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_48: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_7, p_unet_up_blocks_0_resnets_2_conv_shortcut_weight, p_unet_up_blocks_0_resnets_2_conv_shortcut_bias);  cat_7 = p_unet_up_blocks_0_resnets_2_conv_shortcut_weight = p_unet_up_blocks_0_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_63: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_48, conv2d_47);  conv2d_48 = conv2d_47 = None\n",
       "                    div_27: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_63, 1.0);  add_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.upsample_nearest2d.vec(div_27, None, [2.0, 2.0]);  div_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_49: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(upsample_nearest2d, p_unet_up_blocks_0_upsamplers_0_conv_weight, p_unet_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d = p_unet_up_blocks_0_upsamplers_0_conv_weight = p_unet_up_blocks_0_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_8: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.cat.default([conv2d_49, add_45], 1);  conv2d_49 = add_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_33: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.group_norm.default(cat_8, 32, p_unet_up_blocks_1_resnets_0_norm1_weight, p_unet_up_blocks_1_resnets_0_norm1_bias);  p_unet_up_blocks_1_resnets_0_norm1_weight = p_unet_up_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_40: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.silu.default(group_norm_33);  group_norm_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_50: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_40, p_unet_up_blocks_1_resnets_0_conv1_weight, p_unet_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_40 = p_unet_up_blocks_1_resnets_0_conv1_weight = p_unet_up_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_41: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_109: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_41, p_unet_up_blocks_1_resnets_0_time_emb_proj_weight, p_unet_up_blocks_1_resnets_0_time_emb_proj_bias);  silu_41 = p_unet_up_blocks_1_resnets_0_time_emb_proj_weight = p_unet_up_blocks_1_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_28: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_109, 2);  linear_109 = None\n",
       "                    unsqueeze_29: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_28, 3);  unsqueeze_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_64: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_50, unsqueeze_29);  conv2d_50 = unsqueeze_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_64, 32, p_unet_up_blocks_1_resnets_0_norm2_weight, p_unet_up_blocks_1_resnets_0_norm2_bias);  add_64 = p_unet_up_blocks_1_resnets_0_norm2_weight = p_unet_up_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_42: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_34);  group_norm_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_66: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_42);  silu_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_51: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_66, p_unet_up_blocks_1_resnets_0_conv2_weight, p_unet_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_66 = p_unet_up_blocks_1_resnets_0_conv2_weight = p_unet_up_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_52: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_8, p_unet_up_blocks_1_resnets_0_conv_shortcut_weight, p_unet_up_blocks_1_resnets_0_conv_shortcut_bias);  cat_8 = p_unet_up_blocks_1_resnets_0_conv_shortcut_weight = p_unet_up_blocks_1_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_65: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_52, conv2d_51);  conv2d_52 = conv2d_51 = None\n",
       "                    div_28: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_65, 1.0);  add_65 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_35: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_28, 32, p_unet_up_blocks_1_attentions_0_norm_weight, p_unet_up_blocks_1_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_0_norm_weight = p_unet_up_blocks_1_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_53: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_35, p_unet_up_blocks_1_attentions_0_proj_in_weight, p_unet_up_blocks_1_attentions_0_proj_in_bias);  group_norm_35 = p_unet_up_blocks_1_attentions_0_proj_in_weight = p_unet_up_blocks_1_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_15: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_53, [0, 2, 3, 1]);  conv2d_53 = None\n",
       "                    view_87: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_15, [8, 64, 1280]);  permute_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_30: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_87, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_110: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_111: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_112: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_30 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_88: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_110, [8, -1, 8, 160]);  linear_110 = None\n",
       "                    transpose_72: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_89: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_111, [8, -1, 8, 160]);  linear_111 = None\n",
       "                    transpose_73: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_89, 1, 2);  view_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_90: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_112, [8, -1, 8, 160]);  linear_112 = None\n",
       "                    transpose_74: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_18: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_72, transpose_73, transpose_74);  transpose_72 = transpose_73 = transpose_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_75: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_18, 1, 2);  scaled_dot_product_attention_18 = None\n",
       "                    view_91: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_75, [8, -1, 1280]);  transpose_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_113: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_91, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_91 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_67: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_113);  linear_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_29: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_67, 1.0);  clone_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_66: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_29, view_87);  div_29 = view_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_31: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_66, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_114: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_31, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_31 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_115: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_116: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_92: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_114, [8, -1, 8, 160]);  linear_114 = None\n",
       "                    transpose_76: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_93: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_115, [8, -1, 8, 160]);  linear_115 = None\n",
       "                    transpose_77: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_93, 1, 2);  view_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_94: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_116, [8, -1, 8, 160]);  linear_116 = None\n",
       "                    transpose_78: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_19: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_76, transpose_77, transpose_78);  transpose_76 = transpose_77 = transpose_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_79: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_19, 1, 2);  scaled_dot_product_attention_19 = None\n",
       "                    view_95: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_79, [8, -1, 1280]);  transpose_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_117: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_95, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_95 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_68: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_117);  linear_117 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_30: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_68, 1.0);  clone_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_67: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_30, add_66);  div_30 = add_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_32: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_67, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_118: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_32, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_32 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_7 = torch.ops.aten.split.Tensor(linear_118, 5120, -1);  linear_118 = None\n",
       "                    getitem_14: \"f32[8, 64, 5120]\" = split_7[0]\n",
       "                    getitem_15: \"f32[8, 64, 5120]\" = split_7[1];  split_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_13: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_10: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_14, gelu_13);  getitem_14 = gelu_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_69: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_10);  mul_10 = None\n",
       "                    linear_119: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_69, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_69 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_68: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_119, add_67);  linear_119 = add_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_96: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_68, [8, 8, 8, 1280]);  add_68 = None\n",
       "                    permute_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_96, [0, 3, 1, 2]);  view_96 = None\n",
       "                    clone_70: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_54: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_70, p_unet_up_blocks_1_attentions_0_proj_out_weight, p_unet_up_blocks_1_attentions_0_proj_out_bias);  clone_70 = p_unet_up_blocks_1_attentions_0_proj_out_weight = p_unet_up_blocks_1_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_69: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_54, div_28);  conv2d_54 = div_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_9: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.cat.default([add_69, add_39], 1);  add_69 = add_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_36: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.group_norm.default(cat_9, 32, p_unet_up_blocks_1_resnets_1_norm1_weight, p_unet_up_blocks_1_resnets_1_norm1_bias);  p_unet_up_blocks_1_resnets_1_norm1_weight = p_unet_up_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_43: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.silu.default(group_norm_36);  group_norm_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_55: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_43, p_unet_up_blocks_1_resnets_1_conv1_weight, p_unet_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_43 = p_unet_up_blocks_1_resnets_1_conv1_weight = p_unet_up_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_44: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_120: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_44, p_unet_up_blocks_1_resnets_1_time_emb_proj_weight, p_unet_up_blocks_1_resnets_1_time_emb_proj_bias);  silu_44 = p_unet_up_blocks_1_resnets_1_time_emb_proj_weight = p_unet_up_blocks_1_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_30: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_120, 2);  linear_120 = None\n",
       "                    unsqueeze_31: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_30, 3);  unsqueeze_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_70: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_55, unsqueeze_31);  conv2d_55 = unsqueeze_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_37: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_70, 32, p_unet_up_blocks_1_resnets_1_norm2_weight, p_unet_up_blocks_1_resnets_1_norm2_bias);  add_70 = p_unet_up_blocks_1_resnets_1_norm2_weight = p_unet_up_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_37);  group_norm_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_71: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_45);  silu_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_56: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_71, p_unet_up_blocks_1_resnets_1_conv2_weight, p_unet_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_71 = p_unet_up_blocks_1_resnets_1_conv2_weight = p_unet_up_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_57: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_9, p_unet_up_blocks_1_resnets_1_conv_shortcut_weight, p_unet_up_blocks_1_resnets_1_conv_shortcut_bias);  cat_9 = p_unet_up_blocks_1_resnets_1_conv_shortcut_weight = p_unet_up_blocks_1_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_71: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_57, conv2d_56);  conv2d_57 = conv2d_56 = None\n",
       "                    div_31: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_71, 1.0);  add_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_38: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_31, 32, p_unet_up_blocks_1_attentions_1_norm_weight, p_unet_up_blocks_1_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_1_norm_weight = p_unet_up_blocks_1_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_58: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_38, p_unet_up_blocks_1_attentions_1_proj_in_weight, p_unet_up_blocks_1_attentions_1_proj_in_bias);  group_norm_38 = p_unet_up_blocks_1_attentions_1_proj_in_weight = p_unet_up_blocks_1_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_17: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_58, [0, 2, 3, 1]);  conv2d_58 = None\n",
       "                    view_97: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_17, [8, 64, 1280]);  permute_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_33: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_97, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_121: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_122: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_123: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_33 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_98: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_121, [8, -1, 8, 160]);  linear_121 = None\n",
       "                    transpose_80: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_98, 1, 2);  view_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_99: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_122, [8, -1, 8, 160]);  linear_122 = None\n",
       "                    transpose_81: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_100: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_123, [8, -1, 8, 160]);  linear_123 = None\n",
       "                    transpose_82: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_20: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_80, transpose_81, transpose_82);  transpose_80 = transpose_81 = transpose_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_83: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_20, 1, 2);  scaled_dot_product_attention_20 = None\n",
       "                    view_101: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_83, [8, -1, 1280]);  transpose_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_124: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_101, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_101 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_72: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_124);  linear_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_32: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_72, 1.0);  clone_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_72: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_32, view_97);  div_32 = view_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_34: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_72, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_125: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_34, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_34 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_126: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_127: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_102: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_125, [8, -1, 8, 160]);  linear_125 = None\n",
       "                    transpose_84: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_102, 1, 2);  view_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_103: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_126, [8, -1, 8, 160]);  linear_126 = None\n",
       "                    transpose_85: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_104: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_127, [8, -1, 8, 160]);  linear_127 = None\n",
       "                    transpose_86: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_21: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_84, transpose_85, transpose_86);  transpose_84 = transpose_85 = transpose_86 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_87: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_21, 1, 2);  scaled_dot_product_attention_21 = None\n",
       "                    view_105: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_87, [8, -1, 1280]);  transpose_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_128: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_105, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_105 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_73: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_128);  linear_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_33: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_73, 1.0);  clone_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_73: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_33, add_72);  div_33 = add_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_35: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_73, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_129: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_35, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_35 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_8 = torch.ops.aten.split.Tensor(linear_129, 5120, -1);  linear_129 = None\n",
       "                    getitem_16: \"f32[8, 64, 5120]\" = split_8[0]\n",
       "                    getitem_17: \"f32[8, 64, 5120]\" = split_8[1];  split_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_14: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_17);  getitem_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_11: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_16, gelu_14);  getitem_16 = gelu_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_74: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_11);  mul_11 = None\n",
       "                    linear_130: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_74, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_74 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_74: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_130, add_73);  linear_130 = add_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_106: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_74, [8, 8, 8, 1280]);  add_74 = None\n",
       "                    permute_18: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_106, [0, 3, 1, 2]);  view_106 = None\n",
       "                    clone_75: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_18, memory_format = torch.contiguous_format);  permute_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_59: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_75, p_unet_up_blocks_1_attentions_1_proj_out_weight, p_unet_up_blocks_1_attentions_1_proj_out_bias);  clone_75 = p_unet_up_blocks_1_attentions_1_proj_out_weight = p_unet_up_blocks_1_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_75: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_59, div_31);  conv2d_59 = div_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_10: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.cat.default([add_75, conv2d_19], 1);  add_75 = conv2d_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_39: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.group_norm.default(cat_10, 32, p_unet_up_blocks_1_resnets_2_norm1_weight, p_unet_up_blocks_1_resnets_2_norm1_bias);  p_unet_up_blocks_1_resnets_2_norm1_weight = p_unet_up_blocks_1_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_46: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.silu.default(group_norm_39);  group_norm_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_60: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_46, p_unet_up_blocks_1_resnets_2_conv1_weight, p_unet_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_46 = p_unet_up_blocks_1_resnets_2_conv1_weight = p_unet_up_blocks_1_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_47: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_131: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_47, p_unet_up_blocks_1_resnets_2_time_emb_proj_weight, p_unet_up_blocks_1_resnets_2_time_emb_proj_bias);  silu_47 = p_unet_up_blocks_1_resnets_2_time_emb_proj_weight = p_unet_up_blocks_1_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_32: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_131, 2);  linear_131 = None\n",
       "                    unsqueeze_33: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_32, 3);  unsqueeze_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_76: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_60, unsqueeze_33);  conv2d_60 = unsqueeze_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_40: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_76, 32, p_unet_up_blocks_1_resnets_2_norm2_weight, p_unet_up_blocks_1_resnets_2_norm2_bias);  add_76 = p_unet_up_blocks_1_resnets_2_norm2_weight = p_unet_up_blocks_1_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_48: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_40);  group_norm_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_76: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_48);  silu_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_61: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_76, p_unet_up_blocks_1_resnets_2_conv2_weight, p_unet_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_76 = p_unet_up_blocks_1_resnets_2_conv2_weight = p_unet_up_blocks_1_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_62: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_10, p_unet_up_blocks_1_resnets_2_conv_shortcut_weight, p_unet_up_blocks_1_resnets_2_conv_shortcut_bias);  cat_10 = p_unet_up_blocks_1_resnets_2_conv_shortcut_weight = p_unet_up_blocks_1_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_77: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_62, conv2d_61);  conv2d_62 = conv2d_61 = None\n",
       "                    div_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_77, 1.0);  add_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_41: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_34, 32, p_unet_up_blocks_1_attentions_2_norm_weight, p_unet_up_blocks_1_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_2_norm_weight = p_unet_up_blocks_1_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_63: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_41, p_unet_up_blocks_1_attentions_2_proj_in_weight, p_unet_up_blocks_1_attentions_2_proj_in_bias);  group_norm_41 = p_unet_up_blocks_1_attentions_2_proj_in_weight = p_unet_up_blocks_1_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_19: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_63, [0, 2, 3, 1]);  conv2d_63 = None\n",
       "                    view_107: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_19, [8, 64, 1280]);  permute_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_36: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_107, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_132: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_133: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_134: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_36 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_108: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_132, [8, -1, 8, 160]);  linear_132 = None\n",
       "                    transpose_88: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_109: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_133, [8, -1, 8, 160]);  linear_133 = None\n",
       "                    transpose_89: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_110: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_134, [8, -1, 8, 160]);  linear_134 = None\n",
       "                    transpose_90: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_110, 1, 2);  view_110 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_22: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_88, transpose_89, transpose_90);  transpose_88 = transpose_89 = transpose_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_91: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_22, 1, 2);  scaled_dot_product_attention_22 = None\n",
       "                    view_111: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_91, [8, -1, 1280]);  transpose_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_135: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_111, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_111 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_77: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_135);  linear_135 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_35: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_77, 1.0);  clone_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_78: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_35, view_107);  div_35 = view_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_37: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_78, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_136: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_37, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_37 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_137: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_138: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_112: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_136, [8, -1, 8, 160]);  linear_136 = None\n",
       "                    transpose_92: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_113: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_137, [8, -1, 8, 160]);  linear_137 = None\n",
       "                    transpose_93: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_114: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_138, [8, -1, 8, 160]);  linear_138 = None\n",
       "                    transpose_94: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_114, 1, 2);  view_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_23: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_92, transpose_93, transpose_94);  transpose_92 = transpose_93 = transpose_94 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_95: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_23, 1, 2);  scaled_dot_product_attention_23 = None\n",
       "                    view_115: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_95, [8, -1, 1280]);  transpose_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_139: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_115, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_115 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_78: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_139);  linear_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_36: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_78, 1.0);  clone_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_79: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_36, add_78);  div_36 = add_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_38: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_79, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_140: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_38, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_38 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_9 = torch.ops.aten.split.Tensor(linear_140, 5120, -1);  linear_140 = None\n",
       "                    getitem_18: \"f32[8, 64, 5120]\" = split_9[0]\n",
       "                    getitem_19: \"f32[8, 64, 5120]\" = split_9[1];  split_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_15: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_19);  getitem_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_12: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_18, gelu_15);  getitem_18 = gelu_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_79: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_12);  mul_12 = None\n",
       "                    linear_141: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_79, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_79 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_80: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_141, add_79);  linear_141 = add_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_116: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_80, [8, 8, 8, 1280]);  add_80 = None\n",
       "                    permute_20: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_116, [0, 3, 1, 2]);  view_116 = None\n",
       "                    clone_80: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_64: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_80, p_unet_up_blocks_1_attentions_2_proj_out_weight, p_unet_up_blocks_1_attentions_2_proj_out_bias);  clone_80 = p_unet_up_blocks_1_attentions_2_proj_out_weight = p_unet_up_blocks_1_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_81: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_64, div_34);  conv2d_64 = div_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_1: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.upsample_nearest2d.vec(add_81, None, [2.0, 2.0]);  add_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_65: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_1, p_unet_up_blocks_1_upsamplers_0_conv_weight, p_unet_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_1 = p_unet_up_blocks_1_upsamplers_0_conv_weight = p_unet_up_blocks_1_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_11: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.cat.default([conv2d_65, add_33], 1);  conv2d_65 = add_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_42: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.group_norm.default(cat_11, 32, p_unet_up_blocks_2_resnets_0_norm1_weight, p_unet_up_blocks_2_resnets_0_norm1_bias);  p_unet_up_blocks_2_resnets_0_norm1_weight = p_unet_up_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_49: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.silu.default(group_norm_42);  group_norm_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_66: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_49, p_unet_up_blocks_2_resnets_0_conv1_weight, p_unet_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_49 = p_unet_up_blocks_2_resnets_0_conv1_weight = p_unet_up_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_50: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_142: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_50, p_unet_up_blocks_2_resnets_0_time_emb_proj_weight, p_unet_up_blocks_2_resnets_0_time_emb_proj_bias);  silu_50 = p_unet_up_blocks_2_resnets_0_time_emb_proj_weight = p_unet_up_blocks_2_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_34: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_142, 2);  linear_142 = None\n",
       "                    unsqueeze_35: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_34, 3);  unsqueeze_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_82: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_66, unsqueeze_35);  conv2d_66 = unsqueeze_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_43: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_82, 32, p_unet_up_blocks_2_resnets_0_norm2_weight, p_unet_up_blocks_2_resnets_0_norm2_bias);  add_82 = p_unet_up_blocks_2_resnets_0_norm2_weight = p_unet_up_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_51: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_43);  group_norm_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_81: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_51);  silu_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_67: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_81, p_unet_up_blocks_2_resnets_0_conv2_weight, p_unet_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_81 = p_unet_up_blocks_2_resnets_0_conv2_weight = p_unet_up_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_68: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_11, p_unet_up_blocks_2_resnets_0_conv_shortcut_weight, p_unet_up_blocks_2_resnets_0_conv_shortcut_bias);  cat_11 = p_unet_up_blocks_2_resnets_0_conv_shortcut_weight = p_unet_up_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_83: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_68, conv2d_67);  conv2d_68 = conv2d_67 = None\n",
       "                    div_37: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_83, 1.0);  add_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_44: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_37, 32, p_unet_up_blocks_2_attentions_0_norm_weight, p_unet_up_blocks_2_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_0_norm_weight = p_unet_up_blocks_2_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_69: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_44, p_unet_up_blocks_2_attentions_0_proj_in_weight, p_unet_up_blocks_2_attentions_0_proj_in_bias);  group_norm_44 = p_unet_up_blocks_2_attentions_0_proj_in_weight = p_unet_up_blocks_2_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_21: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_69, [0, 2, 3, 1]);  conv2d_69 = None\n",
       "                    view_117: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_21, [8, 256, 640]);  permute_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_39: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_117, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_143: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_144: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_145: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_39 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_118: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_143, [8, -1, 8, 80]);  linear_143 = None\n",
       "                    transpose_96: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_118, 1, 2);  view_118 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_119: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_144, [8, -1, 8, 80]);  linear_144 = None\n",
       "                    transpose_97: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_120: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_145, [8, -1, 8, 80]);  linear_145 = None\n",
       "                    transpose_98: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_24: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_96, transpose_97, transpose_98);  transpose_96 = transpose_97 = transpose_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_99: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_24, 1, 2);  scaled_dot_product_attention_24 = None\n",
       "                    view_121: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_99, [8, -1, 640]);  transpose_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_146: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_121, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_121 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_82: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_146);  linear_146 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_38: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_82, 1.0);  clone_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_84: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_38, view_117);  div_38 = view_117 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_40: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_84, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_147: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_40, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_40 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_148: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_149: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_122: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_147, [8, -1, 8, 80]);  linear_147 = None\n",
       "                    transpose_100: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_122, 1, 2);  view_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_123: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_148, [8, -1, 8, 80]);  linear_148 = None\n",
       "                    transpose_101: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_124: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_149, [8, -1, 8, 80]);  linear_149 = None\n",
       "                    transpose_102: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_25: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_100, transpose_101, transpose_102);  transpose_100 = transpose_101 = transpose_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_103: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_25, 1, 2);  scaled_dot_product_attention_25 = None\n",
       "                    view_125: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_103, [8, -1, 640]);  transpose_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_150: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_125, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_125 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_83: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_150);  linear_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_39: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_83, 1.0);  clone_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_85: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_39, add_84);  div_39 = add_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_41: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_85, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_151: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_41, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_41 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_10 = torch.ops.aten.split.Tensor(linear_151, 2560, -1);  linear_151 = None\n",
       "                    getitem_20: \"f32[8, 256, 2560]\" = split_10[0]\n",
       "                    getitem_21: \"f32[8, 256, 2560]\" = split_10[1];  split_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_16: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_21);  getitem_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_13: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_20, gelu_16);  getitem_20 = gelu_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_84: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_13);  mul_13 = None\n",
       "                    linear_152: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_84, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_84 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_86: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_152, add_85);  linear_152 = add_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_126: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_86, [8, 16, 16, 640]);  add_86 = None\n",
       "                    permute_22: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_126, [0, 3, 1, 2]);  view_126 = None\n",
       "                    clone_85: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_70: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_85, p_unet_up_blocks_2_attentions_0_proj_out_weight, p_unet_up_blocks_2_attentions_0_proj_out_bias);  clone_85 = p_unet_up_blocks_2_attentions_0_proj_out_weight = p_unet_up_blocks_2_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_87: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_70, div_37);  conv2d_70 = div_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_12: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.cat.default([add_87, add_27], 1);  add_87 = add_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_45: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.group_norm.default(cat_12, 32, p_unet_up_blocks_2_resnets_1_norm1_weight, p_unet_up_blocks_2_resnets_1_norm1_bias);  p_unet_up_blocks_2_resnets_1_norm1_weight = p_unet_up_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_52: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.silu.default(group_norm_45);  group_norm_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_71: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_52, p_unet_up_blocks_2_resnets_1_conv1_weight, p_unet_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_52 = p_unet_up_blocks_2_resnets_1_conv1_weight = p_unet_up_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_53: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_153: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_53, p_unet_up_blocks_2_resnets_1_time_emb_proj_weight, p_unet_up_blocks_2_resnets_1_time_emb_proj_bias);  silu_53 = p_unet_up_blocks_2_resnets_1_time_emb_proj_weight = p_unet_up_blocks_2_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_36: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_153, 2);  linear_153 = None\n",
       "                    unsqueeze_37: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_36, 3);  unsqueeze_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_88: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_71, unsqueeze_37);  conv2d_71 = unsqueeze_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_46: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_88, 32, p_unet_up_blocks_2_resnets_1_norm2_weight, p_unet_up_blocks_2_resnets_1_norm2_bias);  add_88 = p_unet_up_blocks_2_resnets_1_norm2_weight = p_unet_up_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_54: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_46);  group_norm_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_86: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_54);  silu_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_72: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_86, p_unet_up_blocks_2_resnets_1_conv2_weight, p_unet_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_86 = p_unet_up_blocks_2_resnets_1_conv2_weight = p_unet_up_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_73: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_12, p_unet_up_blocks_2_resnets_1_conv_shortcut_weight, p_unet_up_blocks_2_resnets_1_conv_shortcut_bias);  cat_12 = p_unet_up_blocks_2_resnets_1_conv_shortcut_weight = p_unet_up_blocks_2_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_89: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_73, conv2d_72);  conv2d_73 = conv2d_72 = None\n",
       "                    div_40: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_89, 1.0);  add_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_47: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_40, 32, p_unet_up_blocks_2_attentions_1_norm_weight, p_unet_up_blocks_2_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_1_norm_weight = p_unet_up_blocks_2_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_74: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_47, p_unet_up_blocks_2_attentions_1_proj_in_weight, p_unet_up_blocks_2_attentions_1_proj_in_bias);  group_norm_47 = p_unet_up_blocks_2_attentions_1_proj_in_weight = p_unet_up_blocks_2_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_23: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_74, [0, 2, 3, 1]);  conv2d_74 = None\n",
       "                    view_127: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_23, [8, 256, 640]);  permute_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_42: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_127, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_154: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_155: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_156: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_42 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_128: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_154, [8, -1, 8, 80]);  linear_154 = None\n",
       "                    transpose_104: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_129: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_155, [8, -1, 8, 80]);  linear_155 = None\n",
       "                    transpose_105: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_130: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_156, [8, -1, 8, 80]);  linear_156 = None\n",
       "                    transpose_106: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_130, 1, 2);  view_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_26: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_104, transpose_105, transpose_106);  transpose_104 = transpose_105 = transpose_106 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_107: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_26, 1, 2);  scaled_dot_product_attention_26 = None\n",
       "                    view_131: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_107, [8, -1, 640]);  transpose_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_157: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_131, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_131 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_87: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_157);  linear_157 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_41: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_87, 1.0);  clone_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_90: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_41, view_127);  div_41 = view_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_43: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_90, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_158: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_43, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_43 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_159: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_160: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_132: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_158, [8, -1, 8, 80]);  linear_158 = None\n",
       "                    transpose_108: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_133: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_159, [8, -1, 8, 80]);  linear_159 = None\n",
       "                    transpose_109: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_134: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_160, [8, -1, 8, 80]);  linear_160 = None\n",
       "                    transpose_110: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_134, 1, 2);  view_134 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_27: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_108, transpose_109, transpose_110);  transpose_108 = transpose_109 = transpose_110 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_111: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_27, 1, 2);  scaled_dot_product_attention_27 = None\n",
       "                    view_135: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_111, [8, -1, 640]);  transpose_111 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_161: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_135, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_135 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_88: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_161);  linear_161 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_42: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_88, 1.0);  clone_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_91: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_42, add_90);  div_42 = add_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_44: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_91, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_162: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_44, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_44 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_11 = torch.ops.aten.split.Tensor(linear_162, 2560, -1);  linear_162 = None\n",
       "                    getitem_22: \"f32[8, 256, 2560]\" = split_11[0]\n",
       "                    getitem_23: \"f32[8, 256, 2560]\" = split_11[1];  split_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_17: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_23);  getitem_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_14: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_22, gelu_17);  getitem_22 = gelu_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_89: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_14);  mul_14 = None\n",
       "                    linear_163: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_89, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_89 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_92: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_163, add_91);  linear_163 = add_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_136: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_92, [8, 16, 16, 640]);  add_92 = None\n",
       "                    permute_24: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_136, [0, 3, 1, 2]);  view_136 = None\n",
       "                    clone_90: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_24, memory_format = torch.contiguous_format);  permute_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_75: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_90, p_unet_up_blocks_2_attentions_1_proj_out_weight, p_unet_up_blocks_2_attentions_1_proj_out_bias);  clone_90 = p_unet_up_blocks_2_attentions_1_proj_out_weight = p_unet_up_blocks_2_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_93: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_75, div_40);  conv2d_75 = div_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_13: \"f32[8, 960, 16, 16]\" = torch.ops.aten.cat.default([add_93, conv2d_9], 1);  add_93 = conv2d_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_48: \"f32[8, 960, 16, 16]\" = torch.ops.aten.group_norm.default(cat_13, 32, p_unet_up_blocks_2_resnets_2_norm1_weight, p_unet_up_blocks_2_resnets_2_norm1_bias);  p_unet_up_blocks_2_resnets_2_norm1_weight = p_unet_up_blocks_2_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_55: \"f32[8, 960, 16, 16]\" = torch.ops.aten.silu.default(group_norm_48);  group_norm_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_76: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_55, p_unet_up_blocks_2_resnets_2_conv1_weight, p_unet_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_55 = p_unet_up_blocks_2_resnets_2_conv1_weight = p_unet_up_blocks_2_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_56: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_164: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_56, p_unet_up_blocks_2_resnets_2_time_emb_proj_weight, p_unet_up_blocks_2_resnets_2_time_emb_proj_bias);  silu_56 = p_unet_up_blocks_2_resnets_2_time_emb_proj_weight = p_unet_up_blocks_2_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_38: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_164, 2);  linear_164 = None\n",
       "                    unsqueeze_39: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_38, 3);  unsqueeze_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_94: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_76, unsqueeze_39);  conv2d_76 = unsqueeze_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_49: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_94, 32, p_unet_up_blocks_2_resnets_2_norm2_weight, p_unet_up_blocks_2_resnets_2_norm2_bias);  add_94 = p_unet_up_blocks_2_resnets_2_norm2_weight = p_unet_up_blocks_2_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_57: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_49);  group_norm_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_91: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_57);  silu_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_77: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_91, p_unet_up_blocks_2_resnets_2_conv2_weight, p_unet_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_91 = p_unet_up_blocks_2_resnets_2_conv2_weight = p_unet_up_blocks_2_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_78: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_13, p_unet_up_blocks_2_resnets_2_conv_shortcut_weight, p_unet_up_blocks_2_resnets_2_conv_shortcut_bias);  cat_13 = p_unet_up_blocks_2_resnets_2_conv_shortcut_weight = p_unet_up_blocks_2_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_95: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_78, conv2d_77);  conv2d_78 = conv2d_77 = None\n",
       "                    div_43: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_95, 1.0);  add_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_50: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_43, 32, p_unet_up_blocks_2_attentions_2_norm_weight, p_unet_up_blocks_2_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_2_norm_weight = p_unet_up_blocks_2_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_79: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_50, p_unet_up_blocks_2_attentions_2_proj_in_weight, p_unet_up_blocks_2_attentions_2_proj_in_bias);  group_norm_50 = p_unet_up_blocks_2_attentions_2_proj_in_weight = p_unet_up_blocks_2_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_25: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_79, [0, 2, 3, 1]);  conv2d_79 = None\n",
       "                    view_137: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_25, [8, 256, 640]);  permute_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_45: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_137, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_165: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_166: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_167: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_45 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_138: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_165, [8, -1, 8, 80]);  linear_165 = None\n",
       "                    transpose_112: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_138, 1, 2);  view_138 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_139: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_166, [8, -1, 8, 80]);  linear_166 = None\n",
       "                    transpose_113: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_140: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_167, [8, -1, 8, 80]);  linear_167 = None\n",
       "                    transpose_114: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_28: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_112, transpose_113, transpose_114);  transpose_112 = transpose_113 = transpose_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_115: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_28, 1, 2);  scaled_dot_product_attention_28 = None\n",
       "                    view_141: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_115, [8, -1, 640]);  transpose_115 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_168: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_141, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_141 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_92: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_168);  linear_168 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_44: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_92, 1.0);  clone_92 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_96: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_44, view_137);  div_44 = view_137 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_46: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_96, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_169: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_46, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_46 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_170: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_171: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_142: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_169, [8, -1, 8, 80]);  linear_169 = None\n",
       "                    transpose_116: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_142, 1, 2);  view_142 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_143: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_170, [8, -1, 8, 80]);  linear_170 = None\n",
       "                    transpose_117: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_144: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_171, [8, -1, 8, 80]);  linear_171 = None\n",
       "                    transpose_118: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_29: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_116, transpose_117, transpose_118);  transpose_116 = transpose_117 = transpose_118 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_119: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_29, 1, 2);  scaled_dot_product_attention_29 = None\n",
       "                    view_145: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_119, [8, -1, 640]);  transpose_119 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_172: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_145, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_145 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_93: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_172);  linear_172 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_45: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_93, 1.0);  clone_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_97: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_45, add_96);  div_45 = add_96 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_47: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_97, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_173: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_47, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_47 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_12 = torch.ops.aten.split.Tensor(linear_173, 2560, -1);  linear_173 = None\n",
       "                    getitem_24: \"f32[8, 256, 2560]\" = split_12[0]\n",
       "                    getitem_25: \"f32[8, 256, 2560]\" = split_12[1];  split_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_18: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_25);  getitem_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_15: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_24, gelu_18);  getitem_24 = gelu_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_94: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_15);  mul_15 = None\n",
       "                    linear_174: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_94, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_94 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_98: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_174, add_97);  linear_174 = add_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_146: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_98, [8, 16, 16, 640]);  add_98 = None\n",
       "                    permute_26: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_146, [0, 3, 1, 2]);  view_146 = None\n",
       "                    clone_95: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_80: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_95, p_unet_up_blocks_2_attentions_2_proj_out_weight, p_unet_up_blocks_2_attentions_2_proj_out_bias);  clone_95 = p_unet_up_blocks_2_attentions_2_proj_out_weight = p_unet_up_blocks_2_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_99: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_80, div_43);  conv2d_80 = div_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_2: \"f32[8, 640, 32, 32]\" = torch.ops.aten.upsample_nearest2d.vec(add_99, None, [2.0, 2.0]);  add_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_81: \"f32[8, 640, 32, 32]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_2, p_unet_up_blocks_2_upsamplers_0_conv_weight, p_unet_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_2 = p_unet_up_blocks_2_upsamplers_0_conv_weight = p_unet_up_blocks_2_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_14: \"f32[8, 960, 32, 32]\" = torch.ops.aten.cat.default([conv2d_81, add_21], 1);  conv2d_81 = add_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_51: \"f32[8, 960, 32, 32]\" = torch.ops.aten.group_norm.default(cat_14, 32, p_unet_up_blocks_3_resnets_0_norm1_weight, p_unet_up_blocks_3_resnets_0_norm1_bias);  p_unet_up_blocks_3_resnets_0_norm1_weight = p_unet_up_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_58: \"f32[8, 960, 32, 32]\" = torch.ops.aten.silu.default(group_norm_51);  group_norm_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_82: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_58, p_unet_up_blocks_3_resnets_0_conv1_weight, p_unet_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_58 = p_unet_up_blocks_3_resnets_0_conv1_weight = p_unet_up_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_59: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_175: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_59, p_unet_up_blocks_3_resnets_0_time_emb_proj_weight, p_unet_up_blocks_3_resnets_0_time_emb_proj_bias);  silu_59 = p_unet_up_blocks_3_resnets_0_time_emb_proj_weight = p_unet_up_blocks_3_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_40: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_175, 2);  linear_175 = None\n",
       "                    unsqueeze_41: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_40, 3);  unsqueeze_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_100: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_82, unsqueeze_41);  conv2d_82 = unsqueeze_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_52: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_100, 32, p_unet_up_blocks_3_resnets_0_norm2_weight, p_unet_up_blocks_3_resnets_0_norm2_bias);  add_100 = p_unet_up_blocks_3_resnets_0_norm2_weight = p_unet_up_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_60: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_52);  group_norm_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_96: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_60);  silu_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_83: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_96, p_unet_up_blocks_3_resnets_0_conv2_weight, p_unet_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_96 = p_unet_up_blocks_3_resnets_0_conv2_weight = p_unet_up_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_84: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_14, p_unet_up_blocks_3_resnets_0_conv_shortcut_weight, p_unet_up_blocks_3_resnets_0_conv_shortcut_bias);  cat_14 = p_unet_up_blocks_3_resnets_0_conv_shortcut_weight = p_unet_up_blocks_3_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_101: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_84, conv2d_83);  conv2d_84 = conv2d_83 = None\n",
       "                    div_46: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_101, 1.0);  add_101 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_53: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_46, 32, p_unet_up_blocks_3_attentions_0_norm_weight, p_unet_up_blocks_3_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_0_norm_weight = p_unet_up_blocks_3_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_85: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_53, p_unet_up_blocks_3_attentions_0_proj_in_weight, p_unet_up_blocks_3_attentions_0_proj_in_bias);  group_norm_53 = p_unet_up_blocks_3_attentions_0_proj_in_weight = p_unet_up_blocks_3_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_27: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_85, [0, 2, 3, 1]);  conv2d_85 = None\n",
       "                    view_147: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_27, [8, 1024, 320]);  permute_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_48: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_147, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_176: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_177: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_178: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_48 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_148: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_176, [8, -1, 8, 40]);  linear_176 = None\n",
       "                    transpose_120: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_149: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_177, [8, -1, 8, 40]);  linear_177 = None\n",
       "                    transpose_121: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_150: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_178, [8, -1, 8, 40]);  linear_178 = None\n",
       "                    transpose_122: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_150, 1, 2);  view_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_30: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_120, transpose_121, transpose_122);  transpose_120 = transpose_121 = transpose_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_123: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_30, 1, 2);  scaled_dot_product_attention_30 = None\n",
       "                    view_151: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_123, [8, -1, 320]);  transpose_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_179: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_151, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_151 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_97: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_179);  linear_179 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_47: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_97, 1.0);  clone_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_102: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_47, view_147);  div_47 = view_147 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_49: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_102, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_180: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_49, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_49 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_181: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_182: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_152: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_180, [8, -1, 8, 40]);  linear_180 = None\n",
       "                    transpose_124: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_153: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_181, [8, -1, 8, 40]);  linear_181 = None\n",
       "                    transpose_125: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_154: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_182, [8, -1, 8, 40]);  linear_182 = None\n",
       "                    transpose_126: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_154, 1, 2);  view_154 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_31: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_124, transpose_125, transpose_126);  transpose_124 = transpose_125 = transpose_126 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_127: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_31, 1, 2);  scaled_dot_product_attention_31 = None\n",
       "                    view_155: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_127, [8, -1, 320]);  transpose_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_183: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_155, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_155 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_98: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_183);  linear_183 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_48: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_98, 1.0);  clone_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_103: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_48, add_102);  div_48 = add_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_50: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_103, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_184: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_50, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_50 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_13 = torch.ops.aten.split.Tensor(linear_184, 1280, -1);  linear_184 = None\n",
       "                    getitem_26: \"f32[8, 1024, 1280]\" = split_13[0]\n",
       "                    getitem_27: \"f32[8, 1024, 1280]\" = split_13[1];  split_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_19: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_27);  getitem_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_16: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_26, gelu_19);  getitem_26 = gelu_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_99: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_16);  mul_16 = None\n",
       "                    linear_185: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_99, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_99 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_104: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_185, add_103);  linear_185 = add_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_156: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_104, [8, 32, 32, 320]);  add_104 = None\n",
       "                    permute_28: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_156, [0, 3, 1, 2]);  view_156 = None\n",
       "                    clone_100: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_86: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_100, p_unet_up_blocks_3_attentions_0_proj_out_weight, p_unet_up_blocks_3_attentions_0_proj_out_bias);  clone_100 = p_unet_up_blocks_3_attentions_0_proj_out_weight = p_unet_up_blocks_3_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_105: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_86, div_46);  conv2d_86 = div_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_15: \"f32[8, 640, 32, 32]\" = torch.ops.aten.cat.default([add_105, add_15], 1);  add_105 = add_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_54: \"f32[8, 640, 32, 32]\" = torch.ops.aten.group_norm.default(cat_15, 32, p_unet_up_blocks_3_resnets_1_norm1_weight, p_unet_up_blocks_3_resnets_1_norm1_bias);  p_unet_up_blocks_3_resnets_1_norm1_weight = p_unet_up_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_61: \"f32[8, 640, 32, 32]\" = torch.ops.aten.silu.default(group_norm_54);  group_norm_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_87: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_61, p_unet_up_blocks_3_resnets_1_conv1_weight, p_unet_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_61 = p_unet_up_blocks_3_resnets_1_conv1_weight = p_unet_up_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_62: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_186: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_62, p_unet_up_blocks_3_resnets_1_time_emb_proj_weight, p_unet_up_blocks_3_resnets_1_time_emb_proj_bias);  silu_62 = p_unet_up_blocks_3_resnets_1_time_emb_proj_weight = p_unet_up_blocks_3_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_42: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_186, 2);  linear_186 = None\n",
       "                    unsqueeze_43: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_42, 3);  unsqueeze_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_106: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_87, unsqueeze_43);  conv2d_87 = unsqueeze_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_55: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_106, 32, p_unet_up_blocks_3_resnets_1_norm2_weight, p_unet_up_blocks_3_resnets_1_norm2_bias);  add_106 = p_unet_up_blocks_3_resnets_1_norm2_weight = p_unet_up_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_63: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_55);  group_norm_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_101: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_63);  silu_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_88: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_101, p_unet_up_blocks_3_resnets_1_conv2_weight, p_unet_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_101 = p_unet_up_blocks_3_resnets_1_conv2_weight = p_unet_up_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_89: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_15, p_unet_up_blocks_3_resnets_1_conv_shortcut_weight, p_unet_up_blocks_3_resnets_1_conv_shortcut_bias);  cat_15 = p_unet_up_blocks_3_resnets_1_conv_shortcut_weight = p_unet_up_blocks_3_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_107: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_89, conv2d_88);  conv2d_89 = conv2d_88 = None\n",
       "                    div_49: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_107, 1.0);  add_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_56: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_49, 32, p_unet_up_blocks_3_attentions_1_norm_weight, p_unet_up_blocks_3_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_1_norm_weight = p_unet_up_blocks_3_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_90: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_56, p_unet_up_blocks_3_attentions_1_proj_in_weight, p_unet_up_blocks_3_attentions_1_proj_in_bias);  group_norm_56 = p_unet_up_blocks_3_attentions_1_proj_in_weight = p_unet_up_blocks_3_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_29: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_90, [0, 2, 3, 1]);  conv2d_90 = None\n",
       "                    view_157: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_29, [8, 1024, 320]);  permute_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_51: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_157, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_187: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_188: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_189: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_51 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_158: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_187, [8, -1, 8, 40]);  linear_187 = None\n",
       "                    transpose_128: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_158, 1, 2);  view_158 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_159: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_188, [8, -1, 8, 40]);  linear_188 = None\n",
       "                    transpose_129: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_160: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_189, [8, -1, 8, 40]);  linear_189 = None\n",
       "                    transpose_130: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_32: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_128, transpose_129, transpose_130);  transpose_128 = transpose_129 = transpose_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_131: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_32, 1, 2);  scaled_dot_product_attention_32 = None\n",
       "                    view_161: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_131, [8, -1, 320]);  transpose_131 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_190: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_161, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_161 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_102: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_190);  linear_190 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_50: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_102, 1.0);  clone_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_108: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_50, view_157);  div_50 = view_157 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_52: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_108, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_191: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_52, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_52 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_192: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_193: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_162: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_191, [8, -1, 8, 40]);  linear_191 = None\n",
       "                    transpose_132: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_162, 1, 2);  view_162 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_163: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_192, [8, -1, 8, 40]);  linear_192 = None\n",
       "                    transpose_133: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_163, 1, 2);  view_163 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_164: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_193, [8, -1, 8, 40]);  linear_193 = None\n",
       "                    transpose_134: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_164, 1, 2);  view_164 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_33: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_132, transpose_133, transpose_134);  transpose_132 = transpose_133 = transpose_134 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_135: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_33, 1, 2);  scaled_dot_product_attention_33 = None\n",
       "                    view_165: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_135, [8, -1, 320]);  transpose_135 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_194: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_165, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_165 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_103: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_194);  linear_194 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_51: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_103, 1.0);  clone_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_109: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_51, add_108);  div_51 = add_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_53: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_109, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_195: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_53, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_53 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_14 = torch.ops.aten.split.Tensor(linear_195, 1280, -1);  linear_195 = None\n",
       "                    getitem_28: \"f32[8, 1024, 1280]\" = split_14[0]\n",
       "                    getitem_29: \"f32[8, 1024, 1280]\" = split_14[1];  split_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_20: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_29);  getitem_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_17: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_28, gelu_20);  getitem_28 = gelu_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_104: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_17);  mul_17 = None\n",
       "                    linear_196: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_104, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_104 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_110: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_196, add_109);  linear_196 = add_109 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_166: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_110, [8, 32, 32, 320]);  add_110 = None\n",
       "                    permute_30: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_166, [0, 3, 1, 2]);  view_166 = None\n",
       "                    clone_105: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_30, memory_format = torch.contiguous_format);  permute_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_91: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_105, p_unet_up_blocks_3_attentions_1_proj_out_weight, p_unet_up_blocks_3_attentions_1_proj_out_bias);  clone_105 = p_unet_up_blocks_3_attentions_1_proj_out_weight = p_unet_up_blocks_3_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_111: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_91, div_49);  conv2d_91 = div_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_16: \"f32[8, 640, 32, 32]\" = torch.ops.aten.cat.default([add_111, conv2d], 1);  add_111 = conv2d = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_57: \"f32[8, 640, 32, 32]\" = torch.ops.aten.group_norm.default(cat_16, 32, p_unet_up_blocks_3_resnets_2_norm1_weight, p_unet_up_blocks_3_resnets_2_norm1_bias);  p_unet_up_blocks_3_resnets_2_norm1_weight = p_unet_up_blocks_3_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_64: \"f32[8, 640, 32, 32]\" = torch.ops.aten.silu.default(group_norm_57);  group_norm_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_92: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_64, p_unet_up_blocks_3_resnets_2_conv1_weight, p_unet_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_64 = p_unet_up_blocks_3_resnets_2_conv1_weight = p_unet_up_blocks_3_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_65: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25);  linear_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_197: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_65, p_unet_up_blocks_3_resnets_2_time_emb_proj_weight, p_unet_up_blocks_3_resnets_2_time_emb_proj_bias);  silu_65 = p_unet_up_blocks_3_resnets_2_time_emb_proj_weight = p_unet_up_blocks_3_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_44: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_197, 2);  linear_197 = None\n",
       "                    unsqueeze_45: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_44, 3);  unsqueeze_44 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_112: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_92, unsqueeze_45);  conv2d_92 = unsqueeze_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_58: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_112, 32, p_unet_up_blocks_3_resnets_2_norm2_weight, p_unet_up_blocks_3_resnets_2_norm2_bias);  add_112 = p_unet_up_blocks_3_resnets_2_norm2_weight = p_unet_up_blocks_3_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_66: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_58);  group_norm_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_106: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_66);  silu_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_93: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_106, p_unet_up_blocks_3_resnets_2_conv2_weight, p_unet_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_106 = p_unet_up_blocks_3_resnets_2_conv2_weight = p_unet_up_blocks_3_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_94: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_16, p_unet_up_blocks_3_resnets_2_conv_shortcut_weight, p_unet_up_blocks_3_resnets_2_conv_shortcut_bias);  cat_16 = p_unet_up_blocks_3_resnets_2_conv_shortcut_weight = p_unet_up_blocks_3_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_113: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_94, conv2d_93);  conv2d_94 = conv2d_93 = None\n",
       "                    div_52: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_113, 1.0);  add_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_59: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_52, 32, p_unet_up_blocks_3_attentions_2_norm_weight, p_unet_up_blocks_3_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_2_norm_weight = p_unet_up_blocks_3_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_95: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_59, p_unet_up_blocks_3_attentions_2_proj_in_weight, p_unet_up_blocks_3_attentions_2_proj_in_bias);  group_norm_59 = p_unet_up_blocks_3_attentions_2_proj_in_weight = p_unet_up_blocks_3_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_31: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_95, [0, 2, 3, 1]);  conv2d_95 = None\n",
       "                    view_167: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_31, [8, 1024, 320]);  permute_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_54: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_167, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_198: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_199: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_200: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_54 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_168: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_198, [8, -1, 8, 40]);  linear_198 = None\n",
       "                    transpose_136: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_168, 1, 2);  view_168 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_169: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_199, [8, -1, 8, 40]);  linear_199 = None\n",
       "                    transpose_137: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_169, 1, 2);  view_169 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_170: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_200, [8, -1, 8, 40]);  linear_200 = None\n",
       "                    transpose_138: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_170, 1, 2);  view_170 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_34: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_136, transpose_137, transpose_138);  transpose_136 = transpose_137 = transpose_138 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_139: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_34, 1, 2);  scaled_dot_product_attention_34 = None\n",
       "                    view_171: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_139, [8, -1, 320]);  transpose_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_201: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_171, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_171 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_107: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_201);  linear_201 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_53: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_107, 1.0);  clone_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_114: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_53, view_167);  div_53 = view_167 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_55: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_114, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_202: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_55, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_55 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_203: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_204: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight);  add_9 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_172: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_202, [8, -1, 8, 40]);  linear_202 = None\n",
       "                    transpose_140: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_172, 1, 2);  view_172 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_173: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_203, [8, -1, 8, 40]);  linear_203 = None\n",
       "                    transpose_141: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_173, 1, 2);  view_173 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_174: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_204, [8, -1, 8, 40]);  linear_204 = None\n",
       "                    transpose_142: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_174, 1, 2);  view_174 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_35: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_140, transpose_141, transpose_142);  transpose_140 = transpose_141 = transpose_142 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_143: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_35, 1, 2);  scaled_dot_product_attention_35 = None\n",
       "                    view_175: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_143, [8, -1, 320]);  transpose_143 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_205: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_175, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_175 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_108: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_205);  linear_205 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_54: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_108, 1.0);  clone_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_115: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_54, add_114);  div_54 = add_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_56: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_115, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_206: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_56, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_56 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_15 = torch.ops.aten.split.Tensor(linear_206, 1280, -1);  linear_206 = None\n",
       "                    getitem_30: \"f32[8, 1024, 1280]\" = split_15[0]\n",
       "                    getitem_31: \"f32[8, 1024, 1280]\" = split_15[1];  split_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_21: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_31);  getitem_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_18: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_30, gelu_21);  getitem_30 = gelu_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_109: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_18);  mul_18 = None\n",
       "                    linear_207: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_109, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_109 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_116: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_207, add_115);  linear_207 = add_115 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_176: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_116, [8, 32, 32, 320]);  add_116 = None\n",
       "                    permute_32: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_176, [0, 3, 1, 2]);  view_176 = None\n",
       "                    clone_110: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_96: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_110, p_unet_up_blocks_3_attentions_2_proj_out_weight, p_unet_up_blocks_3_attentions_2_proj_out_bias);  clone_110 = p_unet_up_blocks_3_attentions_2_proj_out_weight = p_unet_up_blocks_3_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_117: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_96, div_52);  conv2d_96 = div_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1301 in forward, code: sample = self.conv_norm_out(sample)\n",
       "                    group_norm_60: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_117, 32, p_unet_conv_norm_out_weight, p_unet_conv_norm_out_bias);  add_117 = p_unet_conv_norm_out_weight = p_unet_conv_norm_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1302 in forward, code: sample = self.conv_act(sample)\n",
       "                    silu_67: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_60);  group_norm_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1303 in forward, code: sample = self.conv_out(sample)\n",
       "                    conv2d_97: \"f32[8, 4, 32, 32]\" = torch.ops.aten.conv2d.default(silu_67, p_unet_conv_out_weight, p_unet_conv_out_bias, [1, 1], [1, 1]);  silu_67 = p_unet_conv_out_weight = p_unet_conv_out_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:48 in decode_latents, code: latents = (1/  self.scaling_factor) * latents\n",
       "                    mul_19: \"f32[8, 4, 32, 32]\" = torch.ops.aten.mul.Tensor(conv2d_97, 5.489980785067252);  conv2d_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:290 in _decode, code: z = self.post_quant_conv(z)\n",
       "                    conv2d_98: \"f32[8, 4, 32, 32]\" = torch.ops.aten.conv2d.default(mul_19, p_vae_post_quant_conv_weight, p_vae_post_quant_conv_bias);  mul_19 = p_vae_post_quant_conv_weight = p_vae_post_quant_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:291 in forward, code: sample = self.conv_in(sample)\n",
       "                    conv2d_99: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(conv2d_98, p_vae_decoder_conv_in_weight, p_vae_decoder_conv_in_bias, [1, 1], [1, 1]);  conv2d_98 = p_vae_decoder_conv_in_weight = p_vae_decoder_conv_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_61: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_99, 32, p_vae_decoder_mid_block_resnets_0_norm1_weight, p_vae_decoder_mid_block_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_mid_block_resnets_0_norm1_weight = p_vae_decoder_mid_block_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_68: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_61);  group_norm_61 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_100: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_68, p_vae_decoder_mid_block_resnets_0_conv1_weight, p_vae_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_68 = p_vae_decoder_mid_block_resnets_0_conv1_weight = p_vae_decoder_mid_block_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_62: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_100, 32, p_vae_decoder_mid_block_resnets_0_norm2_weight, p_vae_decoder_mid_block_resnets_0_norm2_bias, 1e-06);  conv2d_100 = p_vae_decoder_mid_block_resnets_0_norm2_weight = p_vae_decoder_mid_block_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_69: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_62);  group_norm_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_111: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_69);  silu_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_101: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_111, p_vae_decoder_mid_block_resnets_0_conv2_weight, p_vae_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_111 = p_vae_decoder_mid_block_resnets_0_conv2_weight = p_vae_decoder_mid_block_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_118: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_99, conv2d_101);  conv2d_99 = conv2d_101 = None\n",
       "                    scalar_tensor_default_4: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_55: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_118, scalar_tensor_default_4);  add_118 = scalar_tensor_default_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2318 in __call__, code: hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
       "                    view_177: \"f32[8, 512, 1024]\" = torch.ops.aten.view.default(div_55, [8, 512, 1024])\n",
       "                    transpose_144: \"f32[8, 1024, 512]\" = torch.ops.aten.transpose.int(view_177, 1, 2);  view_177 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2331 in __call__, code: hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
       "                    transpose_145: \"f32[8, 512, 1024]\" = torch.ops.aten.transpose.int(transpose_144, 1, 2);  transpose_144 = None\n",
       "                    group_norm_63: \"f32[8, 512, 1024]\" = torch.ops.aten.group_norm.default(transpose_145, 32, p_vae_decoder_mid_block_attentions_0_group_norm_weight, p_vae_decoder_mid_block_attentions_0_group_norm_bias, 1e-06);  transpose_145 = p_vae_decoder_mid_block_attentions_0_group_norm_weight = p_vae_decoder_mid_block_attentions_0_group_norm_bias = None\n",
       "                    transpose_146: \"f32[8, 1024, 512]\" = torch.ops.aten.transpose.int(group_norm_63, 1, 2);  group_norm_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_208: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_q_weight, p_vae_decoder_mid_block_attentions_0_to_q_bias);  p_vae_decoder_mid_block_attentions_0_to_q_weight = p_vae_decoder_mid_block_attentions_0_to_q_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_209: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_k_weight, p_vae_decoder_mid_block_attentions_0_to_k_bias);  p_vae_decoder_mid_block_attentions_0_to_k_weight = p_vae_decoder_mid_block_attentions_0_to_k_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_210: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_v_weight, p_vae_decoder_mid_block_attentions_0_to_v_bias);  transpose_146 = p_vae_decoder_mid_block_attentions_0_to_v_weight = p_vae_decoder_mid_block_attentions_0_to_v_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_178: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_208, [8, -1, 1, 512]);  linear_208 = None\n",
       "                    transpose_147: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_178, 1, 2);  view_178 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_179: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_209, [8, -1, 1, 512]);  linear_209 = None\n",
       "                    transpose_148: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_179, 1, 2);  view_179 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_180: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_210, [8, -1, 1, 512]);  linear_210 = None\n",
       "                    transpose_149: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_180, 1, 2);  view_180 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_36: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_147, transpose_148, transpose_149);  transpose_147 = transpose_148 = transpose_149 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_150: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_36, 1, 2);  scaled_dot_product_attention_36 = None\n",
       "                    view_181: \"f32[8, 1024, 512]\" = torch.ops.aten.view.default(transpose_150, [8, -1, 512]);  transpose_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_211: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(view_181, p_vae_decoder_mid_block_attentions_0_to_out_0_weight, p_vae_decoder_mid_block_attentions_0_to_out_0_bias);  view_181 = p_vae_decoder_mid_block_attentions_0_to_out_0_weight = p_vae_decoder_mid_block_attentions_0_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_112: \"f32[8, 1024, 512]\" = torch.ops.aten.clone.default(linear_211);  linear_211 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2371 in __call__, code: hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
       "                    transpose_151: \"f32[8, 512, 1024]\" = torch.ops.aten.transpose.int(clone_112, -1, -2);  clone_112 = None\n",
       "                    view_182: \"f32[8, 512, 32, 32]\" = torch.ops.aten.view.default(transpose_151, [8, 512, 32, 32]);  transpose_151 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2374 in __call__, code: hidden_states = hidden_states + residual\n",
       "                    add_119: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(view_182, div_55);  view_182 = div_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    scalar_tensor_default_5: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_56: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_119, scalar_tensor_default_5);  add_119 = scalar_tensor_default_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_64: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_56, 32, p_vae_decoder_mid_block_resnets_1_norm1_weight, p_vae_decoder_mid_block_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_mid_block_resnets_1_norm1_weight = p_vae_decoder_mid_block_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_70: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_64);  group_norm_64 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_102: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_70, p_vae_decoder_mid_block_resnets_1_conv1_weight, p_vae_decoder_mid_block_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_70 = p_vae_decoder_mid_block_resnets_1_conv1_weight = p_vae_decoder_mid_block_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_65: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_102, 32, p_vae_decoder_mid_block_resnets_1_norm2_weight, p_vae_decoder_mid_block_resnets_1_norm2_bias, 1e-06);  conv2d_102 = p_vae_decoder_mid_block_resnets_1_norm2_weight = p_vae_decoder_mid_block_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_71: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_65);  group_norm_65 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_113: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_71);  silu_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_103: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_113, p_vae_decoder_mid_block_resnets_1_conv2_weight, p_vae_decoder_mid_block_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_113 = p_vae_decoder_mid_block_resnets_1_conv2_weight = p_vae_decoder_mid_block_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_120: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_56, conv2d_103);  div_56 = conv2d_103 = None\n",
       "                    scalar_tensor_default_6: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_57: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_120, scalar_tensor_default_6);  add_120 = scalar_tensor_default_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_66: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_57, 32, p_vae_decoder_up_blocks_0_resnets_0_norm1_weight, p_vae_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_0_norm1_weight = p_vae_decoder_up_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_72: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_66);  group_norm_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_104: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_72, p_vae_decoder_up_blocks_0_resnets_0_conv1_weight, p_vae_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_72 = p_vae_decoder_up_blocks_0_resnets_0_conv1_weight = p_vae_decoder_up_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_67: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_104, 32, p_vae_decoder_up_blocks_0_resnets_0_norm2_weight, p_vae_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06);  conv2d_104 = p_vae_decoder_up_blocks_0_resnets_0_norm2_weight = p_vae_decoder_up_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_73: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_67);  group_norm_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_114: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_73);  silu_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_105: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_114, p_vae_decoder_up_blocks_0_resnets_0_conv2_weight, p_vae_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_114 = p_vae_decoder_up_blocks_0_resnets_0_conv2_weight = p_vae_decoder_up_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_121: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_57, conv2d_105);  div_57 = conv2d_105 = None\n",
       "                    div_58: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_121, 1.0);  add_121 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_68: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_58, 32, p_vae_decoder_up_blocks_0_resnets_1_norm1_weight, p_vae_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_1_norm1_weight = p_vae_decoder_up_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_74: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_68);  group_norm_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_106: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_74, p_vae_decoder_up_blocks_0_resnets_1_conv1_weight, p_vae_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_74 = p_vae_decoder_up_blocks_0_resnets_1_conv1_weight = p_vae_decoder_up_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_69: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_106, 32, p_vae_decoder_up_blocks_0_resnets_1_norm2_weight, p_vae_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06);  conv2d_106 = p_vae_decoder_up_blocks_0_resnets_1_norm2_weight = p_vae_decoder_up_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_75: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_69);  group_norm_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_115: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_75);  silu_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_107: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_115, p_vae_decoder_up_blocks_0_resnets_1_conv2_weight, p_vae_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_115 = p_vae_decoder_up_blocks_0_resnets_1_conv2_weight = p_vae_decoder_up_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_122: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_58, conv2d_107);  div_58 = conv2d_107 = None\n",
       "                    div_59: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_122, 1.0);  add_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_70: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_59, 32, p_vae_decoder_up_blocks_0_resnets_2_norm1_weight, p_vae_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_2_norm1_weight = p_vae_decoder_up_blocks_0_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_76: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_70);  group_norm_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_108: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_76, p_vae_decoder_up_blocks_0_resnets_2_conv1_weight, p_vae_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_76 = p_vae_decoder_up_blocks_0_resnets_2_conv1_weight = p_vae_decoder_up_blocks_0_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_71: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_108, 32, p_vae_decoder_up_blocks_0_resnets_2_norm2_weight, p_vae_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06);  conv2d_108 = p_vae_decoder_up_blocks_0_resnets_2_norm2_weight = p_vae_decoder_up_blocks_0_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_77: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_71);  group_norm_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_116: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_77);  silu_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_109: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_116, p_vae_decoder_up_blocks_0_resnets_2_conv2_weight, p_vae_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_116 = p_vae_decoder_up_blocks_0_resnets_2_conv2_weight = p_vae_decoder_up_blocks_0_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_123: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_59, conv2d_109);  div_59 = conv2d_109 = None\n",
       "                    div_60: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_123, 1.0);  add_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_3: \"f32[8, 512, 64, 64]\" = torch.ops.aten.upsample_nearest2d.vec(div_60, None, [2.0, 2.0]);  div_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_110: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_3, p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_3 = p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_72: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_110, 32, p_vae_decoder_up_blocks_1_resnets_0_norm1_weight, p_vae_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_0_norm1_weight = p_vae_decoder_up_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_78: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_72);  group_norm_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_111: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_78, p_vae_decoder_up_blocks_1_resnets_0_conv1_weight, p_vae_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_78 = p_vae_decoder_up_blocks_1_resnets_0_conv1_weight = p_vae_decoder_up_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_73: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_111, 32, p_vae_decoder_up_blocks_1_resnets_0_norm2_weight, p_vae_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06);  conv2d_111 = p_vae_decoder_up_blocks_1_resnets_0_norm2_weight = p_vae_decoder_up_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_79: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_73);  group_norm_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_117: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_79);  silu_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_112: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_117, p_vae_decoder_up_blocks_1_resnets_0_conv2_weight, p_vae_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_117 = p_vae_decoder_up_blocks_1_resnets_0_conv2_weight = p_vae_decoder_up_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_124: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(conv2d_110, conv2d_112);  conv2d_110 = conv2d_112 = None\n",
       "                    div_61: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_124, 1.0);  add_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_74: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(div_61, 32, p_vae_decoder_up_blocks_1_resnets_1_norm1_weight, p_vae_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_1_norm1_weight = p_vae_decoder_up_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_80: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_74);  group_norm_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_113: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_80, p_vae_decoder_up_blocks_1_resnets_1_conv1_weight, p_vae_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_80 = p_vae_decoder_up_blocks_1_resnets_1_conv1_weight = p_vae_decoder_up_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_75: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_113, 32, p_vae_decoder_up_blocks_1_resnets_1_norm2_weight, p_vae_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06);  conv2d_113 = p_vae_decoder_up_blocks_1_resnets_1_norm2_weight = p_vae_decoder_up_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_81: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_75);  group_norm_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_118: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_81);  silu_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_114: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_118, p_vae_decoder_up_blocks_1_resnets_1_conv2_weight, p_vae_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_118 = p_vae_decoder_up_blocks_1_resnets_1_conv2_weight = p_vae_decoder_up_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_125: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(div_61, conv2d_114);  div_61 = conv2d_114 = None\n",
       "                    div_62: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_125, 1.0);  add_125 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_76: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(div_62, 32, p_vae_decoder_up_blocks_1_resnets_2_norm1_weight, p_vae_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_2_norm1_weight = p_vae_decoder_up_blocks_1_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_82: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_76);  group_norm_76 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_115: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_82, p_vae_decoder_up_blocks_1_resnets_2_conv1_weight, p_vae_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_82 = p_vae_decoder_up_blocks_1_resnets_2_conv1_weight = p_vae_decoder_up_blocks_1_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_77: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_115, 32, p_vae_decoder_up_blocks_1_resnets_2_norm2_weight, p_vae_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06);  conv2d_115 = p_vae_decoder_up_blocks_1_resnets_2_norm2_weight = p_vae_decoder_up_blocks_1_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_83: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_77);  group_norm_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_119: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_83);  silu_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_116: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_119, p_vae_decoder_up_blocks_1_resnets_2_conv2_weight, p_vae_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_119 = p_vae_decoder_up_blocks_1_resnets_2_conv2_weight = p_vae_decoder_up_blocks_1_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_126: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(div_62, conv2d_116);  div_62 = conv2d_116 = None\n",
       "                    div_63: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_126, 1.0);  add_126 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_4: \"f32[8, 512, 128, 128]\" = torch.ops.aten.upsample_nearest2d.vec(div_63, None, [2.0, 2.0]);  div_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_117: \"f32[8, 512, 128, 128]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_4, p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_4 = p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_78: \"f32[8, 512, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_117, 32, p_vae_decoder_up_blocks_2_resnets_0_norm1_weight, p_vae_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_0_norm1_weight = p_vae_decoder_up_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_84: \"f32[8, 512, 128, 128]\" = torch.ops.aten.silu.default(group_norm_78);  group_norm_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_118: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_84, p_vae_decoder_up_blocks_2_resnets_0_conv1_weight, p_vae_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_84 = p_vae_decoder_up_blocks_2_resnets_0_conv1_weight = p_vae_decoder_up_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_79: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_118, 32, p_vae_decoder_up_blocks_2_resnets_0_norm2_weight, p_vae_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06);  conv2d_118 = p_vae_decoder_up_blocks_2_resnets_0_norm2_weight = p_vae_decoder_up_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_85: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_79);  group_norm_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_120: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_85);  silu_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_119: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_120, p_vae_decoder_up_blocks_2_resnets_0_conv2_weight, p_vae_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_120 = p_vae_decoder_up_blocks_2_resnets_0_conv2_weight = p_vae_decoder_up_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_120: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(conv2d_117, p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias);  conv2d_117 = p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight = p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_127: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(conv2d_120, conv2d_119);  conv2d_120 = conv2d_119 = None\n",
       "                    div_64: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_127, 1.0);  add_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_80: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(div_64, 32, p_vae_decoder_up_blocks_2_resnets_1_norm1_weight, p_vae_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_1_norm1_weight = p_vae_decoder_up_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_86: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_80);  group_norm_80 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_121: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_86, p_vae_decoder_up_blocks_2_resnets_1_conv1_weight, p_vae_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_86 = p_vae_decoder_up_blocks_2_resnets_1_conv1_weight = p_vae_decoder_up_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_81: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_121, 32, p_vae_decoder_up_blocks_2_resnets_1_norm2_weight, p_vae_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06);  conv2d_121 = p_vae_decoder_up_blocks_2_resnets_1_norm2_weight = p_vae_decoder_up_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_87: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_81);  group_norm_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_121: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_87);  silu_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_122: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_121, p_vae_decoder_up_blocks_2_resnets_1_conv2_weight, p_vae_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_121 = p_vae_decoder_up_blocks_2_resnets_1_conv2_weight = p_vae_decoder_up_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_128: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(div_64, conv2d_122);  div_64 = conv2d_122 = None\n",
       "                    div_65: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_128, 1.0);  add_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_82: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(div_65, 32, p_vae_decoder_up_blocks_2_resnets_2_norm1_weight, p_vae_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_2_norm1_weight = p_vae_decoder_up_blocks_2_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_88: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_82);  group_norm_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_123: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_88, p_vae_decoder_up_blocks_2_resnets_2_conv1_weight, p_vae_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_88 = p_vae_decoder_up_blocks_2_resnets_2_conv1_weight = p_vae_decoder_up_blocks_2_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_83: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_123, 32, p_vae_decoder_up_blocks_2_resnets_2_norm2_weight, p_vae_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06);  conv2d_123 = p_vae_decoder_up_blocks_2_resnets_2_norm2_weight = p_vae_decoder_up_blocks_2_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_89: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_83);  group_norm_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_122: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_89);  silu_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_124: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_122, p_vae_decoder_up_blocks_2_resnets_2_conv2_weight, p_vae_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_122 = p_vae_decoder_up_blocks_2_resnets_2_conv2_weight = p_vae_decoder_up_blocks_2_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_129: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(div_65, conv2d_124);  div_65 = conv2d_124 = None\n",
       "                    div_66: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_129, 1.0);  add_129 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_5: \"f32[8, 256, 256, 256]\" = torch.ops.aten.upsample_nearest2d.vec(div_66, None, [2.0, 2.0]);  div_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_125: \"f32[8, 256, 256, 256]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_5, p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_5 = p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_84: \"f32[8, 256, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_125, 32, p_vae_decoder_up_blocks_3_resnets_0_norm1_weight, p_vae_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_0_norm1_weight = p_vae_decoder_up_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_90: \"f32[8, 256, 256, 256]\" = torch.ops.aten.silu.default(group_norm_84);  group_norm_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_126: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_90, p_vae_decoder_up_blocks_3_resnets_0_conv1_weight, p_vae_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_90 = p_vae_decoder_up_blocks_3_resnets_0_conv1_weight = p_vae_decoder_up_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_85: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_126, 32, p_vae_decoder_up_blocks_3_resnets_0_norm2_weight, p_vae_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06);  conv2d_126 = p_vae_decoder_up_blocks_3_resnets_0_norm2_weight = p_vae_decoder_up_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_91: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_85);  group_norm_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_123: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_91);  silu_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_127: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_123, p_vae_decoder_up_blocks_3_resnets_0_conv2_weight, p_vae_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_123 = p_vae_decoder_up_blocks_3_resnets_0_conv2_weight = p_vae_decoder_up_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_128: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(conv2d_125, p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias);  conv2d_125 = p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight = p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_130: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(conv2d_128, conv2d_127);  conv2d_128 = conv2d_127 = None\n",
       "                    div_67: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_130, 1.0);  add_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_86: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_67, 32, p_vae_decoder_up_blocks_3_resnets_1_norm1_weight, p_vae_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_1_norm1_weight = p_vae_decoder_up_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_92: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_86);  group_norm_86 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_129: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_92, p_vae_decoder_up_blocks_3_resnets_1_conv1_weight, p_vae_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_92 = p_vae_decoder_up_blocks_3_resnets_1_conv1_weight = p_vae_decoder_up_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_87: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_129, 32, p_vae_decoder_up_blocks_3_resnets_1_norm2_weight, p_vae_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06);  conv2d_129 = p_vae_decoder_up_blocks_3_resnets_1_norm2_weight = p_vae_decoder_up_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_93: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_87);  group_norm_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_124: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_93);  silu_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_130: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_124, p_vae_decoder_up_blocks_3_resnets_1_conv2_weight, p_vae_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_124 = p_vae_decoder_up_blocks_3_resnets_1_conv2_weight = p_vae_decoder_up_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_131: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(div_67, conv2d_130);  div_67 = conv2d_130 = None\n",
       "                    div_68: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_131, 1.0);  add_131 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_88: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_68, 32, p_vae_decoder_up_blocks_3_resnets_2_norm1_weight, p_vae_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_2_norm1_weight = p_vae_decoder_up_blocks_3_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_94: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_88);  group_norm_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_131: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_94, p_vae_decoder_up_blocks_3_resnets_2_conv1_weight, p_vae_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_94 = p_vae_decoder_up_blocks_3_resnets_2_conv1_weight = p_vae_decoder_up_blocks_3_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_89: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_131, 32, p_vae_decoder_up_blocks_3_resnets_2_norm2_weight, p_vae_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06);  conv2d_131 = p_vae_decoder_up_blocks_3_resnets_2_norm2_weight = p_vae_decoder_up_blocks_3_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_95: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_89);  group_norm_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_125: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_95);  silu_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_132: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_125, p_vae_decoder_up_blocks_3_resnets_2_conv2_weight, p_vae_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_125 = p_vae_decoder_up_blocks_3_resnets_2_conv2_weight = p_vae_decoder_up_blocks_3_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_132: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(div_68, conv2d_132);  div_68 = conv2d_132 = None\n",
       "                    div_69: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_132, 1.0);  add_132 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:341 in forward, code: sample = self.conv_norm_out(sample)\n",
       "                    group_norm_90: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_69, 32, p_vae_decoder_conv_norm_out_weight, p_vae_decoder_conv_norm_out_bias, 1e-06);  div_69 = p_vae_decoder_conv_norm_out_weight = p_vae_decoder_conv_norm_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:344 in forward, code: sample = self.conv_act(sample)\n",
       "                    silu_96: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_90);  group_norm_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:345 in forward, code: sample = self.conv_out(sample)\n",
       "                    conv2d_133: \"f32[8, 3, 256, 256]\" = torch.ops.aten.conv2d.default(silu_96, p_vae_decoder_conv_out_weight, p_vae_decoder_conv_out_bias, [1, 1], [1, 1]);  silu_96 = p_vae_decoder_conv_out_weight = p_vae_decoder_conv_out_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:50 in decode_latents, code: image = (image / 2 + 0.5).clamp(0, 1)\n",
       "                    scalar_tensor_default_7: \"f32[]\" = torch.ops.aten.scalar_tensor.default(2, dtype = torch.float32)\n",
       "                    div_70: \"f32[8, 3, 256, 256]\" = torch.ops.aten.div.Tensor(conv2d_133, scalar_tensor_default_7);  conv2d_133 = scalar_tensor_default_7 = None\n",
       "                    add_133: \"f32[8, 3, 256, 256]\" = torch.ops.aten.add.Tensor(div_70, 0.5);  div_70 = None\n",
       "                    scalar_tensor_default_8: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
       "                    scalar_tensor_default_9: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    clamp: \"f32[8, 3, 256, 256]\" = torch.ops.aten.clamp.Tensor(add_133, scalar_tensor_default_8, scalar_tensor_default_9);  add_133 = scalar_tensor_default_8 = scalar_tensor_default_9 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:51 in decode_latents, code: image = image.detach().cpu().permute(0, 2, 3, 1).float().numpy()\n",
       "                    detach: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(clamp);  clamp = None\n",
       "                    detach_1: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(detach);  detach = None\n",
       "                    detach_2: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
       "                    _to_copy_4: \"f32[8, 3, 256, 256]\" = torch.ops.aten._to_copy.default(detach_2, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  detach_2 = None\n",
       "                    permute_33: \"f32[8, 256, 256, 3]\" = torch.ops.aten.permute.default(_to_copy_4, [0, 2, 3, 1]);  _to_copy_4 = None\n",
       "                    view_183: \"f32[8, 256, 256, 3]\" = torch.ops.aten.view.default(permute_33, [8, 256, 256, 3]);  permute_33 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:52 in decode_latents, code: image = (image * 255).round().astype(\"uint8\")\n",
       "                    clone_126: \"f32[]\" = torch.ops.aten.clone.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n",
       "                    multiply: \"f32[8, 256, 256, 3]\" = torch.ops.aten.multiply.Tensor(view_183, clone_126);  view_183 = clone_126 = None\n",
       "                    round_1: \"f32[8, 256, 256, 3]\" = torch.ops.aten.round.decimals(multiply, decimals = 0);  multiply = None\n",
       "                    _to_copy_5: \"u8[8, 256, 256, 3]\" = torch.ops.aten._to_copy.default(round_1, dtype = torch.uint8);  round_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:53 in decode_latents, code: image = image[...,::-1] # RGB to BGR\n",
       "                    flip: \"u8[8, 256, 256, 3]\" = torch.ops.aten.flip.default(_to_copy_5, [1]);  _to_copy_5 = None\n",
       "                    alias: \"u8[8, 256, 256, 3]\" = torch.ops.aten.alias.default(flip);  flip = None\n",
       "                    return (alias,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_whisper_encoder_embed_positions_weight: PARAMETER target='whisper_encoder.embed_positions.weight'\n",
       "            p_whisper_encoder_conv1_weight: PARAMETER target='whisper_encoder.conv1.weight'\n",
       "            p_whisper_encoder_conv1_bias: PARAMETER target='whisper_encoder.conv1.bias'\n",
       "            p_whisper_encoder_conv2_weight: PARAMETER target='whisper_encoder.conv2.weight'\n",
       "            p_whisper_encoder_conv2_bias: PARAMETER target='whisper_encoder.conv2.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.0.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.0.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_0_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.0.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_0_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.0.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_0_fc1_weight: PARAMETER target='whisper_encoder.layers.0.fc1.weight'\n",
       "            p_whisper_encoder_layers_0_fc1_bias: PARAMETER target='whisper_encoder.layers.0.fc1.bias'\n",
       "            p_whisper_encoder_layers_0_fc2_weight: PARAMETER target='whisper_encoder.layers.0.fc2.weight'\n",
       "            p_whisper_encoder_layers_0_fc2_bias: PARAMETER target='whisper_encoder.layers.0.fc2.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.1.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.1.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_1_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.1.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_1_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.1.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_1_fc1_weight: PARAMETER target='whisper_encoder.layers.1.fc1.weight'\n",
       "            p_whisper_encoder_layers_1_fc1_bias: PARAMETER target='whisper_encoder.layers.1.fc1.bias'\n",
       "            p_whisper_encoder_layers_1_fc2_weight: PARAMETER target='whisper_encoder.layers.1.fc2.weight'\n",
       "            p_whisper_encoder_layers_1_fc2_bias: PARAMETER target='whisper_encoder.layers.1.fc2.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.2.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.2.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_2_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.2.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_2_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.2.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_2_fc1_weight: PARAMETER target='whisper_encoder.layers.2.fc1.weight'\n",
       "            p_whisper_encoder_layers_2_fc1_bias: PARAMETER target='whisper_encoder.layers.2.fc1.bias'\n",
       "            p_whisper_encoder_layers_2_fc2_weight: PARAMETER target='whisper_encoder.layers.2.fc2.weight'\n",
       "            p_whisper_encoder_layers_2_fc2_bias: PARAMETER target='whisper_encoder.layers.2.fc2.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.3.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.3.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_3_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.3.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_3_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.3.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_3_fc1_weight: PARAMETER target='whisper_encoder.layers.3.fc1.weight'\n",
       "            p_whisper_encoder_layers_3_fc1_bias: PARAMETER target='whisper_encoder.layers.3.fc1.bias'\n",
       "            p_whisper_encoder_layers_3_fc2_weight: PARAMETER target='whisper_encoder.layers.3.fc2.weight'\n",
       "            p_whisper_encoder_layers_3_fc2_bias: PARAMETER target='whisper_encoder.layers.3.fc2.bias'\n",
       "            p_whisper_encoder_layer_norm_weight: PARAMETER target='whisper_encoder.layer_norm.weight'\n",
       "            p_whisper_encoder_layer_norm_bias: PARAMETER target='whisper_encoder.layer_norm.bias'\n",
       "            p_unet_time_embedding_linear_1_weight: PARAMETER target='unet.time_embedding.linear_1.weight'\n",
       "            p_unet_time_embedding_linear_1_bias: PARAMETER target='unet.time_embedding.linear_1.bias'\n",
       "            p_unet_time_embedding_linear_2_weight: PARAMETER target='unet.time_embedding.linear_2.weight'\n",
       "            p_unet_time_embedding_linear_2_bias: PARAMETER target='unet.time_embedding.linear_2.bias'\n",
       "            p_unet_conv_in_weight: PARAMETER target='unet.conv_in.weight'\n",
       "            p_unet_conv_in_bias: PARAMETER target='unet.conv_in.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.0.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.0.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.0.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.0.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.0.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.0.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.0.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.0.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.0.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.0.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.0.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.0.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.0.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.0.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.0.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.0.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.0.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.0.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.0.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.0.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.0.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.0.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.0.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.0.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.0.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.0.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.0.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.0.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.0.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.0.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.0.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.0.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_0_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.0.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_0_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.0.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.1.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.1.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.1.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.1.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.1.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.1.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv_shortcut_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv_shortcut_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.1.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.1.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.1.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.1.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.1.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.1.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.1.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.1.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.1.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.1.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.1.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.1.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.1.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.1.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.1.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.1.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.1.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.1.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.1.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.1.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.1.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.1.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_1_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.1.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_1_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.1.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.2.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.2.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.2.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.2.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.2.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.2.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.2.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.2.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.2.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.2.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.2.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.2.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.2.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.2.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.2.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.2.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.2.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.2.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.2.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.2.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.2.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.2.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.2.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.2.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.2.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.2.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.2.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.2.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_2_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.2.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_2_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.2.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.3.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.3.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.3.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.3.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.3.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.3.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.3.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.3.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.3.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.3.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.3.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.3.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.3.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.3.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.3.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.3.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.3.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.3.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.3.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.3.resnets.1.conv2.bias'\n",
       "            p_unet_mid_block_resnets_0_norm1_weight: PARAMETER target='unet.mid_block.resnets.0.norm1.weight'\n",
       "            p_unet_mid_block_resnets_0_norm1_bias: PARAMETER target='unet.mid_block.resnets.0.norm1.bias'\n",
       "            p_unet_mid_block_resnets_0_conv1_weight: PARAMETER target='unet.mid_block.resnets.0.conv1.weight'\n",
       "            p_unet_mid_block_resnets_0_conv1_bias: PARAMETER target='unet.mid_block.resnets.0.conv1.bias'\n",
       "            p_unet_mid_block_resnets_0_time_emb_proj_weight: PARAMETER target='unet.mid_block.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_mid_block_resnets_0_time_emb_proj_bias: PARAMETER target='unet.mid_block.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_mid_block_resnets_0_norm2_weight: PARAMETER target='unet.mid_block.resnets.0.norm2.weight'\n",
       "            p_unet_mid_block_resnets_0_norm2_bias: PARAMETER target='unet.mid_block.resnets.0.norm2.bias'\n",
       "            p_unet_mid_block_resnets_0_conv2_weight: PARAMETER target='unet.mid_block.resnets.0.conv2.weight'\n",
       "            p_unet_mid_block_resnets_0_conv2_bias: PARAMETER target='unet.mid_block.resnets.0.conv2.bias'\n",
       "            p_unet_mid_block_resnets_1_norm1_weight: PARAMETER target='unet.mid_block.resnets.1.norm1.weight'\n",
       "            p_unet_mid_block_resnets_1_norm1_bias: PARAMETER target='unet.mid_block.resnets.1.norm1.bias'\n",
       "            p_unet_mid_block_resnets_1_conv1_weight: PARAMETER target='unet.mid_block.resnets.1.conv1.weight'\n",
       "            p_unet_mid_block_resnets_1_conv1_bias: PARAMETER target='unet.mid_block.resnets.1.conv1.bias'\n",
       "            p_unet_mid_block_resnets_1_time_emb_proj_weight: PARAMETER target='unet.mid_block.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_mid_block_resnets_1_time_emb_proj_bias: PARAMETER target='unet.mid_block.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_mid_block_resnets_1_norm2_weight: PARAMETER target='unet.mid_block.resnets.1.norm2.weight'\n",
       "            p_unet_mid_block_resnets_1_norm2_bias: PARAMETER target='unet.mid_block.resnets.1.norm2.bias'\n",
       "            p_unet_mid_block_resnets_1_conv2_weight: PARAMETER target='unet.mid_block.resnets.1.conv2.weight'\n",
       "            p_unet_mid_block_resnets_1_conv2_bias: PARAMETER target='unet.mid_block.resnets.1.conv2.bias'\n",
       "            p_unet_mid_block_attentions_0_norm_weight: PARAMETER target='unet.mid_block.attentions.0.norm.weight'\n",
       "            p_unet_mid_block_attentions_0_norm_bias: PARAMETER target='unet.mid_block.attentions.0.norm.bias'\n",
       "            p_unet_mid_block_attentions_0_proj_in_weight: PARAMETER target='unet.mid_block.attentions.0.proj_in.weight'\n",
       "            p_unet_mid_block_attentions_0_proj_in_bias: PARAMETER target='unet.mid_block.attentions.0.proj_in.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_mid_block_attentions_0_proj_out_weight: PARAMETER target='unet.mid_block.attentions.0.proj_out.weight'\n",
       "            p_unet_mid_block_attentions_0_proj_out_bias: PARAMETER target='unet.mid_block.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.0.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_0_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.0.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.2.proj_out.bias'\n",
       "            p_unet_up_blocks_1_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.1.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_1_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.1.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.2.proj_out.bias'\n",
       "            p_unet_up_blocks_2_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.2.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_2_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.2.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.2.proj_out.bias'\n",
       "            p_unet_conv_norm_out_weight: PARAMETER target='unet.conv_norm_out.weight'\n",
       "            p_unet_conv_norm_out_bias: PARAMETER target='unet.conv_norm_out.bias'\n",
       "            p_unet_conv_out_weight: PARAMETER target='unet.conv_out.weight'\n",
       "            p_unet_conv_out_bias: PARAMETER target='unet.conv_out.bias'\n",
       "            p_vae_post_quant_conv_weight: PARAMETER target='vae.post_quant_conv.weight'\n",
       "            p_vae_post_quant_conv_bias: PARAMETER target='vae.post_quant_conv.bias'\n",
       "            p_vae_decoder_conv_in_weight: PARAMETER target='vae.decoder.conv_in.weight'\n",
       "            p_vae_decoder_conv_in_bias: PARAMETER target='vae.decoder.conv_in.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm1_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm1_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv1_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv1_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm2_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm2_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv2_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv2_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm1_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm1_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv1_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv1_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm2_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm2_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv2_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv2_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_group_norm_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.group_norm.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_group_norm_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.group_norm.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_q_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_q.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_q_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_q.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_k_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_k.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_k_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_k.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_v_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_v.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_v_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_v.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_out_0_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_out.0.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_out_0_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_out.0.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.0.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.0.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.1.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.1.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.2.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.2.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_conv_norm_out_weight: PARAMETER target='vae.decoder.conv_norm_out.weight'\n",
       "            p_vae_decoder_conv_norm_out_bias: PARAMETER target='vae.decoder.conv_norm_out.bias'\n",
       "            p_vae_decoder_conv_out_weight: PARAMETER target='vae.decoder.conv_out.weight'\n",
       "            p_vae_decoder_conv_out_bias: PARAMETER target='vae.decoder.conv_out.bias'\n",
       "            c_timesteps: CONSTANT_TENSOR target='timesteps'\n",
       "            c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n",
       "            b_pe_pe: BUFFER target='pe.pe' persistent=True\n",
       "            whisper_input_features_0: USER_INPUT\n",
       "            latent_inputs: USER_INPUT\n",
       "            frame_idx: USER_INPUT\n",
       "            librosa_length: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            alias: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5add71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"musetalk_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7cb8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# onnx_model = onnx.load(\"musetalk_model.onnx\")\n",
    "onnx.checker.check_model(\"musetalk_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b48df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0+cu130'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82117234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0+cu130',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"whisper_input_features_0\"<FLOAT,[1,80,3000]>,\n",
       "                %\"latent_inputs\"<FLOAT,[8,8,32,32]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"alias\"<UINT8,[8,256,256,3]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"whisper_encoder.embed_positions.weight\"<FLOAT,[1500,384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv1.weight\"<FLOAT,[384,80,3]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv2.weight\"<FLOAT,[384,384,3]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.conv2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.0.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.1.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.2.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.q_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.v_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.self_attn.out_proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.final_layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.final_layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layers.3.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layer_norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"whisper_encoder.layer_norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_1.weight\"<FLOAT,[1280,320]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_2.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.time_embedding.linear_2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.conv_in.weight\"<FLOAT,[320,8,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.conv_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv1.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.0.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv1.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.resnets.1.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.0.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.attentions.1.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.downsamplers.0.conv.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.0.downsamplers.0.conv.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv1.weight\"<FLOAT,[640,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv_shortcut.weight\"<FLOAT,[640,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.0.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv1.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.resnets.1.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.0.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.attentions.1.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.downsamplers.0.conv.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.1.downsamplers.0.conv.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv1.weight\"<FLOAT,[1280,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.attentions.1.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.downsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.2.downsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.down_blocks.3.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv1.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.mid_block.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.1.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.resnets.2.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.upsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.0.upsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.0.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv1.weight\"<FLOAT,[1280,2560,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv_shortcut.weight\"<FLOAT,[1280,2560,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.1.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv1.weight\"<FLOAT,[1280,1920,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.time_emb_proj.weight\"<FLOAT,[1280,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.time_emb_proj.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv2.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv_shortcut.weight\"<FLOAT,[1280,1920,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.resnets.2.conv_shortcut.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.0.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.1.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_in.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_in.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[10240]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_out.weight\"<FLOAT,[1280,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.attentions.2.proj_out.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.upsamplers.0.conv.weight\"<FLOAT,[1280,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.1.upsamplers.0.conv.bias\"<FLOAT,[1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv1.weight\"<FLOAT,[640,1920,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[640,1920,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv1.weight\"<FLOAT,[640,1280,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv_shortcut.weight\"<FLOAT,[640,1280,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.1.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv1.weight\"<FLOAT,[640,960,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.time_emb_proj.weight\"<FLOAT,[640,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.time_emb_proj.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv2.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv_shortcut.weight\"<FLOAT,[640,960,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.resnets.2.conv_shortcut.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.0.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.1.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_in.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_in.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[5120]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_out.weight\"<FLOAT,[640,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.attentions.2.proj_out.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.upsamplers.0.conv.weight\"<FLOAT,[640,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.2.upsamplers.0.conv.bias\"<FLOAT,[640]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv1.weight\"<FLOAT,[320,960,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv_shortcut.weight\"<FLOAT,[320,960,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.0.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv1.weight\"<FLOAT,[320,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv_shortcut.weight\"<FLOAT,[320,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.1.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv1.weight\"<FLOAT,[320,640,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.time_emb_proj.weight\"<FLOAT,[320,1280]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.time_emb_proj.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv2.weight\"<FLOAT,[320,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv_shortcut.weight\"<FLOAT,[320,640,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.resnets.2.conv_shortcut.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.0.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.1.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_in.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_in.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"<FLOAT,[2560]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_out.weight\"<FLOAT,[320,320,1,1]>{TorchTensor(...)},\n",
       "                %\"unet.up_blocks.3.attentions.2.proj_out.bias\"<FLOAT,[320]>{TorchTensor(...)},\n",
       "                %\"unet.conv_out.weight\"<FLOAT,[4,320,3,3]>{TorchTensor(...)},\n",
       "                %\"unet.conv_out.bias\"<FLOAT,[4]>{TorchTensor<FLOAT,[4]>(Parameter containing: tensor([-0.0061, -0.0004,  0.0061, -0.0182], device='cuda:0', requires_grad=True), name='unet.conv_out.bias')},\n",
       "                %\"vae.post_quant_conv.weight\"<FLOAT,[4,4,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.post_quant_conv.bias\"<FLOAT,[4]>{TorchTensor<FLOAT,[4]>(Parameter containing: tensor([ 0.0321, -0.0843, -0.2432,  0.1315], device='cuda:0', requires_grad=True), name='vae.post_quant_conv.bias')},\n",
       "                %\"vae.decoder.conv_in.weight\"<FLOAT,[512,4,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_in.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_q.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_k.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_v.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.mid_block.attentions.0.to_out.0.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.resnets.2.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.upsamplers.0.conv.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.0.upsamplers.0.conv.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.0.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.1.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv1.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv2.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.resnets.2.conv2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.upsamplers.0.conv.weight\"<FLOAT,[512,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.1.upsamplers.0.conv.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv1.weight\"<FLOAT,[256,512,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight\"<FLOAT,[256,512,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv1.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.1.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv1.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv2.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.resnets.2.conv2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.upsamplers.0.conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.2.upsamplers.0.conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv1.weight\"<FLOAT,[128,256,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight\"<FLOAT,[128,256,1,1]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv1.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.1.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv1.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv2.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.up_blocks.3.resnets.2.conv2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_out.weight\"<FLOAT,[3,128,3,3]>{TorchTensor(...)},\n",
       "                %\"vae.decoder.conv_out.bias\"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(Parameter containing: tensor([ 0.0158, -0.0203, -0.0464], device='cuda:0', requires_grad=True), name='vae.decoder.conv_out.bias')},\n",
       "                %\"pe.pe\"<FLOAT,[1,5000,384]>{TorchTensor(...)},\n",
       "                %\"val_2\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_4\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_10\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, -1,  6, 64]), name='val_10')},\n",
       "                %\"val_11\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_24\"<INT64,[4]>{Tensor<INT64,[4]>(array([   1, 1500,    6,   64]), name='val_24')},\n",
       "                %\"val_43\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1500,   64]), name='val_43')},\n",
       "                %\"val_46\"<INT64,[4]>{Tensor<INT64,[4]>(array([   1,    6,   64, 1500]), name='val_46')},\n",
       "                %\"val_48\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.35355338], dtype=float32), name='val_48')},\n",
       "                %\"val_59\"<INT64,[3]>{Tensor<INT64,[3]>(array([   1, 1500,  384]), name='val_59')},\n",
       "                %\"val_60\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_64\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_66\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_70\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_72\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_79\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_126\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_130\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_132\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_136\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_138\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_145\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_192\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_196\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_198\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_202\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_204\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_211\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_258\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_262\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_264\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_281\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_281')},\n",
       "                %\"val_285\"<INT64,[1]>{Tensor<INT64,[1]>(array([332]), name='val_285')},\n",
       "                %\"val_289\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_289')},\n",
       "                %\"zeros_like\"<FLOAT,[1,4,5,384]>{Tensor(...)},\n",
       "                %\"zeros_like_1\"<FLOAT,[1,12,5,384]>{Tensor(...)},\n",
       "                %\"val_323\"<INT64,[1]>{Tensor<INT64,[1]>(array([10]), name='val_323')},\n",
       "                %\"val_331\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_331')},\n",
       "                %\"val_334\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_334')},\n",
       "                %\"val_341\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_341')},\n",
       "                %\"val_345\"<INT64,[1]>{Tensor<INT64,[1]>(array([14]), name='val_345')},\n",
       "                %\"val_353\"<INT64,[1]>{Tensor<INT64,[1]>(array([6]), name='val_353')},\n",
       "                %\"val_357\"<INT64,[1]>{Tensor<INT64,[1]>(array([16]), name='val_357')},\n",
       "                %\"val_365\"<INT64,[1]>{Tensor<INT64,[1]>(array([8]), name='val_365')},\n",
       "                %\"val_369\"<INT64,[1]>{Tensor<INT64,[1]>(array([18]), name='val_369')},\n",
       "                %\"val_380\"<INT64,[1]>{Tensor<INT64,[1]>(array([20]), name='val_380')},\n",
       "                %\"val_391\"<INT64,[1]>{Tensor<INT64,[1]>(array([22]), name='val_391')},\n",
       "                %\"val_402\"<INT64,[1]>{Tensor<INT64,[1]>(array([24]), name='val_402')},\n",
       "                %\"val_413\"<INT64,[1]>{Tensor<INT64,[1]>(array([26]), name='val_413')},\n",
       "                %\"val_424\"<INT64,[1]>{Tensor<INT64,[1]>(array([28]), name='val_424')},\n",
       "                %\"val_435\"<INT64,[1]>{Tensor<INT64,[1]>(array([30]), name='val_435')},\n",
       "                %\"val_446\"<INT64,[1]>{Tensor<INT64,[1]>(array([32]), name='val_446')},\n",
       "                %\"val_457\"<INT64,[1]>{Tensor<INT64,[1]>(array([34]), name='val_457')},\n",
       "                %\"val_468\"<INT64,[1]>{Tensor<INT64,[1]>(array([36]), name='val_468')},\n",
       "                %\"val_479\"<INT64,[1]>{Tensor<INT64,[1]>(array([38]), name='val_479')},\n",
       "                %\"val_490\"<INT64,[1]>{Tensor<INT64,[1]>(array([40]), name='val_490')},\n",
       "                %\"val_501\"<INT64,[1]>{Tensor<INT64,[1]>(array([42]), name='val_501')},\n",
       "                %\"val_512\"<INT64,[1]>{Tensor<INT64,[1]>(array([44]), name='val_512')},\n",
       "                %\"val_523\"<INT64,[1]>{Tensor<INT64,[1]>(array([46]), name='val_523')},\n",
       "                %\"val_534\"<INT64,[1]>{Tensor<INT64,[1]>(array([48]), name='val_534')},\n",
       "                %\"val_545\"<INT64,[1]>{Tensor<INT64,[1]>(array([50]), name='val_545')},\n",
       "                %\"val_556\"<INT64,[1]>{Tensor<INT64,[1]>(array([52]), name='val_556')},\n",
       "                %\"val_567\"<INT64,[1]>{Tensor<INT64,[1]>(array([54]), name='val_567')},\n",
       "                %\"val_578\"<INT64,[1]>{Tensor<INT64,[1]>(array([56]), name='val_578')},\n",
       "                %\"val_589\"<INT64,[1]>{Tensor<INT64,[1]>(array([58]), name='val_589')},\n",
       "                %\"val_600\"<INT64,[1]>{Tensor<INT64,[1]>(array([60]), name='val_600')},\n",
       "                %\"val_611\"<INT64,[1]>{Tensor<INT64,[1]>(array([62]), name='val_611')},\n",
       "                %\"val_622\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_622')},\n",
       "                %\"val_633\"<INT64,[1]>{Tensor<INT64,[1]>(array([66]), name='val_633')},\n",
       "                %\"val_644\"<INT64,[1]>{Tensor<INT64,[1]>(array([68]), name='val_644')},\n",
       "                %\"val_655\"<INT64,[1]>{Tensor<INT64,[1]>(array([70]), name='val_655')},\n",
       "                %\"val_666\"<INT64,[1]>{Tensor<INT64,[1]>(array([72]), name='val_666')},\n",
       "                %\"val_677\"<INT64,[1]>{Tensor<INT64,[1]>(array([74]), name='val_677')},\n",
       "                %\"val_688\"<INT64,[1]>{Tensor<INT64,[1]>(array([76]), name='val_688')},\n",
       "                %\"val_699\"<INT64,[1]>{Tensor<INT64,[1]>(array([78]), name='val_699')},\n",
       "                %\"val_710\"<INT64,[1]>{Tensor<INT64,[1]>(array([80]), name='val_710')},\n",
       "                %\"val_721\"<INT64,[1]>{Tensor<INT64,[1]>(array([82]), name='val_721')},\n",
       "                %\"val_732\"<INT64,[1]>{Tensor<INT64,[1]>(array([84]), name='val_732')},\n",
       "                %\"val_743\"<INT64,[1]>{Tensor<INT64,[1]>(array([86]), name='val_743')},\n",
       "                %\"val_754\"<INT64,[1]>{Tensor<INT64,[1]>(array([88]), name='val_754')},\n",
       "                %\"val_765\"<INT64,[1]>{Tensor<INT64,[1]>(array([90]), name='val_765')},\n",
       "                %\"val_776\"<INT64,[1]>{Tensor<INT64,[1]>(array([92]), name='val_776')},\n",
       "                %\"val_787\"<INT64,[1]>{Tensor<INT64,[1]>(array([94]), name='val_787')},\n",
       "                %\"val_798\"<INT64,[1]>{Tensor<INT64,[1]>(array([96]), name='val_798')},\n",
       "                %\"val_809\"<INT64,[1]>{Tensor<INT64,[1]>(array([98]), name='val_809')},\n",
       "                %\"val_820\"<INT64,[1]>{Tensor<INT64,[1]>(array([100]), name='val_820')},\n",
       "                %\"val_831\"<INT64,[1]>{Tensor<INT64,[1]>(array([102]), name='val_831')},\n",
       "                %\"val_842\"<INT64,[1]>{Tensor<INT64,[1]>(array([104]), name='val_842')},\n",
       "                %\"val_853\"<INT64,[1]>{Tensor<INT64,[1]>(array([106]), name='val_853')},\n",
       "                %\"val_864\"<INT64,[1]>{Tensor<INT64,[1]>(array([108]), name='val_864')},\n",
       "                %\"val_875\"<INT64,[1]>{Tensor<INT64,[1]>(array([110]), name='val_875')},\n",
       "                %\"val_886\"<INT64,[1]>{Tensor<INT64,[1]>(array([112]), name='val_886')},\n",
       "                %\"val_897\"<INT64,[1]>{Tensor<INT64,[1]>(array([114]), name='val_897')},\n",
       "                %\"val_908\"<INT64,[1]>{Tensor<INT64,[1]>(array([116]), name='val_908')},\n",
       "                %\"val_919\"<INT64,[1]>{Tensor<INT64,[1]>(array([118]), name='val_919')},\n",
       "                %\"val_930\"<INT64,[1]>{Tensor<INT64,[1]>(array([120]), name='val_930')},\n",
       "                %\"val_941\"<INT64,[1]>{Tensor<INT64,[1]>(array([122]), name='val_941')},\n",
       "                %\"val_952\"<INT64,[1]>{Tensor<INT64,[1]>(array([124]), name='val_952')},\n",
       "                %\"val_963\"<INT64,[1]>{Tensor<INT64,[1]>(array([126]), name='val_963')},\n",
       "                %\"val_974\"<INT64,[1]>{Tensor<INT64,[1]>(array([128]), name='val_974')},\n",
       "                %\"val_985\"<INT64,[1]>{Tensor<INT64,[1]>(array([130]), name='val_985')},\n",
       "                %\"val_996\"<INT64,[1]>{Tensor<INT64,[1]>(array([132]), name='val_996')},\n",
       "                %\"val_1007\"<INT64,[1]>{Tensor<INT64,[1]>(array([134]), name='val_1007')},\n",
       "                %\"val_1018\"<INT64,[1]>{Tensor<INT64,[1]>(array([136]), name='val_1018')},\n",
       "                %\"val_1029\"<INT64,[1]>{Tensor<INT64,[1]>(array([138]), name='val_1029')},\n",
       "                %\"val_1040\"<INT64,[1]>{Tensor<INT64,[1]>(array([140]), name='val_1040')},\n",
       "                %\"val_1051\"<INT64,[1]>{Tensor<INT64,[1]>(array([142]), name='val_1051')},\n",
       "                %\"val_1062\"<INT64,[1]>{Tensor<INT64,[1]>(array([144]), name='val_1062')},\n",
       "                %\"val_1073\"<INT64,[1]>{Tensor<INT64,[1]>(array([146]), name='val_1073')},\n",
       "                %\"val_1084\"<INT64,[1]>{Tensor<INT64,[1]>(array([148]), name='val_1084')},\n",
       "                %\"val_1095\"<INT64,[1]>{Tensor<INT64,[1]>(array([150]), name='val_1095')},\n",
       "                %\"val_1106\"<INT64,[1]>{Tensor<INT64,[1]>(array([152]), name='val_1106')},\n",
       "                %\"val_1117\"<INT64,[1]>{Tensor<INT64,[1]>(array([154]), name='val_1117')},\n",
       "                %\"val_1128\"<INT64,[1]>{Tensor<INT64,[1]>(array([156]), name='val_1128')},\n",
       "                %\"val_1139\"<INT64,[1]>{Tensor<INT64,[1]>(array([158]), name='val_1139')},\n",
       "                %\"val_1150\"<INT64,[1]>{Tensor<INT64,[1]>(array([160]), name='val_1150')},\n",
       "                %\"val_1161\"<INT64,[1]>{Tensor<INT64,[1]>(array([162]), name='val_1161')},\n",
       "                %\"val_1172\"<INT64,[1]>{Tensor<INT64,[1]>(array([164]), name='val_1172')},\n",
       "                %\"val_1183\"<INT64,[1]>{Tensor<INT64,[1]>(array([166]), name='val_1183')},\n",
       "                %\"val_1194\"<INT64,[1]>{Tensor<INT64,[1]>(array([168]), name='val_1194')},\n",
       "                %\"val_1205\"<INT64,[1]>{Tensor<INT64,[1]>(array([170]), name='val_1205')},\n",
       "                %\"val_1216\"<INT64,[1]>{Tensor<INT64,[1]>(array([172]), name='val_1216')},\n",
       "                %\"val_1227\"<INT64,[1]>{Tensor<INT64,[1]>(array([174]), name='val_1227')},\n",
       "                %\"val_1238\"<INT64,[1]>{Tensor<INT64,[1]>(array([176]), name='val_1238')},\n",
       "                %\"val_1249\"<INT64,[1]>{Tensor<INT64,[1]>(array([178]), name='val_1249')},\n",
       "                %\"val_1260\"<INT64,[1]>{Tensor<INT64,[1]>(array([180]), name='val_1260')},\n",
       "                %\"val_1271\"<INT64,[1]>{Tensor<INT64,[1]>(array([182]), name='val_1271')},\n",
       "                %\"val_1282\"<INT64,[1]>{Tensor<INT64,[1]>(array([184]), name='val_1282')},\n",
       "                %\"val_1293\"<INT64,[1]>{Tensor<INT64,[1]>(array([186]), name='val_1293')},\n",
       "                %\"val_1304\"<INT64,[1]>{Tensor<INT64,[1]>(array([188]), name='val_1304')},\n",
       "                %\"val_1315\"<INT64,[1]>{Tensor<INT64,[1]>(array([190]), name='val_1315')},\n",
       "                %\"val_1326\"<INT64,[1]>{Tensor<INT64,[1]>(array([192]), name='val_1326')},\n",
       "                %\"val_1337\"<INT64,[1]>{Tensor<INT64,[1]>(array([194]), name='val_1337')},\n",
       "                %\"val_1348\"<INT64,[1]>{Tensor<INT64,[1]>(array([196]), name='val_1348')},\n",
       "                %\"val_1359\"<INT64,[1]>{Tensor<INT64,[1]>(array([198]), name='val_1359')},\n",
       "                %\"val_1370\"<INT64,[1]>{Tensor<INT64,[1]>(array([200]), name='val_1370')},\n",
       "                %\"val_1381\"<INT64,[1]>{Tensor<INT64,[1]>(array([202]), name='val_1381')},\n",
       "                %\"val_1392\"<INT64,[1]>{Tensor<INT64,[1]>(array([204]), name='val_1392')},\n",
       "                %\"val_1403\"<INT64,[1]>{Tensor<INT64,[1]>(array([206]), name='val_1403')},\n",
       "                %\"val_1414\"<INT64,[1]>{Tensor<INT64,[1]>(array([208]), name='val_1414')},\n",
       "                %\"val_1425\"<INT64,[1]>{Tensor<INT64,[1]>(array([210]), name='val_1425')},\n",
       "                %\"val_1436\"<INT64,[1]>{Tensor<INT64,[1]>(array([212]), name='val_1436')},\n",
       "                %\"val_1447\"<INT64,[1]>{Tensor<INT64,[1]>(array([214]), name='val_1447')},\n",
       "                %\"val_1458\"<INT64,[1]>{Tensor<INT64,[1]>(array([216]), name='val_1458')},\n",
       "                %\"val_1469\"<INT64,[1]>{Tensor<INT64,[1]>(array([218]), name='val_1469')},\n",
       "                %\"val_1480\"<INT64,[1]>{Tensor<INT64,[1]>(array([220]), name='val_1480')},\n",
       "                %\"val_1491\"<INT64,[1]>{Tensor<INT64,[1]>(array([222]), name='val_1491')},\n",
       "                %\"val_1502\"<INT64,[1]>{Tensor<INT64,[1]>(array([224]), name='val_1502')},\n",
       "                %\"val_1513\"<INT64,[1]>{Tensor<INT64,[1]>(array([226]), name='val_1513')},\n",
       "                %\"val_1524\"<INT64,[1]>{Tensor<INT64,[1]>(array([228]), name='val_1524')},\n",
       "                %\"val_1535\"<INT64,[1]>{Tensor<INT64,[1]>(array([230]), name='val_1535')},\n",
       "                %\"val_1546\"<INT64,[1]>{Tensor<INT64,[1]>(array([232]), name='val_1546')},\n",
       "                %\"val_1557\"<INT64,[1]>{Tensor<INT64,[1]>(array([234]), name='val_1557')},\n",
       "                %\"val_1568\"<INT64,[1]>{Tensor<INT64,[1]>(array([236]), name='val_1568')},\n",
       "                %\"val_1579\"<INT64,[1]>{Tensor<INT64,[1]>(array([238]), name='val_1579')},\n",
       "                %\"val_1590\"<INT64,[1]>{Tensor<INT64,[1]>(array([240]), name='val_1590')},\n",
       "                %\"val_1601\"<INT64,[1]>{Tensor<INT64,[1]>(array([242]), name='val_1601')},\n",
       "                %\"val_1612\"<INT64,[1]>{Tensor<INT64,[1]>(array([244]), name='val_1612')},\n",
       "                %\"val_1623\"<INT64,[1]>{Tensor<INT64,[1]>(array([246]), name='val_1623')},\n",
       "                %\"val_1634\"<INT64,[1]>{Tensor<INT64,[1]>(array([248]), name='val_1634')},\n",
       "                %\"val_1645\"<INT64,[1]>{Tensor<INT64,[1]>(array([250]), name='val_1645')},\n",
       "                %\"val_1656\"<INT64,[1]>{Tensor<INT64,[1]>(array([252]), name='val_1656')},\n",
       "                %\"val_1667\"<INT64,[1]>{Tensor<INT64,[1]>(array([254]), name='val_1667')},\n",
       "                %\"val_1678\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_1678')},\n",
       "                %\"val_1689\"<INT64,[1]>{Tensor<INT64,[1]>(array([258]), name='val_1689')},\n",
       "                %\"val_1700\"<INT64,[1]>{Tensor<INT64,[1]>(array([260]), name='val_1700')},\n",
       "                %\"val_1711\"<INT64,[1]>{Tensor<INT64,[1]>(array([262]), name='val_1711')},\n",
       "                %\"val_1722\"<INT64,[1]>{Tensor<INT64,[1]>(array([264]), name='val_1722')},\n",
       "                %\"val_1733\"<INT64,[1]>{Tensor<INT64,[1]>(array([266]), name='val_1733')},\n",
       "                %\"val_1744\"<INT64,[1]>{Tensor<INT64,[1]>(array([268]), name='val_1744')},\n",
       "                %\"val_1755\"<INT64,[1]>{Tensor<INT64,[1]>(array([270]), name='val_1755')},\n",
       "                %\"val_1766\"<INT64,[1]>{Tensor<INT64,[1]>(array([272]), name='val_1766')},\n",
       "                %\"val_1777\"<INT64,[1]>{Tensor<INT64,[1]>(array([274]), name='val_1777')},\n",
       "                %\"val_1788\"<INT64,[1]>{Tensor<INT64,[1]>(array([276]), name='val_1788')},\n",
       "                %\"val_1799\"<INT64,[1]>{Tensor<INT64,[1]>(array([278]), name='val_1799')},\n",
       "                %\"val_1810\"<INT64,[1]>{Tensor<INT64,[1]>(array([280]), name='val_1810')},\n",
       "                %\"val_1821\"<INT64,[1]>{Tensor<INT64,[1]>(array([282]), name='val_1821')},\n",
       "                %\"val_1832\"<INT64,[1]>{Tensor<INT64,[1]>(array([284]), name='val_1832')},\n",
       "                %\"val_1843\"<INT64,[1]>{Tensor<INT64,[1]>(array([286]), name='val_1843')},\n",
       "                %\"val_1854\"<INT64,[1]>{Tensor<INT64,[1]>(array([288]), name='val_1854')},\n",
       "                %\"val_1865\"<INT64,[1]>{Tensor<INT64,[1]>(array([290]), name='val_1865')},\n",
       "                %\"val_1876\"<INT64,[1]>{Tensor<INT64,[1]>(array([292]), name='val_1876')},\n",
       "                %\"val_1887\"<INT64,[1]>{Tensor<INT64,[1]>(array([294]), name='val_1887')},\n",
       "                %\"val_1898\"<INT64,[1]>{Tensor<INT64,[1]>(array([296]), name='val_1898')},\n",
       "                %\"val_1909\"<INT64,[1]>{Tensor<INT64,[1]>(array([298]), name='val_1909')},\n",
       "                %\"val_1920\"<INT64,[1]>{Tensor<INT64,[1]>(array([300]), name='val_1920')},\n",
       "                %\"val_1931\"<INT64,[1]>{Tensor<INT64,[1]>(array([302]), name='val_1931')},\n",
       "                %\"val_1942\"<INT64,[1]>{Tensor<INT64,[1]>(array([304]), name='val_1942')},\n",
       "                %\"val_1953\"<INT64,[1]>{Tensor<INT64,[1]>(array([306]), name='val_1953')},\n",
       "                %\"val_1964\"<INT64,[1]>{Tensor<INT64,[1]>(array([308]), name='val_1964')},\n",
       "                %\"val_1975\"<INT64,[1]>{Tensor<INT64,[1]>(array([310]), name='val_1975')},\n",
       "                %\"val_1986\"<INT64,[1]>{Tensor<INT64,[1]>(array([312]), name='val_1986')},\n",
       "                %\"val_1997\"<INT64,[1]>{Tensor<INT64,[1]>(array([314]), name='val_1997')},\n",
       "                %\"val_2008\"<INT64,[1]>{Tensor<INT64,[1]>(array([316]), name='val_2008')},\n",
       "                %\"val_2019\"<INT64,[1]>{Tensor<INT64,[1]>(array([318]), name='val_2019')},\n",
       "                %\"val_2030\"<INT64,[1]>{Tensor<INT64,[1]>(array([320]), name='val_2030')},\n",
       "                %\"val_2041\"<INT64,[1]>{Tensor<INT64,[1]>(array([322]), name='val_2041')},\n",
       "                %\"val_2052\"<INT64,[1]>{Tensor<INT64,[1]>(array([324]), name='val_2052')},\n",
       "                %\"val_2063\"<INT64,[1]>{Tensor<INT64,[1]>(array([326]), name='val_2063')},\n",
       "                %\"val_2074\"<INT64,[1]>{Tensor<INT64,[1]>(array([328]), name='val_2074')},\n",
       "                %\"val_2085\"<INT64,[1]>{Tensor<INT64,[1]>(array([330]), name='val_2085')},\n",
       "                %\"val_2106\"<INT64,[1]>{Tensor<INT64,[1]>(array([334]), name='val_2106')},\n",
       "                %\"val_2117\"<INT64,[1]>{Tensor<INT64,[1]>(array([336]), name='val_2117')},\n",
       "                %\"val_2128\"<INT64,[1]>{Tensor<INT64,[1]>(array([338]), name='val_2128')},\n",
       "                %\"val_2139\"<INT64,[1]>{Tensor<INT64,[1]>(array([340]), name='val_2139')},\n",
       "                %\"val_2148\"<INT64,[3]>{Tensor<INT64,[3]>(array([166,  50, 384]), name='val_2148')},\n",
       "                %\"cat_4\"<FLOAT,[8,320]>{Tensor(...)},\n",
       "                %\"val_2203\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 0, 32, -1]), name='val_2203')},\n",
       "                %\"val_2207\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"val_2210\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"val_2219\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2221\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2245\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2247\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2269\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2271\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2276\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8, 1024,  320]), name='val_2276')},\n",
       "                %\"val_2279\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2280\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2281\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2287\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8, -1,  8, 40]), name='val_2287')},\n",
       "                %\"val_2316\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1024,   40]), name='val_2316')},\n",
       "                %\"val_2319\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    8,   40, 1024]), name='val_2319')},\n",
       "                %\"val_2321\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.39763537], dtype=float32), name='val_2321')},\n",
       "                %\"val_2332\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 320]), name='val_2332')},\n",
       "                %\"val_2333\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2337\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2338\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2339\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2374\"<INT64,[3]>{Tensor<INT64,[3]>(array([-1, 50, 40]), name='val_2374')},\n",
       "                %\"val_2377\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8,  8, 40, 50]), name='val_2377')},\n",
       "                %\"val_2391\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2395\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_2398\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_2405\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  32,  32, 320]), name='val_2405')},\n",
       "                %\"val_2425\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2427\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2449\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2451\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2472\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2474\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2482\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2483\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2484\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2536\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2540\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2541\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2542\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_2594\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_2598\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_2600\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_2627\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2629\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_2651\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2653\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2674\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2676\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2681\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8, 256, 640]), name='val_2681')},\n",
       "                %\"val_2684\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2685\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2686\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2692\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8, -1,  8, 80]), name='val_2692')},\n",
       "                %\"val_2721\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 256,  80]), name='val_2721')},\n",
       "                %\"val_2724\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8,  80, 256]), name='val_2724')},\n",
       "                %\"val_2726\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.33437014], dtype=float32), name='val_2726')},\n",
       "                %\"val_2737\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 640]), name='val_2737')},\n",
       "                %\"val_2738\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2742\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2743\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2744\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2779\"<INT64,[3]>{Tensor<INT64,[3]>(array([-1, 50, 80]), name='val_2779')},\n",
       "                %\"val_2782\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 8,  8, 80, 50]), name='val_2782')},\n",
       "                %\"val_2796\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2800\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_2803\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_2810\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  16,  16, 640]), name='val_2810')},\n",
       "                %\"val_2830\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2832\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2854\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2856\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2877\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2879\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_2887\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2888\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2889\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2941\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2945\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_2946\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2947\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_2999\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_3003\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_3005\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_3032\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_3034\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_3056\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3058\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3079\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3081\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3086\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   64, 1280]), name='val_3086')},\n",
       "                %\"val_3089\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3090\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3091\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3097\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  -1,   8, 160]), name='val_3097')},\n",
       "                %\"val_3126\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  64, 160]), name='val_3126')},\n",
       "                %\"val_3129\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  64]), name='val_3129')},\n",
       "                %\"val_3131\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.28117067], dtype=float32), name='val_3131')},\n",
       "                %\"val_3142\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   -1, 1280]), name='val_3142')},\n",
       "                %\"val_3143\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3147\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3148\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3149\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3184\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  50, 160]), name='val_3184')},\n",
       "                %\"val_3187\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  50]), name='val_3187')},\n",
       "                %\"val_3201\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3205\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3208\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3215\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    8,    8, 1280]), name='val_3215')},\n",
       "                %\"val_3235\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3237\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3259\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3261\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3282\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3284\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3292\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3293\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3294\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3346\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3350\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3351\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3352\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3404\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3408\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3410\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3437\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3439\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3461\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3463\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3484\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3486\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3508\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3510\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3531\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3533\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3555\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3557\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3578\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3580\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3585\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,   16, 1280]), name='val_3585')},\n",
       "                %\"val_3588\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3589\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3590\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3625\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  16, 160]), name='val_3625')},\n",
       "                %\"val_3628\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   8, 160,  16]), name='val_3628')},\n",
       "                %\"val_3642\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3646\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3647\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3648\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_3700\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3704\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_3706\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_3713\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    4,    4, 1280]), name='val_3713')},\n",
       "                %\"val_3733\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3735\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3757\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3759\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3780\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3782\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3804\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3806\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3827\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3829\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3851\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3853\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3874\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3876\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3898\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3900\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3922\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3924\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_3946\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3948\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3969\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3971\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_3979\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3980\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_3981\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4033\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4037\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4038\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4039\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4091\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4095\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4097\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4124\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_4126\"<FLOAT,[2560,1,1]>{Tensor(...)},\n",
       "                %\"val_4148\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4150\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4171\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4173\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4181\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4182\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4183\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4235\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4239\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4240\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4241\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4293\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4297\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4299\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4326\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4328\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4350\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4352\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4373\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4375\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4383\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4384\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4385\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4437\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4441\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4442\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4443\"<FLOAT,[384,1280]>{Tensor(...)},\n",
       "                %\"val_4495\"<FLOAT,[1280,1280]>{Tensor(...)},\n",
       "                %\"val_4499\"<FLOAT,[1280,10240]>{Tensor(...)},\n",
       "                %\"val_4501\"<FLOAT,[5120,1280]>{Tensor(...)},\n",
       "                %\"val_4529\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4531\"<FLOAT,[1920,1,1]>{Tensor(...)},\n",
       "                %\"val_4553\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4555\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4576\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4578\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4586\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4587\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4588\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4640\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4644\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4645\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4646\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4698\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4702\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_4704\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_4731\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4733\"<FLOAT,[1280,1,1]>{Tensor(...)},\n",
       "                %\"val_4755\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4757\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4778\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4780\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4788\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4789\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4790\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4842\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4846\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4847\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4848\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_4900\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4904\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_4906\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_4933\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_4935\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_4957\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4959\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4980\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4982\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_4990\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4991\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_4992\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5044\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5048\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5049\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_5050\"<FLOAT,[384,640]>{Tensor(...)},\n",
       "                %\"val_5102\"<FLOAT,[640,640]>{Tensor(...)},\n",
       "                %\"val_5106\"<FLOAT,[640,5120]>{Tensor(...)},\n",
       "                %\"val_5108\"<FLOAT,[2560,640]>{Tensor(...)},\n",
       "                %\"val_5136\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_5138\"<FLOAT,[960,1,1]>{Tensor(...)},\n",
       "                %\"val_5160\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5162\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5183\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5185\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5193\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5194\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5195\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5247\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5251\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5252\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5253\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5305\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5309\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5311\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5338\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5340\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5362\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5364\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5385\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5387\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5395\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5396\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5397\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5449\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5453\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5454\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5455\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5507\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5511\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5513\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5540\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5542\"<FLOAT,[640,1,1]>{Tensor(...)},\n",
       "                %\"val_5564\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5566\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5587\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5589\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5597\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5598\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5599\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5651\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5655\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5656\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5657\"<FLOAT,[384,320]>{Tensor(...)},\n",
       "                %\"val_5709\"<FLOAT,[320,320]>{Tensor(...)},\n",
       "                %\"val_5713\"<FLOAT,[320,2560]>{Tensor(...)},\n",
       "                %\"val_5715\"<FLOAT,[1280,320]>{Tensor(...)},\n",
       "                %\"val_5742\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5744\"<FLOAT,[320,1,1]>{Tensor(...)},\n",
       "                %\"val_5766\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5768\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5789\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5791\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5797\"<INT64,[3]>{Tensor<INT64,[3]>(array([   8,  512, 1024]), name='val_5797')},\n",
       "                %\"val_5818\"<FLOAT,[512,1]>{Tensor(...)},\n",
       "                %\"val_5820\"<FLOAT,[512,1]>{Tensor(...)},\n",
       "                %\"val_5821\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5823\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5825\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5832\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,  -1,   1, 512]), name='val_5832')},\n",
       "                %\"val_5861\"<INT64,[3]>{Tensor<INT64,[3]>(array([  -1, 1024,  512]), name='val_5861')},\n",
       "                %\"val_5864\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8,    1,  512, 1024]), name='val_5864')},\n",
       "                %\"val_5866\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.2102241], dtype=float32), name='val_5866')},\n",
       "                %\"val_5877\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,  -1, 512]), name='val_5877')},\n",
       "                %\"val_5878\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_5885\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512,  32,  32]), name='val_5885')},\n",
       "                %\"val_5905\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5907\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5928\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5930\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5951\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5953\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5974\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5976\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5997\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_5999\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6020\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6022\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6043\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6045\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6066\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6068\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6090\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6092\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6113\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6115\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6136\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6138\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6159\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6161\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6182\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6184\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6205\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6207\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6229\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6231\"<FLOAT,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_6252\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6254\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6275\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6277\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6298\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6300\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6321\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6323\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6344\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6346\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6368\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6370\"<FLOAT,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_6391\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6393\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6414\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6416\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6437\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6439\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6460\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6462\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6483\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6485\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6506\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_6508\"<FLOAT,[128,1,1]>{Tensor(...)},\n",
       "                %\"scalar_tensor_default_7\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(2., dtype=float32), name='scalar_tensor_default_7')},\n",
       "                %\"clone_126\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(255., dtype=float32), name='clone_126')},\n",
       "                %\"add_133_min\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0., dtype=float32), name='add_133_min')},\n",
       "                %\"add_133_max\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='add_133_max')},\n",
       "                %\"val_2212\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 320,  32,  32]), name='val_2212')},\n",
       "                %\"val_6521\"<INT64,[2]>{Tensor<INT64,[2]>(array([2, 3]), name='val_6521')},\n",
       "                %\"val_2620\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 320,  16,  16]), name='val_2620')},\n",
       "                %\"val_2644\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,  16,  16]), name='val_2644')},\n",
       "                %\"val_3025\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,   8,   8]), name='val_3025')},\n",
       "                %\"val_3049\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,    8,    8]), name='val_3049')},\n",
       "                %\"val_3430\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,    4,    4]), name='val_3430')},\n",
       "                %\"val_3773\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 2560,    4,    4]), name='val_3773')},\n",
       "                %\"val_3902\"<FLOAT,[4]>{Tensor<FLOAT,[4]>(array([1., 1., 2., 2.], dtype=float32), name='val_3902')},\n",
       "                %\"val_3915\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 2560,    8,    8]), name='val_3915')},\n",
       "                %\"val_4319\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1920,    8,    8]), name='val_4319')},\n",
       "                %\"val_4522\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1920,   16,   16]), name='val_4522')},\n",
       "                %\"val_4724\"<INT64,[4]>{Tensor<INT64,[4]>(array([   8, 1280,   16,   16]), name='val_4724')},\n",
       "                %\"val_4926\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 960,  16,  16]), name='val_4926')},\n",
       "                %\"val_5129\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 960,  32,  32]), name='val_5129')},\n",
       "                %\"val_5331\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 640,  32,  32]), name='val_5331')},\n",
       "                %\"val_5746\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(5.4899807, dtype=float32), name='val_5746')},\n",
       "                %\"val_6083\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512,  64,  64]), name='val_6083')},\n",
       "                %\"val_6222\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 512, 128, 128]), name='val_6222')},\n",
       "                %\"val_6245\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 256, 128, 128]), name='val_6245')},\n",
       "                %\"val_6361\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 256, 256, 256]), name='val_6361')},\n",
       "                %\"val_6384\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8, 128, 256, 256]), name='val_6384')},\n",
       "                %\"val_6510\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name='val_6510')},\n",
       "                %\"val_6518\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_6518')},\n",
       "                %\"val_6519\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_6519')}\n",
       "            ),\n",
       "        ) {\n",
       "               0 |  # node_conv1d\n",
       "                    %\"conv1d\"<FLOAT,[1,384,3000]>  ::Conv(%\"whisper_input_features_0\", %\"whisper_encoder.conv1.weight\"{...}, %\"whisper_encoder.conv1.bias\"{...}) {group=1, pads=(1, 1), auto_pad='NOTSET', strides=(1,), dilations=(1,)}\n",
       "               1 |  # node_gelu\n",
       "                    %\"gelu\"<FLOAT,[1,384,3000]>  ::Gelu(%\"conv1d\") {approximate='none'}\n",
       "               2 |  # node_conv1d_1\n",
       "                    %\"conv1d_1\"<FLOAT,[1,384,1500]>  ::Conv(%\"gelu\", %\"whisper_encoder.conv2.weight\"{...}, %\"whisper_encoder.conv2.bias\"{...}) {group=1, pads=(1, 1), auto_pad='NOTSET', strides=(2,), dilations=(1,)}\n",
       "               3 |  # node_gelu_1\n",
       "                    %\"gelu_1\"<FLOAT,[1,384,1500]>  ::Gelu(%\"conv1d_1\") {approximate='none'}\n",
       "               4 |  # node_permute\n",
       "                    %\"permute\"<FLOAT,[1,1500,384]>  ::Transpose(%\"gelu_1\") {perm=(0, 2, 1)}\n",
       "               5 |  # node_add\n",
       "                    %\"add\"<FLOAT,[1,1500,384]>  ::Add(%\"permute\", %\"whisper_encoder.embed_positions.weight\"{...})\n",
       "               6 |  # node_layer_norm\n",
       "                    %\"layer_norm\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add\", %\"whisper_encoder.layers.0.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.0.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "               7 |  # node_MatMul_1\n",
       "                    %\"val_3\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_2\"{...})\n",
       "               8 |  # node_linear\n",
       "                    %\"linear\"<FLOAT,[1,1500,384]>  ::Add(%\"val_3\", %\"whisper_encoder.layers.0.self_attn.q_proj.bias\"{...})\n",
       "               9 |  # node_linear_1\n",
       "                    %\"linear_1\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_4\"{...})\n",
       "              10 |  # node_view\n",
       "                    %\"view\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_1\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              11 |  # node_transpose\n",
       "                    %\"transpose\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view\") {perm=(0, 2, 1, 3)}\n",
       "              12 |  # node_MatMul_10\n",
       "                    %\"val_12\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm\", %\"val_11\"{...})\n",
       "              13 |  # node_linear_2\n",
       "                    %\"linear_2\"<FLOAT,[1,1500,384]>  ::Add(%\"val_12\", %\"whisper_encoder.layers.0.self_attn.v_proj.bias\"{...})\n",
       "              14 |  # node_view_1\n",
       "                    %\"view_1\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_2\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              15 |  # node_transpose_1\n",
       "                    %\"transpose_1\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_1\") {perm=(0, 2, 1, 3)}\n",
       "              16 |  # node_view_2\n",
       "                    %\"view_2\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              17 |  # node_transpose_2\n",
       "                    %\"transpose_2\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
       "              18 |  # node_Reshape_42\n",
       "                    %\"val_44\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              19 |  # node_Transpose_43\n",
       "                    %\"val_45\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_44\") {perm=(0, 2, 1)}\n",
       "              20 |  # node_Reshape_45\n",
       "                    %\"val_47\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_45\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              21 |  # node_Mul_47\n",
       "                    %\"val_49\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_2\", %\"val_48\"{[0.3535533845424652]})\n",
       "              22 |  # node_Mul_50\n",
       "                    %\"val_52\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_47\", %\"val_48\"{[0.3535533845424652]})\n",
       "              23 |  # node_MatMul_51\n",
       "                    %\"val_53\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_49\", %\"val_52\")\n",
       "              24 |  # node_Softmax_52\n",
       "                    %\"val_54\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_53\") {axis=-1}\n",
       "              25 |  # node_scaled_dot_product_attention\n",
       "                    %\"scaled_dot_product_attention\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_54\", %\"transpose_1\")\n",
       "              26 |  # node_transpose_3\n",
       "                    %\"transpose_3\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention\") {perm=(0, 2, 1, 3)}\n",
       "              27 |  # node_view_3\n",
       "                    %\"view_3\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_3\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              28 |  # node_MatMul_59\n",
       "                    %\"val_61\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_3\", %\"val_60\"{...})\n",
       "              29 |  # node_linear_3\n",
       "                    %\"linear_3\"<FLOAT,[1,1500,384]>  ::Add(%\"val_61\", %\"whisper_encoder.layers.0.self_attn.out_proj.bias\"{...})\n",
       "              30 |  # node_add_1\n",
       "                    %\"add_1\"<FLOAT,[1,1500,384]>  ::Add(%\"add\", %\"linear_3\")\n",
       "              31 |  # node_layer_norm_1\n",
       "                    %\"layer_norm_1\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1\", %\"whisper_encoder.layers.0.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.0.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              32 |  # node_MatMul_61\n",
       "                    %\"val_65\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_1\", %\"val_64\"{...})\n",
       "              33 |  # node_linear_4\n",
       "                    %\"linear_4\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_65\", %\"whisper_encoder.layers.0.fc1.bias\"{...})\n",
       "              34 |  # node_gelu_2\n",
       "                    %\"gelu_2\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_4\") {approximate='none'}\n",
       "              35 |  # node_MatMul_63\n",
       "                    %\"val_67\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_2\", %\"val_66\"{...})\n",
       "              36 |  # node_linear_5\n",
       "                    %\"linear_5\"<FLOAT,[1,1500,384]>  ::Add(%\"val_67\", %\"whisper_encoder.layers.0.fc2.bias\"{...})\n",
       "              37 |  # node_add_2\n",
       "                    %\"add_2\"<FLOAT,[1,1500,384]>  ::Add(%\"add_1\", %\"linear_5\")\n",
       "              38 |  # node_layer_norm_2\n",
       "                    %\"layer_norm_2\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2\", %\"whisper_encoder.layers.1.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.1.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              39 |  # node_MatMul_65\n",
       "                    %\"val_71\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_70\"{...})\n",
       "              40 |  # node_linear_6\n",
       "                    %\"linear_6\"<FLOAT,[1,1500,384]>  ::Add(%\"val_71\", %\"whisper_encoder.layers.1.self_attn.q_proj.bias\"{...})\n",
       "              41 |  # node_linear_7\n",
       "                    %\"linear_7\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_72\"{...})\n",
       "              42 |  # node_view_4\n",
       "                    %\"view_4\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_7\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              43 |  # node_transpose_4\n",
       "                    %\"transpose_4\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
       "              44 |  # node_MatMul_74\n",
       "                    %\"val_80\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_2\", %\"val_79\"{...})\n",
       "              45 |  # node_linear_8\n",
       "                    %\"linear_8\"<FLOAT,[1,1500,384]>  ::Add(%\"val_80\", %\"whisper_encoder.layers.1.self_attn.v_proj.bias\"{...})\n",
       "              46 |  # node_view_5\n",
       "                    %\"view_5\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_8\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              47 |  # node_transpose_5\n",
       "                    %\"transpose_5\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_5\") {perm=(0, 2, 1, 3)}\n",
       "              48 |  # node_view_6\n",
       "                    %\"view_6\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_6\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              49 |  # node_transpose_6\n",
       "                    %\"transpose_6\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_6\") {perm=(0, 2, 1, 3)}\n",
       "              50 |  # node_Reshape_104\n",
       "                    %\"val_110\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_4\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              51 |  # node_Transpose_105\n",
       "                    %\"val_111\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_110\") {perm=(0, 2, 1)}\n",
       "              52 |  # node_Reshape_107\n",
       "                    %\"val_113\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_111\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              53 |  # node_Mul_109\n",
       "                    %\"val_115\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_6\", %\"val_48\"{[0.3535533845424652]})\n",
       "              54 |  # node_Mul_112\n",
       "                    %\"val_118\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_113\", %\"val_48\"{[0.3535533845424652]})\n",
       "              55 |  # node_MatMul_113\n",
       "                    %\"val_119\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_115\", %\"val_118\")\n",
       "              56 |  # node_Softmax_114\n",
       "                    %\"val_120\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_119\") {axis=-1}\n",
       "              57 |  # node_scaled_dot_product_attention_1\n",
       "                    %\"scaled_dot_product_attention_1\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_120\", %\"transpose_5\")\n",
       "              58 |  # node_transpose_7\n",
       "                    %\"transpose_7\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(0, 2, 1, 3)}\n",
       "              59 |  # node_view_7\n",
       "                    %\"view_7\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_7\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              60 |  # node_MatMul_121\n",
       "                    %\"val_127\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_7\", %\"val_126\"{...})\n",
       "              61 |  # node_linear_9\n",
       "                    %\"linear_9\"<FLOAT,[1,1500,384]>  ::Add(%\"val_127\", %\"whisper_encoder.layers.1.self_attn.out_proj.bias\"{...})\n",
       "              62 |  # node_add_3\n",
       "                    %\"add_3\"<FLOAT,[1,1500,384]>  ::Add(%\"add_2\", %\"linear_9\")\n",
       "              63 |  # node_layer_norm_3\n",
       "                    %\"layer_norm_3\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3\", %\"whisper_encoder.layers.1.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.1.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              64 |  # node_MatMul_123\n",
       "                    %\"val_131\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_3\", %\"val_130\"{...})\n",
       "              65 |  # node_linear_10\n",
       "                    %\"linear_10\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_131\", %\"whisper_encoder.layers.1.fc1.bias\"{...})\n",
       "              66 |  # node_gelu_3\n",
       "                    %\"gelu_3\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_10\") {approximate='none'}\n",
       "              67 |  # node_MatMul_125\n",
       "                    %\"val_133\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_3\", %\"val_132\"{...})\n",
       "              68 |  # node_linear_11\n",
       "                    %\"linear_11\"<FLOAT,[1,1500,384]>  ::Add(%\"val_133\", %\"whisper_encoder.layers.1.fc2.bias\"{...})\n",
       "              69 |  # node_add_4\n",
       "                    %\"add_4\"<FLOAT,[1,1500,384]>  ::Add(%\"add_3\", %\"linear_11\")\n",
       "              70 |  # node_layer_norm_4\n",
       "                    %\"layer_norm_4\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4\", %\"whisper_encoder.layers.2.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.2.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              71 |  # node_MatMul_127\n",
       "                    %\"val_137\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_136\"{...})\n",
       "              72 |  # node_linear_12\n",
       "                    %\"linear_12\"<FLOAT,[1,1500,384]>  ::Add(%\"val_137\", %\"whisper_encoder.layers.2.self_attn.q_proj.bias\"{...})\n",
       "              73 |  # node_linear_13\n",
       "                    %\"linear_13\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_138\"{...})\n",
       "              74 |  # node_view_8\n",
       "                    %\"view_8\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_13\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              75 |  # node_transpose_8\n",
       "                    %\"transpose_8\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
       "              76 |  # node_MatMul_136\n",
       "                    %\"val_146\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_4\", %\"val_145\"{...})\n",
       "              77 |  # node_linear_14\n",
       "                    %\"linear_14\"<FLOAT,[1,1500,384]>  ::Add(%\"val_146\", %\"whisper_encoder.layers.2.self_attn.v_proj.bias\"{...})\n",
       "              78 |  # node_view_9\n",
       "                    %\"view_9\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_14\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "              79 |  # node_transpose_9\n",
       "                    %\"transpose_9\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_9\") {perm=(0, 2, 1, 3)}\n",
       "              80 |  # node_view_10\n",
       "                    %\"view_10\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_12\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "              81 |  # node_transpose_10\n",
       "                    %\"transpose_10\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
       "              82 |  # node_Reshape_166\n",
       "                    %\"val_176\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_8\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "              83 |  # node_Transpose_167\n",
       "                    %\"val_177\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_176\") {perm=(0, 2, 1)}\n",
       "              84 |  # node_Reshape_169\n",
       "                    %\"val_179\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_177\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "              85 |  # node_Mul_171\n",
       "                    %\"val_181\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_10\", %\"val_48\"{[0.3535533845424652]})\n",
       "              86 |  # node_Mul_174\n",
       "                    %\"val_184\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_179\", %\"val_48\"{[0.3535533845424652]})\n",
       "              87 |  # node_MatMul_175\n",
       "                    %\"val_185\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_181\", %\"val_184\")\n",
       "              88 |  # node_Softmax_176\n",
       "                    %\"val_186\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_185\") {axis=-1}\n",
       "              89 |  # node_scaled_dot_product_attention_2\n",
       "                    %\"scaled_dot_product_attention_2\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_186\", %\"transpose_9\")\n",
       "              90 |  # node_transpose_11\n",
       "                    %\"transpose_11\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(0, 2, 1, 3)}\n",
       "              91 |  # node_view_11\n",
       "                    %\"view_11\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_11\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "              92 |  # node_MatMul_183\n",
       "                    %\"val_193\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_11\", %\"val_192\"{...})\n",
       "              93 |  # node_linear_15\n",
       "                    %\"linear_15\"<FLOAT,[1,1500,384]>  ::Add(%\"val_193\", %\"whisper_encoder.layers.2.self_attn.out_proj.bias\"{...})\n",
       "              94 |  # node_add_5\n",
       "                    %\"add_5\"<FLOAT,[1,1500,384]>  ::Add(%\"add_4\", %\"linear_15\")\n",
       "              95 |  # node_layer_norm_5\n",
       "                    %\"layer_norm_5\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_5\", %\"whisper_encoder.layers.2.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.2.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              96 |  # node_MatMul_185\n",
       "                    %\"val_197\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_5\", %\"val_196\"{...})\n",
       "              97 |  # node_linear_16\n",
       "                    %\"linear_16\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_197\", %\"whisper_encoder.layers.2.fc1.bias\"{...})\n",
       "              98 |  # node_gelu_4\n",
       "                    %\"gelu_4\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_16\") {approximate='none'}\n",
       "              99 |  # node_MatMul_187\n",
       "                    %\"val_199\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_4\", %\"val_198\"{...})\n",
       "             100 |  # node_linear_17\n",
       "                    %\"linear_17\"<FLOAT,[1,1500,384]>  ::Add(%\"val_199\", %\"whisper_encoder.layers.2.fc2.bias\"{...})\n",
       "             101 |  # node_add_6\n",
       "                    %\"add_6\"<FLOAT,[1,1500,384]>  ::Add(%\"add_5\", %\"linear_17\")\n",
       "             102 |  # node_layer_norm_6\n",
       "                    %\"layer_norm_6\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_6\", %\"whisper_encoder.layers.3.self_attn_layer_norm.weight\"{...}, %\"whisper_encoder.layers.3.self_attn_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             103 |  # node_MatMul_189\n",
       "                    %\"val_203\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_202\"{...})\n",
       "             104 |  # node_linear_18\n",
       "                    %\"linear_18\"<FLOAT,[1,1500,384]>  ::Add(%\"val_203\", %\"whisper_encoder.layers.3.self_attn.q_proj.bias\"{...})\n",
       "             105 |  # node_linear_19\n",
       "                    %\"linear_19\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_204\"{...})\n",
       "             106 |  # node_view_12\n",
       "                    %\"view_12\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_19\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "             107 |  # node_transpose_12\n",
       "                    %\"transpose_12\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_12\") {perm=(0, 2, 1, 3)}\n",
       "             108 |  # node_MatMul_198\n",
       "                    %\"val_212\"<FLOAT,[1,1500,384]>  ::MatMul(%\"layer_norm_6\", %\"val_211\"{...})\n",
       "             109 |  # node_linear_20\n",
       "                    %\"linear_20\"<FLOAT,[1,1500,384]>  ::Add(%\"val_212\", %\"whisper_encoder.layers.3.self_attn.v_proj.bias\"{...})\n",
       "             110 |  # node_view_13\n",
       "                    %\"view_13\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_20\", %\"val_10\"{[1, -1, 6, 64]}) {allowzero=1}\n",
       "             111 |  # node_transpose_13\n",
       "                    %\"transpose_13\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_13\") {perm=(0, 2, 1, 3)}\n",
       "             112 |  # node_view_14\n",
       "                    %\"view_14\"<FLOAT,[1,1500,6,64]>  ::Reshape(%\"linear_18\", %\"val_24\"{[1, 1500, 6, 64]}) {allowzero=1}\n",
       "             113 |  # node_transpose_14\n",
       "                    %\"transpose_14\"<FLOAT,[1,6,1500,64]>  ::Transpose(%\"view_14\") {perm=(0, 2, 1, 3)}\n",
       "             114 |  # node_Reshape_228\n",
       "                    %\"val_242\"<FLOAT,[6,1500,64]>  ::Reshape(%\"transpose_12\", %\"val_43\"{[-1, 1500, 64]}) {allowzero=0}\n",
       "             115 |  # node_Transpose_229\n",
       "                    %\"val_243\"<FLOAT,[6,64,1500]>  ::Transpose(%\"val_242\") {perm=(0, 2, 1)}\n",
       "             116 |  # node_Reshape_231\n",
       "                    %\"val_245\"<FLOAT,[1,6,64,1500]>  ::Reshape(%\"val_243\", %\"val_46\"{[1, 6, 64, 1500]}) {allowzero=0}\n",
       "             117 |  # node_Mul_233\n",
       "                    %\"val_247\"<FLOAT,[1,6,1500,64]>  ::Mul(%\"transpose_14\", %\"val_48\"{[0.3535533845424652]})\n",
       "             118 |  # node_Mul_236\n",
       "                    %\"val_250\"<FLOAT,[1,6,64,1500]>  ::Mul(%\"val_245\", %\"val_48\"{[0.3535533845424652]})\n",
       "             119 |  # node_MatMul_237\n",
       "                    %\"val_251\"<FLOAT,[1,6,1500,1500]>  ::MatMul(%\"val_247\", %\"val_250\")\n",
       "             120 |  # node_Softmax_238\n",
       "                    %\"val_252\"<FLOAT,[1,6,1500,1500]>  ::Softmax(%\"val_251\") {axis=-1}\n",
       "             121 |  # node_scaled_dot_product_attention_3\n",
       "                    %\"scaled_dot_product_attention_3\"<FLOAT,[1,6,1500,64]>  ::MatMul(%\"val_252\", %\"transpose_13\")\n",
       "             122 |  # node_transpose_15\n",
       "                    %\"transpose_15\"<FLOAT,[1,1500,6,64]>  ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(0, 2, 1, 3)}\n",
       "             123 |  # node_view_15\n",
       "                    %\"view_15\"<FLOAT,[1,1500,384]>  ::Reshape(%\"transpose_15\", %\"val_59\"{[1, 1500, 384]}) {allowzero=1}\n",
       "             124 |  # node_MatMul_245\n",
       "                    %\"val_259\"<FLOAT,[1,1500,384]>  ::MatMul(%\"view_15\", %\"val_258\"{...})\n",
       "             125 |  # node_linear_21\n",
       "                    %\"linear_21\"<FLOAT,[1,1500,384]>  ::Add(%\"val_259\", %\"whisper_encoder.layers.3.self_attn.out_proj.bias\"{...})\n",
       "             126 |  # node_add_7\n",
       "                    %\"add_7\"<FLOAT,[1,1500,384]>  ::Add(%\"add_6\", %\"linear_21\")\n",
       "             127 |  # node_layer_norm_7\n",
       "                    %\"layer_norm_7\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_7\", %\"whisper_encoder.layers.3.final_layer_norm.weight\"{...}, %\"whisper_encoder.layers.3.final_layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             128 |  # node_MatMul_247\n",
       "                    %\"val_263\"<FLOAT,[1,1500,1536]>  ::MatMul(%\"layer_norm_7\", %\"val_262\"{...})\n",
       "             129 |  # node_linear_22\n",
       "                    %\"linear_22\"<FLOAT,[1,1500,1536]>  ::Add(%\"val_263\", %\"whisper_encoder.layers.3.fc1.bias\"{...})\n",
       "             130 |  # node_gelu_5\n",
       "                    %\"gelu_5\"<FLOAT,[1,1500,1536]>  ::Gelu(%\"linear_22\") {approximate='none'}\n",
       "             131 |  # node_MatMul_249\n",
       "                    %\"val_265\"<FLOAT,[1,1500,384]>  ::MatMul(%\"gelu_5\", %\"val_264\"{...})\n",
       "             132 |  # node_linear_23\n",
       "                    %\"linear_23\"<FLOAT,[1,1500,384]>  ::Add(%\"val_265\", %\"whisper_encoder.layers.3.fc2.bias\"{...})\n",
       "             133 |  # node_add_8\n",
       "                    %\"add_8\"<FLOAT,[1,1500,384]>  ::Add(%\"add_7\", %\"linear_23\")\n",
       "             134 |  # node_layer_norm_8\n",
       "                    %\"layer_norm_8\"<FLOAT,[1,1500,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_8\", %\"whisper_encoder.layer_norm.weight\"{...}, %\"whisper_encoder.layer_norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             135 |  # node_Unsqueeze_251\n",
       "                    %\"val_269\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add\", %\"val_331\"{[2]})\n",
       "             136 |  # node_Unsqueeze_253\n",
       "                    %\"val_271\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_2\", %\"val_331\"{[2]})\n",
       "             137 |  # node_Unsqueeze_255\n",
       "                    %\"val_273\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_4\", %\"val_331\"{[2]})\n",
       "             138 |  # node_Unsqueeze_257\n",
       "                    %\"val_275\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"add_6\", %\"val_331\"{[2]})\n",
       "             139 |  # node_Unsqueeze_259\n",
       "                    %\"val_277\"<FLOAT,[1,1500,1,384]>  ::Unsqueeze(%\"layer_norm_8\", %\"val_331\"{[2]})\n",
       "             140 |  # node_stack\n",
       "                    %\"stack\"<FLOAT,[1,1500,5,384]>  ::Concat(%\"val_269\", %\"val_271\", %\"val_273\", %\"val_275\", %\"val_277\") {axis=2}\n",
       "             141 |  # node_slice_1\n",
       "                    %\"slice_1\"<FLOAT,[1,332,5,384]>  ::Slice(%\"stack\", %\"val_281\"{[0]}, %\"val_285\"{[332]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             142 |  # node_cat_1\n",
       "                    %\"cat_1\"<FLOAT,[1,348,5,384]>  ::Concat(%\"zeros_like\"{...}, %\"slice_1\", %\"zeros_like_1\"{...}) {axis=1}\n",
       "             143 |  # node_slice_4\n",
       "                    %\"slice_4\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_281\"{[0]}, %\"val_323\"{[10]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             144 |  # node_slice_5\n",
       "                    %\"slice_5\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_331\"{[2]}, %\"val_334\"{[12]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             145 |  # node_slice_6\n",
       "                    %\"slice_6\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_341\"{[4]}, %\"val_345\"{[14]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             146 |  # node_slice_7\n",
       "                    %\"slice_7\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_353\"{[6]}, %\"val_357\"{[16]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             147 |  # node_slice_8\n",
       "                    %\"slice_8\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_365\"{[8]}, %\"val_369\"{[18]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             148 |  # node_slice_9\n",
       "                    %\"slice_9\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_323\"{[10]}, %\"val_380\"{[20]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             149 |  # node_slice_10\n",
       "                    %\"slice_10\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_334\"{[12]}, %\"val_391\"{[22]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             150 |  # node_slice_11\n",
       "                    %\"slice_11\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_345\"{[14]}, %\"val_402\"{[24]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             151 |  # node_slice_12\n",
       "                    %\"slice_12\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_357\"{[16]}, %\"val_413\"{[26]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             152 |  # node_slice_13\n",
       "                    %\"slice_13\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_369\"{[18]}, %\"val_424\"{[28]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             153 |  # node_slice_14\n",
       "                    %\"slice_14\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_380\"{[20]}, %\"val_435\"{[30]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             154 |  # node_slice_15\n",
       "                    %\"slice_15\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_391\"{[22]}, %\"val_446\"{[32]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             155 |  # node_slice_16\n",
       "                    %\"slice_16\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_402\"{[24]}, %\"val_457\"{[34]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             156 |  # node_slice_17\n",
       "                    %\"slice_17\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_413\"{[26]}, %\"val_468\"{[36]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             157 |  # node_slice_18\n",
       "                    %\"slice_18\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_424\"{[28]}, %\"val_479\"{[38]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             158 |  # node_slice_19\n",
       "                    %\"slice_19\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_435\"{[30]}, %\"val_490\"{[40]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             159 |  # node_slice_20\n",
       "                    %\"slice_20\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_446\"{[32]}, %\"val_501\"{[42]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             160 |  # node_slice_21\n",
       "                    %\"slice_21\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_457\"{[34]}, %\"val_512\"{[44]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             161 |  # node_slice_22\n",
       "                    %\"slice_22\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_468\"{[36]}, %\"val_523\"{[46]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             162 |  # node_slice_23\n",
       "                    %\"slice_23\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_479\"{[38]}, %\"val_534\"{[48]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             163 |  # node_slice_24\n",
       "                    %\"slice_24\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_490\"{[40]}, %\"val_545\"{[50]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             164 |  # node_slice_25\n",
       "                    %\"slice_25\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_501\"{[42]}, %\"val_556\"{[52]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             165 |  # node_slice_26\n",
       "                    %\"slice_26\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_512\"{[44]}, %\"val_567\"{[54]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             166 |  # node_slice_27\n",
       "                    %\"slice_27\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_523\"{[46]}, %\"val_578\"{[56]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             167 |  # node_slice_28\n",
       "                    %\"slice_28\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_534\"{[48]}, %\"val_589\"{[58]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             168 |  # node_slice_29\n",
       "                    %\"slice_29\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_545\"{[50]}, %\"val_600\"{[60]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             169 |  # node_slice_30\n",
       "                    %\"slice_30\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_556\"{[52]}, %\"val_611\"{[62]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             170 |  # node_slice_31\n",
       "                    %\"slice_31\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_567\"{[54]}, %\"val_622\"{[64]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             171 |  # node_slice_32\n",
       "                    %\"slice_32\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_578\"{[56]}, %\"val_633\"{[66]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             172 |  # node_slice_33\n",
       "                    %\"slice_33\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_589\"{[58]}, %\"val_644\"{[68]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             173 |  # node_slice_34\n",
       "                    %\"slice_34\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_600\"{[60]}, %\"val_655\"{[70]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             174 |  # node_slice_35\n",
       "                    %\"slice_35\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_611\"{[62]}, %\"val_666\"{[72]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             175 |  # node_slice_36\n",
       "                    %\"slice_36\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_622\"{[64]}, %\"val_677\"{[74]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             176 |  # node_slice_37\n",
       "                    %\"slice_37\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_633\"{[66]}, %\"val_688\"{[76]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             177 |  # node_slice_38\n",
       "                    %\"slice_38\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_644\"{[68]}, %\"val_699\"{[78]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             178 |  # node_slice_39\n",
       "                    %\"slice_39\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_655\"{[70]}, %\"val_710\"{[80]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             179 |  # node_slice_40\n",
       "                    %\"slice_40\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_666\"{[72]}, %\"val_721\"{[82]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             180 |  # node_slice_41\n",
       "                    %\"slice_41\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_677\"{[74]}, %\"val_732\"{[84]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             181 |  # node_slice_42\n",
       "                    %\"slice_42\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_688\"{[76]}, %\"val_743\"{[86]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             182 |  # node_slice_43\n",
       "                    %\"slice_43\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_699\"{[78]}, %\"val_754\"{[88]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             183 |  # node_slice_44\n",
       "                    %\"slice_44\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_710\"{[80]}, %\"val_765\"{[90]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             184 |  # node_slice_45\n",
       "                    %\"slice_45\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_721\"{[82]}, %\"val_776\"{[92]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             185 |  # node_slice_46\n",
       "                    %\"slice_46\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_732\"{[84]}, %\"val_787\"{[94]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             186 |  # node_slice_47\n",
       "                    %\"slice_47\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_743\"{[86]}, %\"val_798\"{[96]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             187 |  # node_slice_48\n",
       "                    %\"slice_48\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_754\"{[88]}, %\"val_809\"{[98]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             188 |  # node_slice_49\n",
       "                    %\"slice_49\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_765\"{[90]}, %\"val_820\"{[100]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             189 |  # node_slice_50\n",
       "                    %\"slice_50\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_776\"{[92]}, %\"val_831\"{[102]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             190 |  # node_slice_51\n",
       "                    %\"slice_51\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_787\"{[94]}, %\"val_842\"{[104]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             191 |  # node_slice_52\n",
       "                    %\"slice_52\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_798\"{[96]}, %\"val_853\"{[106]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             192 |  # node_slice_53\n",
       "                    %\"slice_53\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_809\"{[98]}, %\"val_864\"{[108]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             193 |  # node_slice_54\n",
       "                    %\"slice_54\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_820\"{[100]}, %\"val_875\"{[110]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             194 |  # node_slice_55\n",
       "                    %\"slice_55\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_831\"{[102]}, %\"val_886\"{[112]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             195 |  # node_slice_56\n",
       "                    %\"slice_56\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_842\"{[104]}, %\"val_897\"{[114]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             196 |  # node_slice_57\n",
       "                    %\"slice_57\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_853\"{[106]}, %\"val_908\"{[116]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             197 |  # node_slice_58\n",
       "                    %\"slice_58\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_864\"{[108]}, %\"val_919\"{[118]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             198 |  # node_slice_59\n",
       "                    %\"slice_59\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_875\"{[110]}, %\"val_930\"{[120]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             199 |  # node_slice_60\n",
       "                    %\"slice_60\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_886\"{[112]}, %\"val_941\"{[122]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             200 |  # node_slice_61\n",
       "                    %\"slice_61\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_897\"{[114]}, %\"val_952\"{[124]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             201 |  # node_slice_62\n",
       "                    %\"slice_62\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_908\"{[116]}, %\"val_963\"{[126]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             202 |  # node_slice_63\n",
       "                    %\"slice_63\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_919\"{[118]}, %\"val_974\"{[128]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             203 |  # node_slice_64\n",
       "                    %\"slice_64\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_930\"{[120]}, %\"val_985\"{[130]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             204 |  # node_slice_65\n",
       "                    %\"slice_65\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_941\"{[122]}, %\"val_996\"{[132]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             205 |  # node_slice_66\n",
       "                    %\"slice_66\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_952\"{[124]}, %\"val_1007\"{[134]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             206 |  # node_slice_67\n",
       "                    %\"slice_67\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_963\"{[126]}, %\"val_1018\"{[136]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             207 |  # node_slice_68\n",
       "                    %\"slice_68\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_974\"{[128]}, %\"val_1029\"{[138]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             208 |  # node_slice_69\n",
       "                    %\"slice_69\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_985\"{[130]}, %\"val_1040\"{[140]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             209 |  # node_slice_70\n",
       "                    %\"slice_70\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_996\"{[132]}, %\"val_1051\"{[142]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             210 |  # node_slice_71\n",
       "                    %\"slice_71\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1007\"{[134]}, %\"val_1062\"{[144]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             211 |  # node_slice_72\n",
       "                    %\"slice_72\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1018\"{[136]}, %\"val_1073\"{[146]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             212 |  # node_slice_73\n",
       "                    %\"slice_73\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1029\"{[138]}, %\"val_1084\"{[148]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             213 |  # node_slice_74\n",
       "                    %\"slice_74\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1040\"{[140]}, %\"val_1095\"{[150]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             214 |  # node_slice_75\n",
       "                    %\"slice_75\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1051\"{[142]}, %\"val_1106\"{[152]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             215 |  # node_slice_76\n",
       "                    %\"slice_76\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1062\"{[144]}, %\"val_1117\"{[154]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             216 |  # node_slice_77\n",
       "                    %\"slice_77\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1073\"{[146]}, %\"val_1128\"{[156]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             217 |  # node_slice_78\n",
       "                    %\"slice_78\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1084\"{[148]}, %\"val_1139\"{[158]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             218 |  # node_slice_79\n",
       "                    %\"slice_79\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1095\"{[150]}, %\"val_1150\"{[160]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             219 |  # node_slice_80\n",
       "                    %\"slice_80\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1106\"{[152]}, %\"val_1161\"{[162]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             220 |  # node_slice_81\n",
       "                    %\"slice_81\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1117\"{[154]}, %\"val_1172\"{[164]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             221 |  # node_slice_82\n",
       "                    %\"slice_82\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1128\"{[156]}, %\"val_1183\"{[166]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             222 |  # node_slice_83\n",
       "                    %\"slice_83\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1139\"{[158]}, %\"val_1194\"{[168]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             223 |  # node_slice_84\n",
       "                    %\"slice_84\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1150\"{[160]}, %\"val_1205\"{[170]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             224 |  # node_slice_85\n",
       "                    %\"slice_85\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1161\"{[162]}, %\"val_1216\"{[172]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             225 |  # node_slice_86\n",
       "                    %\"slice_86\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1172\"{[164]}, %\"val_1227\"{[174]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             226 |  # node_slice_87\n",
       "                    %\"slice_87\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1183\"{[166]}, %\"val_1238\"{[176]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             227 |  # node_slice_88\n",
       "                    %\"slice_88\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1194\"{[168]}, %\"val_1249\"{[178]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             228 |  # node_slice_89\n",
       "                    %\"slice_89\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1205\"{[170]}, %\"val_1260\"{[180]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             229 |  # node_slice_90\n",
       "                    %\"slice_90\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1216\"{[172]}, %\"val_1271\"{[182]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             230 |  # node_slice_91\n",
       "                    %\"slice_91\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1227\"{[174]}, %\"val_1282\"{[184]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             231 |  # node_slice_92\n",
       "                    %\"slice_92\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1238\"{[176]}, %\"val_1293\"{[186]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             232 |  # node_slice_93\n",
       "                    %\"slice_93\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1249\"{[178]}, %\"val_1304\"{[188]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             233 |  # node_slice_94\n",
       "                    %\"slice_94\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1260\"{[180]}, %\"val_1315\"{[190]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             234 |  # node_slice_95\n",
       "                    %\"slice_95\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1271\"{[182]}, %\"val_1326\"{[192]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             235 |  # node_slice_96\n",
       "                    %\"slice_96\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1282\"{[184]}, %\"val_1337\"{[194]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             236 |  # node_slice_97\n",
       "                    %\"slice_97\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1293\"{[186]}, %\"val_1348\"{[196]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             237 |  # node_slice_98\n",
       "                    %\"slice_98\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1304\"{[188]}, %\"val_1359\"{[198]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             238 |  # node_slice_99\n",
       "                    %\"slice_99\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1315\"{[190]}, %\"val_1370\"{[200]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             239 |  # node_slice_100\n",
       "                    %\"slice_100\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1326\"{[192]}, %\"val_1381\"{[202]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             240 |  # node_slice_101\n",
       "                    %\"slice_101\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1337\"{[194]}, %\"val_1392\"{[204]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             241 |  # node_slice_102\n",
       "                    %\"slice_102\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1348\"{[196]}, %\"val_1403\"{[206]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             242 |  # node_slice_103\n",
       "                    %\"slice_103\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1359\"{[198]}, %\"val_1414\"{[208]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             243 |  # node_slice_104\n",
       "                    %\"slice_104\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1370\"{[200]}, %\"val_1425\"{[210]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             244 |  # node_slice_105\n",
       "                    %\"slice_105\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1381\"{[202]}, %\"val_1436\"{[212]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             245 |  # node_slice_106\n",
       "                    %\"slice_106\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1392\"{[204]}, %\"val_1447\"{[214]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             246 |  # node_slice_107\n",
       "                    %\"slice_107\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1403\"{[206]}, %\"val_1458\"{[216]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             247 |  # node_slice_108\n",
       "                    %\"slice_108\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1414\"{[208]}, %\"val_1469\"{[218]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             248 |  # node_slice_109\n",
       "                    %\"slice_109\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1425\"{[210]}, %\"val_1480\"{[220]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             249 |  # node_slice_110\n",
       "                    %\"slice_110\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1436\"{[212]}, %\"val_1491\"{[222]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             250 |  # node_slice_111\n",
       "                    %\"slice_111\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1447\"{[214]}, %\"val_1502\"{[224]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             251 |  # node_slice_112\n",
       "                    %\"slice_112\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1458\"{[216]}, %\"val_1513\"{[226]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             252 |  # node_slice_113\n",
       "                    %\"slice_113\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1469\"{[218]}, %\"val_1524\"{[228]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             253 |  # node_slice_114\n",
       "                    %\"slice_114\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1480\"{[220]}, %\"val_1535\"{[230]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             254 |  # node_slice_115\n",
       "                    %\"slice_115\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1491\"{[222]}, %\"val_1546\"{[232]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             255 |  # node_slice_116\n",
       "                    %\"slice_116\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1502\"{[224]}, %\"val_1557\"{[234]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             256 |  # node_slice_117\n",
       "                    %\"slice_117\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1513\"{[226]}, %\"val_1568\"{[236]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             257 |  # node_slice_118\n",
       "                    %\"slice_118\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1524\"{[228]}, %\"val_1579\"{[238]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             258 |  # node_slice_119\n",
       "                    %\"slice_119\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1535\"{[230]}, %\"val_1590\"{[240]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             259 |  # node_slice_120\n",
       "                    %\"slice_120\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1546\"{[232]}, %\"val_1601\"{[242]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             260 |  # node_slice_121\n",
       "                    %\"slice_121\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1557\"{[234]}, %\"val_1612\"{[244]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             261 |  # node_slice_122\n",
       "                    %\"slice_122\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1568\"{[236]}, %\"val_1623\"{[246]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             262 |  # node_slice_123\n",
       "                    %\"slice_123\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1579\"{[238]}, %\"val_1634\"{[248]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             263 |  # node_slice_124\n",
       "                    %\"slice_124\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1590\"{[240]}, %\"val_1645\"{[250]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             264 |  # node_slice_125\n",
       "                    %\"slice_125\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1601\"{[242]}, %\"val_1656\"{[252]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             265 |  # node_slice_126\n",
       "                    %\"slice_126\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1612\"{[244]}, %\"val_1667\"{[254]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             266 |  # node_slice_127\n",
       "                    %\"slice_127\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1623\"{[246]}, %\"val_1678\"{[256]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             267 |  # node_slice_128\n",
       "                    %\"slice_128\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1634\"{[248]}, %\"val_1689\"{[258]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             268 |  # node_slice_129\n",
       "                    %\"slice_129\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1645\"{[250]}, %\"val_1700\"{[260]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             269 |  # node_slice_130\n",
       "                    %\"slice_130\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1656\"{[252]}, %\"val_1711\"{[262]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             270 |  # node_slice_131\n",
       "                    %\"slice_131\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1667\"{[254]}, %\"val_1722\"{[264]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             271 |  # node_slice_132\n",
       "                    %\"slice_132\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1678\"{[256]}, %\"val_1733\"{[266]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             272 |  # node_slice_133\n",
       "                    %\"slice_133\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1689\"{[258]}, %\"val_1744\"{[268]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             273 |  # node_slice_134\n",
       "                    %\"slice_134\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1700\"{[260]}, %\"val_1755\"{[270]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             274 |  # node_slice_135\n",
       "                    %\"slice_135\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1711\"{[262]}, %\"val_1766\"{[272]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             275 |  # node_slice_136\n",
       "                    %\"slice_136\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1722\"{[264]}, %\"val_1777\"{[274]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             276 |  # node_slice_137\n",
       "                    %\"slice_137\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1733\"{[266]}, %\"val_1788\"{[276]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             277 |  # node_slice_138\n",
       "                    %\"slice_138\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1744\"{[268]}, %\"val_1799\"{[278]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             278 |  # node_slice_139\n",
       "                    %\"slice_139\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1755\"{[270]}, %\"val_1810\"{[280]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             279 |  # node_slice_140\n",
       "                    %\"slice_140\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1766\"{[272]}, %\"val_1821\"{[282]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             280 |  # node_slice_141\n",
       "                    %\"slice_141\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1777\"{[274]}, %\"val_1832\"{[284]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             281 |  # node_slice_142\n",
       "                    %\"slice_142\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1788\"{[276]}, %\"val_1843\"{[286]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             282 |  # node_slice_143\n",
       "                    %\"slice_143\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1799\"{[278]}, %\"val_1854\"{[288]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             283 |  # node_slice_144\n",
       "                    %\"slice_144\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1810\"{[280]}, %\"val_1865\"{[290]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             284 |  # node_slice_145\n",
       "                    %\"slice_145\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1821\"{[282]}, %\"val_1876\"{[292]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             285 |  # node_slice_146\n",
       "                    %\"slice_146\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1832\"{[284]}, %\"val_1887\"{[294]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             286 |  # node_slice_147\n",
       "                    %\"slice_147\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1843\"{[286]}, %\"val_1898\"{[296]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             287 |  # node_slice_148\n",
       "                    %\"slice_148\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1854\"{[288]}, %\"val_1909\"{[298]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             288 |  # node_slice_149\n",
       "                    %\"slice_149\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1865\"{[290]}, %\"val_1920\"{[300]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             289 |  # node_slice_150\n",
       "                    %\"slice_150\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1876\"{[292]}, %\"val_1931\"{[302]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             290 |  # node_slice_151\n",
       "                    %\"slice_151\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1887\"{[294]}, %\"val_1942\"{[304]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             291 |  # node_slice_152\n",
       "                    %\"slice_152\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1898\"{[296]}, %\"val_1953\"{[306]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             292 |  # node_slice_153\n",
       "                    %\"slice_153\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1909\"{[298]}, %\"val_1964\"{[308]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             293 |  # node_slice_154\n",
       "                    %\"slice_154\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1920\"{[300]}, %\"val_1975\"{[310]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             294 |  # node_slice_155\n",
       "                    %\"slice_155\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1931\"{[302]}, %\"val_1986\"{[312]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             295 |  # node_slice_156\n",
       "                    %\"slice_156\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1942\"{[304]}, %\"val_1997\"{[314]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             296 |  # node_slice_157\n",
       "                    %\"slice_157\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1953\"{[306]}, %\"val_2008\"{[316]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             297 |  # node_slice_158\n",
       "                    %\"slice_158\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1964\"{[308]}, %\"val_2019\"{[318]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             298 |  # node_slice_159\n",
       "                    %\"slice_159\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1975\"{[310]}, %\"val_2030\"{[320]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             299 |  # node_slice_160\n",
       "                    %\"slice_160\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1986\"{[312]}, %\"val_2041\"{[322]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             300 |  # node_slice_161\n",
       "                    %\"slice_161\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_1997\"{[314]}, %\"val_2052\"{[324]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             301 |  # node_slice_162\n",
       "                    %\"slice_162\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2008\"{[316]}, %\"val_2063\"{[326]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             302 |  # node_slice_163\n",
       "                    %\"slice_163\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2019\"{[318]}, %\"val_2074\"{[328]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             303 |  # node_slice_164\n",
       "                    %\"slice_164\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2030\"{[320]}, %\"val_2085\"{[330]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             304 |  # node_slice_165\n",
       "                    %\"slice_165\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2041\"{[322]}, %\"val_285\"{[332]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             305 |  # node_slice_166\n",
       "                    %\"slice_166\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2052\"{[324]}, %\"val_2106\"{[334]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             306 |  # node_slice_167\n",
       "                    %\"slice_167\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2063\"{[326]}, %\"val_2117\"{[336]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             307 |  # node_slice_168\n",
       "                    %\"slice_168\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2074\"{[328]}, %\"val_2128\"{[338]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             308 |  # node_slice_169\n",
       "                    %\"slice_169\"<FLOAT,[1,10,5,384]>  ::Slice(%\"cat_1\", %\"val_2085\"{[330]}, %\"val_2139\"{[340]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             309 |  # node_cat_2\n",
       "                    %\"cat_2\"<FLOAT,[166,10,5,384]>  ::Concat(%\"slice_4\", %\"slice_5\", %\"slice_6\", %\"slice_7\", %\"slice_8\", %\"slice_9\", %\"slice_10\", %\"slice_11\", %\"slice_12\", %\"slice_13\", %\"slice_14\", %\"slice_15\", %\"slice_16\", %\"slice_17\", %\"slice_18\", %\"slice_19\", %\"slice_20\", %\"slice_21\", %\"slice_22\", %\"slice_23\", %\"slice_24\", %\"slice_25\", %\"slice_26\", %\"slice_27\", %\"slice_28\", %\"slice_29\", %\"slice_30\", %\"slice_31\", %\"slice_32\", %\"slice_33\", %\"slice_34\", %\"slice_35\", %\"slice_36\", %\"slice_37\", %\"slice_38\", %\"slice_39\", %\"slice_40\", %\"slice_41\", %\"slice_42\", %\"slice_43\", %\"slice_44\", %\"slice_45\", %\"slice_46\", %\"slice_47\", %\"slice_48\", %\"slice_49\", %\"slice_50\", %\"slice_51\", %\"slice_52\", %\"slice_53\", %\"slice_54\", %\"slice_55\", %\"slice_56\", %\"slice_57\", %\"slice_58\", %\"slice_59\", %\"slice_60\", %\"slice_61\", %\"slice_62\", %\"slice_63\", %\"slice_64\", %\"slice_65\", %\"slice_66\", %\"slice_67\", %\"slice_68\", %\"slice_69\", %\"slice_70\", %\"slice_71\", %\"slice_72\", %\"slice_73\", %\"slice_74\", %\"slice_75\", %\"slice_76\", %\"slice_77\", %\"slice_78\", %\"slice_79\", %\"slice_80\", %\"slice_81\", %\"slice_82\", %\"slice_83\", %\"slice_84\", %\"slice_85\", %\"slice_86\", %\"slice_87\", %\"slice_88\", %\"slice_89\", %\"slice_90\", %\"slice_91\", %\"slice_92\", %\"slice_93\", %\"slice_94\", %\"slice_95\", %\"slice_96\", %\"slice_97\", %\"slice_98\", %\"slice_99\", %\"slice_100\", %\"slice_101\", %\"slice_102\", %\"slice_103\", %\"slice_104\", %\"slice_105\", %\"slice_106\", %\"slice_107\", %\"slice_108\", %\"slice_109\", %\"slice_110\", %\"slice_111\", %\"slice_112\", %\"slice_113\", %\"slice_114\", %\"slice_115\", %\"slice_116\", %\"slice_117\", %\"slice_118\", %\"slice_119\", %\"slice_120\", %\"slice_121\", %\"slice_122\", %\"slice_123\", %\"slice_124\", %\"slice_125\", %\"slice_126\", %\"slice_127\", %\"slice_128\", %\"slice_129\", %\"slice_130\", %\"slice_131\", %\"slice_132\", %\"slice_133\", %\"slice_134\", %\"slice_135\", %\"slice_136\", %\"slice_137\", %\"slice_138\", %\"slice_139\", %\"slice_140\", %\"slice_141\", %\"slice_142\", %\"slice_143\", %\"slice_144\", %\"slice_145\", %\"slice_146\", %\"slice_147\", %\"slice_148\", %\"slice_149\", %\"slice_150\", %\"slice_151\", %\"slice_152\", %\"slice_153\", %\"slice_154\", %\"slice_155\", %\"slice_156\", %\"slice_157\", %\"slice_158\", %\"slice_159\", %\"slice_160\", %\"slice_161\", %\"slice_162\", %\"slice_163\", %\"slice_164\", %\"slice_165\", %\"slice_166\", %\"slice_167\", %\"slice_168\", %\"slice_169\") {axis=0}\n",
       "             310 |  # node_view_16\n",
       "                    %\"view_16\"<FLOAT,[166,50,384]>  ::Reshape(%\"cat_2\", %\"val_2148\"{[166, 50, 384]}) {allowzero=1}\n",
       "             311 |  # node_slice_170\n",
       "                    %\"slice_170\"<FLOAT,[8,50,384]>  ::Slice(%\"view_16\", %\"val_281\"{[0]}, %\"val_365\"{[8]}, %\"val_281\"{[0]}, %\"val_289\"{[1]})\n",
       "             312 |  # node_slice_171\n",
       "                    %\"slice_171\"<FLOAT,[1,50,384]>  ::Slice(%\"pe.pe\"{...}, %\"val_281\"{[0]}, %\"val_545\"{[50]}, %\"val_289\"{[1]}, %\"val_289\"{[1]})\n",
       "             313 |  # node_add_9\n",
       "                    %\"add_9\"<FLOAT,[8,50,384]>  ::Add(%\"slice_170\", %\"slice_171\")\n",
       "             314 |  # node_linear_24\n",
       "                    %\"linear_24\"<FLOAT,[8,1280]>  ::Gemm(%\"cat_4\"{...}, %\"unet.time_embedding.linear_1.weight\"{...}, %\"unet.time_embedding.linear_1.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             315 |  # node_Sigmoid_2181\n",
       "                    %\"val_2199\"<FLOAT,[8,1280]>  ::Sigmoid(%\"linear_24\")\n",
       "             316 |  # node_silu\n",
       "                    %\"silu\"<FLOAT,[8,1280]>  ::Mul(%\"linear_24\", %\"val_2199\")\n",
       "             317 |  # node_linear_25\n",
       "                    %\"linear_25\"<FLOAT,[8,1280]>  ::Gemm(%\"silu\", %\"unet.time_embedding.linear_2.weight\"{...}, %\"unet.time_embedding.linear_2.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             318 |  # node_conv2d\n",
       "                    %\"conv2d\"<FLOAT,[8,320,32,32]>  ::Conv(%\"latent_inputs\", %\"unet.conv_in.weight\"{...}, %\"unet.conv_in.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             319 |  # node_Reshape_2186\n",
       "                    %\"val_2204\"<FLOAT,[8,32,10240]>  ::Reshape(%\"conv2d\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             320 |  # node_InstanceNormalization_2193\n",
       "                    %\"val_2211\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2204\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             321 |  # node_Reshape_2195\n",
       "                    %\"val_2213\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2211\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             322 |  # node_Mul_2202\n",
       "                    %\"val_2220\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2213\", %\"val_2219\"{...})\n",
       "             323 |  # node_group_norm\n",
       "                    %\"group_norm\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2220\", %\"val_2221\"{...})\n",
       "             324 |  # node_Sigmoid_2204\n",
       "                    %\"val_2222\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm\")\n",
       "             325 |  # node_silu_1\n",
       "                    %\"silu_1\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm\", %\"val_2222\")\n",
       "             326 |  # node_conv2d_1\n",
       "                    %\"conv2d_1\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_1\", %\"unet.down_blocks.0.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             327 |  # node_Sigmoid_2205\n",
       "                    %\"val_2223\"<FLOAT,[8,1280]>  ::Sigmoid(%\"linear_25\")\n",
       "             328 |  # node_silu_2\n",
       "                    %\"silu_2\"<FLOAT,[8,1280]>  ::Mul(%\"linear_25\", %\"val_2223\")\n",
       "             329 |  # node_linear_26\n",
       "                    %\"linear_26\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.0.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.0.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             330 |  # node_Unsqueeze_7717\n",
       "                    %\"unsqueeze_3\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_26\", %\"val_6521\"{[2, 3]})\n",
       "             331 |  # node_add_10\n",
       "                    %\"add_10\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_1\", %\"unsqueeze_3\")\n",
       "             332 |  # node_Reshape_2212\n",
       "                    %\"val_2230\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_10\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             333 |  # node_InstanceNormalization_2219\n",
       "                    %\"val_2237\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2230\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             334 |  # node_Reshape_2221\n",
       "                    %\"val_2239\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2237\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             335 |  # node_Mul_2228\n",
       "                    %\"val_2246\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2239\", %\"val_2245\"{...})\n",
       "             336 |  # node_group_norm_1\n",
       "                    %\"group_norm_1\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2246\", %\"val_2247\"{...})\n",
       "             337 |  # node_Sigmoid_2230\n",
       "                    %\"val_2248\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_1\")\n",
       "             338 |  # node_silu_3\n",
       "                    %\"silu_3\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_1\", %\"val_2248\")\n",
       "             339 |  # node_conv2d_2\n",
       "                    %\"conv2d_2\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_3\", %\"unet.down_blocks.0.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             340 |  # node_add_11\n",
       "                    %\"add_11\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d\", %\"conv2d_2\")\n",
       "             341 |  # node_Reshape_2236\n",
       "                    %\"val_2254\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_11\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             342 |  # node_InstanceNormalization_2243\n",
       "                    %\"val_2261\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2254\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             343 |  # node_Reshape_2245\n",
       "                    %\"val_2263\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2261\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             344 |  # node_Mul_2252\n",
       "                    %\"val_2270\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2263\", %\"val_2269\"{...})\n",
       "             345 |  # node_group_norm_2\n",
       "                    %\"group_norm_2\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2270\", %\"val_2271\"{...})\n",
       "             346 |  # node_conv2d_3\n",
       "                    %\"conv2d_3\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_2\", %\"unet.down_blocks.0.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.0.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             347 |  # node_permute_1\n",
       "                    %\"permute_1\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_3\") {perm=(0, 2, 3, 1)}\n",
       "             348 |  # node_view_17\n",
       "                    %\"view_17\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_1\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "             349 |  # node_layer_norm_9\n",
       "                    %\"layer_norm_9\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_17\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             350 |  # node_linear_27\n",
       "                    %\"linear_27\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2279\"{...})\n",
       "             351 |  # node_linear_28\n",
       "                    %\"linear_28\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2280\"{...})\n",
       "             352 |  # node_linear_29\n",
       "                    %\"linear_29\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_9\", %\"val_2281\"{...})\n",
       "             353 |  # node_view_18\n",
       "                    %\"view_18\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_27\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             354 |  # node_transpose_16\n",
       "                    %\"transpose_16\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
       "             355 |  # node_view_19\n",
       "                    %\"view_19\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_28\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             356 |  # node_transpose_17\n",
       "                    %\"transpose_17\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_19\") {perm=(0, 2, 1, 3)}\n",
       "             357 |  # node_view_20\n",
       "                    %\"view_20\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_29\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             358 |  # node_transpose_18\n",
       "                    %\"transpose_18\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_20\") {perm=(0, 2, 1, 3)}\n",
       "             359 |  # node_Reshape_2297\n",
       "                    %\"val_2317\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_17\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "             360 |  # node_Transpose_2298\n",
       "                    %\"val_2318\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_2317\") {perm=(0, 2, 1)}\n",
       "             361 |  # node_Reshape_2300\n",
       "                    %\"val_2320\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_2318\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "             362 |  # node_Mul_2302\n",
       "                    %\"val_2322\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_16\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             363 |  # node_Mul_2305\n",
       "                    %\"val_2325\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_2320\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             364 |  # node_MatMul_2306\n",
       "                    %\"val_2326\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_2322\", %\"val_2325\")\n",
       "             365 |  # node_Softmax_2307\n",
       "                    %\"val_2327\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_2326\") {axis=-1}\n",
       "             366 |  # node_scaled_dot_product_attention_4\n",
       "                    %\"scaled_dot_product_attention_4\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2327\", %\"transpose_18\")\n",
       "             367 |  # node_transpose_19\n",
       "                    %\"transpose_19\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(0, 2, 1, 3)}\n",
       "             368 |  # node_view_21\n",
       "                    %\"view_21\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_19\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             369 |  # node_MatMul_2314\n",
       "                    %\"val_2334\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_21\", %\"val_2333\"{...})\n",
       "             370 |  # node_linear_30\n",
       "                    %\"linear_30\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2334\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             371 |  # node_add_12\n",
       "                    %\"add_12\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_30\", %\"view_17\")\n",
       "             372 |  # node_layer_norm_10\n",
       "                    %\"layer_norm_10\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_12\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             373 |  # node_linear_31\n",
       "                    %\"linear_31\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_10\", %\"val_2337\"{...})\n",
       "             374 |  # node_linear_32\n",
       "                    %\"linear_32\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2338\"{...})\n",
       "             375 |  # node_linear_33\n",
       "                    %\"linear_33\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2339\"{...})\n",
       "             376 |  # node_view_22\n",
       "                    %\"view_22\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_31\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             377 |  # node_transpose_20\n",
       "                    %\"transpose_20\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
       "             378 |  # node_view_23\n",
       "                    %\"view_23\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_32\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             379 |  # node_transpose_21\n",
       "                    %\"transpose_21\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_23\") {perm=(0, 2, 1, 3)}\n",
       "             380 |  # node_view_24\n",
       "                    %\"view_24\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_33\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             381 |  # node_transpose_22\n",
       "                    %\"transpose_22\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_24\") {perm=(0, 2, 1, 3)}\n",
       "             382 |  # node_Reshape_2353\n",
       "                    %\"val_2375\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_21\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "             383 |  # node_Transpose_2354\n",
       "                    %\"val_2376\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_2375\") {perm=(0, 2, 1)}\n",
       "             384 |  # node_Reshape_2356\n",
       "                    %\"val_2378\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_2376\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "             385 |  # node_Mul_2358\n",
       "                    %\"val_2380\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_20\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             386 |  # node_Mul_2361\n",
       "                    %\"val_2383\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_2378\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             387 |  # node_MatMul_2362\n",
       "                    %\"val_2384\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_2380\", %\"val_2383\")\n",
       "             388 |  # node_Softmax_2363\n",
       "                    %\"val_2385\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_2384\") {axis=-1}\n",
       "             389 |  # node_scaled_dot_product_attention_5\n",
       "                    %\"scaled_dot_product_attention_5\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2385\", %\"transpose_22\")\n",
       "             390 |  # node_transpose_23\n",
       "                    %\"transpose_23\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(0, 2, 1, 3)}\n",
       "             391 |  # node_view_25\n",
       "                    %\"view_25\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_23\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             392 |  # node_MatMul_2370\n",
       "                    %\"val_2392\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_25\", %\"val_2391\"{...})\n",
       "             393 |  # node_linear_34\n",
       "                    %\"linear_34\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2392\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             394 |  # node_add_13\n",
       "                    %\"add_13\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_34\", %\"add_12\")\n",
       "             395 |  # node_layer_norm_11\n",
       "                    %\"layer_norm_11\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_13\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             396 |  # node_MatMul_2372\n",
       "                    %\"val_2396\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_11\", %\"val_2395\"{...})\n",
       "             397 |  # node_linear_35\n",
       "                    %\"linear_35\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_2396\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             398 |  # node_Split_6991\n",
       "                    %\"split_split_0\"<FLOAT,[8,1024,1280]>, %\"split_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_35\") {axis=2, num_outputs=2}\n",
       "             399 |  # node_gelu_6\n",
       "                    %\"gelu_6\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_split_1\") {approximate='none'}\n",
       "             400 |  # node_mul_3\n",
       "                    %\"mul_3\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_split_0\", %\"gelu_6\")\n",
       "             401 |  # node_MatMul_2375\n",
       "                    %\"val_2399\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_3\", %\"val_2398\"{...})\n",
       "             402 |  # node_linear_36\n",
       "                    %\"linear_36\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2399\", %\"unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             403 |  # node_add_14\n",
       "                    %\"add_14\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_36\", %\"add_13\")\n",
       "             404 |  # node_view_26\n",
       "                    %\"view_26\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_14\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "             405 |  # node_permute_2\n",
       "                    %\"permute_2\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_26\") {perm=(0, 3, 1, 2)}\n",
       "             406 |  # node_conv2d_4\n",
       "                    %\"conv2d_4\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_2\", %\"unet.down_blocks.0.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.0.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             407 |  # node_add_15\n",
       "                    %\"add_15\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_4\", %\"add_11\")\n",
       "             408 |  # node_Reshape_2386\n",
       "                    %\"val_2410\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_15\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             409 |  # node_InstanceNormalization_2393\n",
       "                    %\"val_2417\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2410\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             410 |  # node_Reshape_2395\n",
       "                    %\"val_2419\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2417\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             411 |  # node_Mul_2402\n",
       "                    %\"val_2426\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2419\", %\"val_2425\"{...})\n",
       "             412 |  # node_group_norm_3\n",
       "                    %\"group_norm_3\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2426\", %\"val_2427\"{...})\n",
       "             413 |  # node_Sigmoid_2404\n",
       "                    %\"val_2428\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_3\")\n",
       "             414 |  # node_silu_4\n",
       "                    %\"silu_4\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_3\", %\"val_2428\")\n",
       "             415 |  # node_conv2d_5\n",
       "                    %\"conv2d_5\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_4\", %\"unet.down_blocks.0.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             416 |  # node_linear_37\n",
       "                    %\"linear_37\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.0.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.0.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             417 |  # node_Unsqueeze_7722\n",
       "                    %\"unsqueeze_5\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_37\", %\"val_6521\"{[2, 3]})\n",
       "             418 |  # node_add_16\n",
       "                    %\"add_16\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_5\", %\"unsqueeze_5\")\n",
       "             419 |  # node_Reshape_2410\n",
       "                    %\"val_2434\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_16\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             420 |  # node_InstanceNormalization_2417\n",
       "                    %\"val_2441\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2434\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             421 |  # node_Reshape_2419\n",
       "                    %\"val_2443\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2441\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             422 |  # node_Mul_2426\n",
       "                    %\"val_2450\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2443\", %\"val_2449\"{...})\n",
       "             423 |  # node_group_norm_4\n",
       "                    %\"group_norm_4\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2450\", %\"val_2451\"{...})\n",
       "             424 |  # node_Sigmoid_2428\n",
       "                    %\"val_2452\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_4\")\n",
       "             425 |  # node_silu_6\n",
       "                    %\"silu_6\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_4\", %\"val_2452\")\n",
       "             426 |  # node_conv2d_6\n",
       "                    %\"conv2d_6\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_6\", %\"unet.down_blocks.0.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             427 |  # node_add_17\n",
       "                    %\"add_17\"<FLOAT,[8,320,32,32]>  ::Add(%\"add_15\", %\"conv2d_6\")\n",
       "             428 |  # node_Reshape_2433\n",
       "                    %\"val_2457\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_17\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             429 |  # node_InstanceNormalization_2440\n",
       "                    %\"val_2464\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_2457\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             430 |  # node_Reshape_2442\n",
       "                    %\"val_2466\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_2464\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "             431 |  # node_Mul_2449\n",
       "                    %\"val_2473\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_2466\", %\"val_2472\"{...})\n",
       "             432 |  # node_group_norm_5\n",
       "                    %\"group_norm_5\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_2473\", %\"val_2474\"{...})\n",
       "             433 |  # node_conv2d_7\n",
       "                    %\"conv2d_7\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_5\", %\"unet.down_blocks.0.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.0.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             434 |  # node_permute_3\n",
       "                    %\"permute_3\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_7\") {perm=(0, 2, 3, 1)}\n",
       "             435 |  # node_view_27\n",
       "                    %\"view_27\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_3\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "             436 |  # node_layer_norm_12\n",
       "                    %\"layer_norm_12\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_27\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             437 |  # node_linear_38\n",
       "                    %\"linear_38\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2482\"{...})\n",
       "             438 |  # node_linear_39\n",
       "                    %\"linear_39\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2483\"{...})\n",
       "             439 |  # node_linear_40\n",
       "                    %\"linear_40\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_12\", %\"val_2484\"{...})\n",
       "             440 |  # node_view_28\n",
       "                    %\"view_28\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_38\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             441 |  # node_transpose_24\n",
       "                    %\"transpose_24\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_28\") {perm=(0, 2, 1, 3)}\n",
       "             442 |  # node_view_29\n",
       "                    %\"view_29\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_39\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             443 |  # node_transpose_25\n",
       "                    %\"transpose_25\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_29\") {perm=(0, 2, 1, 3)}\n",
       "             444 |  # node_view_30\n",
       "                    %\"view_30\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_40\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             445 |  # node_transpose_26\n",
       "                    %\"transpose_26\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_30\") {perm=(0, 2, 1, 3)}\n",
       "             446 |  # node_Reshape_2494\n",
       "                    %\"val_2520\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_25\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "             447 |  # node_Transpose_2495\n",
       "                    %\"val_2521\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_2520\") {perm=(0, 2, 1)}\n",
       "             448 |  # node_Reshape_2497\n",
       "                    %\"val_2523\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_2521\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "             449 |  # node_Mul_2499\n",
       "                    %\"val_2525\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_24\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             450 |  # node_Mul_2502\n",
       "                    %\"val_2528\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_2523\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             451 |  # node_MatMul_2503\n",
       "                    %\"val_2529\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_2525\", %\"val_2528\")\n",
       "             452 |  # node_Softmax_2504\n",
       "                    %\"val_2530\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_2529\") {axis=-1}\n",
       "             453 |  # node_scaled_dot_product_attention_6\n",
       "                    %\"scaled_dot_product_attention_6\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2530\", %\"transpose_26\")\n",
       "             454 |  # node_transpose_27\n",
       "                    %\"transpose_27\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_6\") {perm=(0, 2, 1, 3)}\n",
       "             455 |  # node_view_31\n",
       "                    %\"view_31\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_27\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             456 |  # node_MatMul_2511\n",
       "                    %\"val_2537\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_31\", %\"val_2536\"{...})\n",
       "             457 |  # node_linear_41\n",
       "                    %\"linear_41\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2537\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             458 |  # node_add_18\n",
       "                    %\"add_18\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_41\", %\"view_27\")\n",
       "             459 |  # node_layer_norm_13\n",
       "                    %\"layer_norm_13\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_18\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             460 |  # node_linear_42\n",
       "                    %\"linear_42\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_13\", %\"val_2540\"{...})\n",
       "             461 |  # node_linear_43\n",
       "                    %\"linear_43\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2541\"{...})\n",
       "             462 |  # node_linear_44\n",
       "                    %\"linear_44\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_2542\"{...})\n",
       "             463 |  # node_view_32\n",
       "                    %\"view_32\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_42\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             464 |  # node_transpose_28\n",
       "                    %\"transpose_28\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_32\") {perm=(0, 2, 1, 3)}\n",
       "             465 |  # node_view_33\n",
       "                    %\"view_33\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_43\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             466 |  # node_transpose_29\n",
       "                    %\"transpose_29\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_33\") {perm=(0, 2, 1, 3)}\n",
       "             467 |  # node_view_34\n",
       "                    %\"view_34\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_44\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "             468 |  # node_transpose_30\n",
       "                    %\"transpose_30\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_34\") {perm=(0, 2, 1, 3)}\n",
       "             469 |  # node_Reshape_2550\n",
       "                    %\"val_2578\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_29\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "             470 |  # node_Transpose_2551\n",
       "                    %\"val_2579\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_2578\") {perm=(0, 2, 1)}\n",
       "             471 |  # node_Reshape_2553\n",
       "                    %\"val_2581\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_2579\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "             472 |  # node_Mul_2555\n",
       "                    %\"val_2583\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_28\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             473 |  # node_Mul_2558\n",
       "                    %\"val_2586\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_2581\", %\"val_2321\"{[0.3976353704929352]})\n",
       "             474 |  # node_MatMul_2559\n",
       "                    %\"val_2587\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_2583\", %\"val_2586\")\n",
       "             475 |  # node_Softmax_2560\n",
       "                    %\"val_2588\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_2587\") {axis=-1}\n",
       "             476 |  # node_scaled_dot_product_attention_7\n",
       "                    %\"scaled_dot_product_attention_7\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_2588\", %\"transpose_30\")\n",
       "             477 |  # node_transpose_31\n",
       "                    %\"transpose_31\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_7\") {perm=(0, 2, 1, 3)}\n",
       "             478 |  # node_view_35\n",
       "                    %\"view_35\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_31\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "             479 |  # node_MatMul_2567\n",
       "                    %\"val_2595\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_35\", %\"val_2594\"{...})\n",
       "             480 |  # node_linear_45\n",
       "                    %\"linear_45\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2595\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             481 |  # node_add_19\n",
       "                    %\"add_19\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_45\", %\"add_18\")\n",
       "             482 |  # node_layer_norm_14\n",
       "                    %\"layer_norm_14\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_19\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             483 |  # node_MatMul_2569\n",
       "                    %\"val_2599\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_14\", %\"val_2598\"{...})\n",
       "             484 |  # node_linear_46\n",
       "                    %\"linear_46\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_2599\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             485 |  # node_Split_7024\n",
       "                    %\"split_1_split_0\"<FLOAT,[8,1024,1280]>, %\"split_1_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_46\") {axis=2, num_outputs=2}\n",
       "             486 |  # node_gelu_7\n",
       "                    %\"gelu_7\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_1_split_1\") {approximate='none'}\n",
       "             487 |  # node_mul_4\n",
       "                    %\"mul_4\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_1_split_0\", %\"gelu_7\")\n",
       "             488 |  # node_MatMul_2571\n",
       "                    %\"val_2601\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_4\", %\"val_2600\"{...})\n",
       "             489 |  # node_linear_47\n",
       "                    %\"linear_47\"<FLOAT,[8,1024,320]>  ::Add(%\"val_2601\", %\"unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             490 |  # node_add_20\n",
       "                    %\"add_20\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_47\", %\"add_19\")\n",
       "             491 |  # node_view_36\n",
       "                    %\"view_36\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_20\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "             492 |  # node_permute_4\n",
       "                    %\"permute_4\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_36\") {perm=(0, 3, 1, 2)}\n",
       "             493 |  # node_conv2d_8\n",
       "                    %\"conv2d_8\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_4\", %\"unet.down_blocks.0.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.0.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             494 |  # node_add_21\n",
       "                    %\"add_21\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_8\", %\"add_17\")\n",
       "             495 |  # node_conv2d_9\n",
       "                    %\"conv2d_9\"<FLOAT,[8,320,16,16]>  ::Conv(%\"add_21\", %\"unet.down_blocks.0.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.0.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             496 |  # node_Reshape_2582\n",
       "                    %\"val_2612\"<FLOAT,[8,32,2560]>  ::Reshape(%\"conv2d_9\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             497 |  # node_InstanceNormalization_2589\n",
       "                    %\"val_2619\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_2612\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             498 |  # node_Reshape_2591\n",
       "                    %\"val_2621\"<FLOAT,[8,320,16,16]>  ::Reshape(%\"val_2619\", %\"val_2620\"{[8, 320, 16, 16]}) {allowzero=0}\n",
       "             499 |  # node_Mul_2598\n",
       "                    %\"val_2628\"<FLOAT,[8,320,16,16]>  ::Mul(%\"val_2621\", %\"val_2627\"{...})\n",
       "             500 |  # node_group_norm_6\n",
       "                    %\"group_norm_6\"<FLOAT,[8,320,16,16]>  ::Add(%\"val_2628\", %\"val_2629\"{...})\n",
       "             501 |  # node_Sigmoid_2600\n",
       "                    %\"val_2630\"<FLOAT,[8,320,16,16]>  ::Sigmoid(%\"group_norm_6\")\n",
       "             502 |  # node_silu_7\n",
       "                    %\"silu_7\"<FLOAT,[8,320,16,16]>  ::Mul(%\"group_norm_6\", %\"val_2630\")\n",
       "             503 |  # node_conv2d_10\n",
       "                    %\"conv2d_10\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_7\", %\"unet.down_blocks.1.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             504 |  # node_linear_48\n",
       "                    %\"linear_48\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.1.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.1.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             505 |  # node_Unsqueeze_7727\n",
       "                    %\"unsqueeze_7\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_48\", %\"val_6521\"{[2, 3]})\n",
       "             506 |  # node_add_22\n",
       "                    %\"add_22\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_10\", %\"unsqueeze_7\")\n",
       "             507 |  # node_Reshape_2606\n",
       "                    %\"val_2636\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_22\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             508 |  # node_InstanceNormalization_2613\n",
       "                    %\"val_2643\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2636\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             509 |  # node_Reshape_2615\n",
       "                    %\"val_2645\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2643\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             510 |  # node_Mul_2622\n",
       "                    %\"val_2652\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2645\", %\"val_2651\"{...})\n",
       "             511 |  # node_group_norm_7\n",
       "                    %\"group_norm_7\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2652\", %\"val_2653\"{...})\n",
       "             512 |  # node_Sigmoid_2624\n",
       "                    %\"val_2654\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_7\")\n",
       "             513 |  # node_silu_9\n",
       "                    %\"silu_9\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_7\", %\"val_2654\")\n",
       "             514 |  # node_conv2d_11\n",
       "                    %\"conv2d_11\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_9\", %\"unet.down_blocks.1.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             515 |  # node_conv2d_12\n",
       "                    %\"conv2d_12\"<FLOAT,[8,640,16,16]>  ::Conv(%\"conv2d_9\", %\"unet.down_blocks.1.resnets.0.conv_shortcut.weight\"{...}, %\"unet.down_blocks.1.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             516 |  # node_add_23\n",
       "                    %\"add_23\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_12\", %\"conv2d_11\")\n",
       "             517 |  # node_Reshape_2629\n",
       "                    %\"val_2659\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_23\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             518 |  # node_InstanceNormalization_2636\n",
       "                    %\"val_2666\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2659\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             519 |  # node_Reshape_2638\n",
       "                    %\"val_2668\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2666\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             520 |  # node_Mul_2645\n",
       "                    %\"val_2675\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2668\", %\"val_2674\"{...})\n",
       "             521 |  # node_group_norm_8\n",
       "                    %\"group_norm_8\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2675\", %\"val_2676\"{...})\n",
       "             522 |  # node_conv2d_13\n",
       "                    %\"conv2d_13\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_8\", %\"unet.down_blocks.1.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.1.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             523 |  # node_permute_5\n",
       "                    %\"permute_5\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_13\") {perm=(0, 2, 3, 1)}\n",
       "             524 |  # node_view_37\n",
       "                    %\"view_37\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_5\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "             525 |  # node_layer_norm_15\n",
       "                    %\"layer_norm_15\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_37\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             526 |  # node_linear_49\n",
       "                    %\"linear_49\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2684\"{...})\n",
       "             527 |  # node_linear_50\n",
       "                    %\"linear_50\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2685\"{...})\n",
       "             528 |  # node_linear_51\n",
       "                    %\"linear_51\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_15\", %\"val_2686\"{...})\n",
       "             529 |  # node_view_38\n",
       "                    %\"view_38\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_49\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             530 |  # node_transpose_32\n",
       "                    %\"transpose_32\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_38\") {perm=(0, 2, 1, 3)}\n",
       "             531 |  # node_view_39\n",
       "                    %\"view_39\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_50\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             532 |  # node_transpose_33\n",
       "                    %\"transpose_33\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_39\") {perm=(0, 2, 1, 3)}\n",
       "             533 |  # node_view_40\n",
       "                    %\"view_40\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_51\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             534 |  # node_transpose_34\n",
       "                    %\"transpose_34\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_40\") {perm=(0, 2, 1, 3)}\n",
       "             535 |  # node_Reshape_2690\n",
       "                    %\"val_2722\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_33\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "             536 |  # node_Transpose_2691\n",
       "                    %\"val_2723\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_2722\") {perm=(0, 2, 1)}\n",
       "             537 |  # node_Reshape_2693\n",
       "                    %\"val_2725\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_2723\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "             538 |  # node_Mul_2695\n",
       "                    %\"val_2727\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_32\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             539 |  # node_Mul_2698\n",
       "                    %\"val_2730\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_2725\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             540 |  # node_MatMul_2699\n",
       "                    %\"val_2731\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_2727\", %\"val_2730\")\n",
       "             541 |  # node_Softmax_2700\n",
       "                    %\"val_2732\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_2731\") {axis=-1}\n",
       "             542 |  # node_scaled_dot_product_attention_8\n",
       "                    %\"scaled_dot_product_attention_8\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2732\", %\"transpose_34\")\n",
       "             543 |  # node_transpose_35\n",
       "                    %\"transpose_35\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_8\") {perm=(0, 2, 1, 3)}\n",
       "             544 |  # node_view_41\n",
       "                    %\"view_41\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_35\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             545 |  # node_MatMul_2707\n",
       "                    %\"val_2739\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_41\", %\"val_2738\"{...})\n",
       "             546 |  # node_linear_52\n",
       "                    %\"linear_52\"<FLOAT,[8,256,640]>  ::Add(%\"val_2739\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             547 |  # node_add_24\n",
       "                    %\"add_24\"<FLOAT,[8,256,640]>  ::Add(%\"linear_52\", %\"view_37\")\n",
       "             548 |  # node_layer_norm_16\n",
       "                    %\"layer_norm_16\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_24\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             549 |  # node_linear_53\n",
       "                    %\"linear_53\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_16\", %\"val_2742\"{...})\n",
       "             550 |  # node_linear_54\n",
       "                    %\"linear_54\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2743\"{...})\n",
       "             551 |  # node_linear_55\n",
       "                    %\"linear_55\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2744\"{...})\n",
       "             552 |  # node_view_42\n",
       "                    %\"view_42\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_53\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             553 |  # node_transpose_36\n",
       "                    %\"transpose_36\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_42\") {perm=(0, 2, 1, 3)}\n",
       "             554 |  # node_view_43\n",
       "                    %\"view_43\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_54\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             555 |  # node_transpose_37\n",
       "                    %\"transpose_37\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_43\") {perm=(0, 2, 1, 3)}\n",
       "             556 |  # node_view_44\n",
       "                    %\"view_44\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_55\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             557 |  # node_transpose_38\n",
       "                    %\"transpose_38\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_44\") {perm=(0, 2, 1, 3)}\n",
       "             558 |  # node_Reshape_2746\n",
       "                    %\"val_2780\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_37\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "             559 |  # node_Transpose_2747\n",
       "                    %\"val_2781\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_2780\") {perm=(0, 2, 1)}\n",
       "             560 |  # node_Reshape_2749\n",
       "                    %\"val_2783\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_2781\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "             561 |  # node_Mul_2751\n",
       "                    %\"val_2785\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_36\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             562 |  # node_Mul_2754\n",
       "                    %\"val_2788\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_2783\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             563 |  # node_MatMul_2755\n",
       "                    %\"val_2789\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_2785\", %\"val_2788\")\n",
       "             564 |  # node_Softmax_2756\n",
       "                    %\"val_2790\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_2789\") {axis=-1}\n",
       "             565 |  # node_scaled_dot_product_attention_9\n",
       "                    %\"scaled_dot_product_attention_9\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2790\", %\"transpose_38\")\n",
       "             566 |  # node_transpose_39\n",
       "                    %\"transpose_39\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_9\") {perm=(0, 2, 1, 3)}\n",
       "             567 |  # node_view_45\n",
       "                    %\"view_45\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_39\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             568 |  # node_MatMul_2763\n",
       "                    %\"val_2797\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_45\", %\"val_2796\"{...})\n",
       "             569 |  # node_linear_56\n",
       "                    %\"linear_56\"<FLOAT,[8,256,640]>  ::Add(%\"val_2797\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             570 |  # node_add_25\n",
       "                    %\"add_25\"<FLOAT,[8,256,640]>  ::Add(%\"linear_56\", %\"add_24\")\n",
       "             571 |  # node_layer_norm_17\n",
       "                    %\"layer_norm_17\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_25\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             572 |  # node_MatMul_2765\n",
       "                    %\"val_2801\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_17\", %\"val_2800\"{...})\n",
       "             573 |  # node_linear_57\n",
       "                    %\"linear_57\"<FLOAT,[8,256,5120]>  ::Add(%\"val_2801\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             574 |  # node_Split_7057\n",
       "                    %\"split_2_split_0\"<FLOAT,[8,256,2560]>, %\"split_2_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_57\") {axis=2, num_outputs=2}\n",
       "             575 |  # node_gelu_8\n",
       "                    %\"gelu_8\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_2_split_1\") {approximate='none'}\n",
       "             576 |  # node_mul_5\n",
       "                    %\"mul_5\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_2_split_0\", %\"gelu_8\")\n",
       "             577 |  # node_MatMul_2768\n",
       "                    %\"val_2804\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_5\", %\"val_2803\"{...})\n",
       "             578 |  # node_linear_58\n",
       "                    %\"linear_58\"<FLOAT,[8,256,640]>  ::Add(%\"val_2804\", %\"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             579 |  # node_add_26\n",
       "                    %\"add_26\"<FLOAT,[8,256,640]>  ::Add(%\"linear_58\", %\"add_25\")\n",
       "             580 |  # node_view_46\n",
       "                    %\"view_46\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_26\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "             581 |  # node_permute_6\n",
       "                    %\"permute_6\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_46\") {perm=(0, 3, 1, 2)}\n",
       "             582 |  # node_conv2d_14\n",
       "                    %\"conv2d_14\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_6\", %\"unet.down_blocks.1.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.1.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             583 |  # node_add_27\n",
       "                    %\"add_27\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_14\", %\"add_23\")\n",
       "             584 |  # node_Reshape_2779\n",
       "                    %\"val_2815\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_27\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             585 |  # node_InstanceNormalization_2786\n",
       "                    %\"val_2822\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2815\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             586 |  # node_Reshape_2788\n",
       "                    %\"val_2824\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2822\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             587 |  # node_Mul_2795\n",
       "                    %\"val_2831\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2824\", %\"val_2830\"{...})\n",
       "             588 |  # node_group_norm_9\n",
       "                    %\"group_norm_9\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2831\", %\"val_2832\"{...})\n",
       "             589 |  # node_Sigmoid_2797\n",
       "                    %\"val_2833\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_9\")\n",
       "             590 |  # node_silu_10\n",
       "                    %\"silu_10\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_9\", %\"val_2833\")\n",
       "             591 |  # node_conv2d_15\n",
       "                    %\"conv2d_15\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_10\", %\"unet.down_blocks.1.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             592 |  # node_linear_59\n",
       "                    %\"linear_59\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.1.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.1.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             593 |  # node_Unsqueeze_7732\n",
       "                    %\"unsqueeze_9\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_59\", %\"val_6521\"{[2, 3]})\n",
       "             594 |  # node_add_28\n",
       "                    %\"add_28\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_15\", %\"unsqueeze_9\")\n",
       "             595 |  # node_Reshape_2803\n",
       "                    %\"val_2839\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_28\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             596 |  # node_InstanceNormalization_2810\n",
       "                    %\"val_2846\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2839\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             597 |  # node_Reshape_2812\n",
       "                    %\"val_2848\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2846\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             598 |  # node_Mul_2819\n",
       "                    %\"val_2855\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2848\", %\"val_2854\"{...})\n",
       "             599 |  # node_group_norm_10\n",
       "                    %\"group_norm_10\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2855\", %\"val_2856\"{...})\n",
       "             600 |  # node_Sigmoid_2821\n",
       "                    %\"val_2857\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_10\")\n",
       "             601 |  # node_silu_12\n",
       "                    %\"silu_12\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_10\", %\"val_2857\")\n",
       "             602 |  # node_conv2d_16\n",
       "                    %\"conv2d_16\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_12\", %\"unet.down_blocks.1.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             603 |  # node_add_29\n",
       "                    %\"add_29\"<FLOAT,[8,640,16,16]>  ::Add(%\"add_27\", %\"conv2d_16\")\n",
       "             604 |  # node_Reshape_2826\n",
       "                    %\"val_2862\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_29\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             605 |  # node_InstanceNormalization_2833\n",
       "                    %\"val_2869\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_2862\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             606 |  # node_Reshape_2835\n",
       "                    %\"val_2871\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_2869\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "             607 |  # node_Mul_2842\n",
       "                    %\"val_2878\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_2871\", %\"val_2877\"{...})\n",
       "             608 |  # node_group_norm_11\n",
       "                    %\"group_norm_11\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_2878\", %\"val_2879\"{...})\n",
       "             609 |  # node_conv2d_17\n",
       "                    %\"conv2d_17\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_11\", %\"unet.down_blocks.1.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.1.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             610 |  # node_permute_7\n",
       "                    %\"permute_7\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_17\") {perm=(0, 2, 3, 1)}\n",
       "             611 |  # node_view_47\n",
       "                    %\"view_47\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_7\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "             612 |  # node_layer_norm_18\n",
       "                    %\"layer_norm_18\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_47\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             613 |  # node_linear_60\n",
       "                    %\"linear_60\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2887\"{...})\n",
       "             614 |  # node_linear_61\n",
       "                    %\"linear_61\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2888\"{...})\n",
       "             615 |  # node_linear_62\n",
       "                    %\"linear_62\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_18\", %\"val_2889\"{...})\n",
       "             616 |  # node_view_48\n",
       "                    %\"view_48\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_60\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             617 |  # node_transpose_40\n",
       "                    %\"transpose_40\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_48\") {perm=(0, 2, 1, 3)}\n",
       "             618 |  # node_view_49\n",
       "                    %\"view_49\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_61\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             619 |  # node_transpose_41\n",
       "                    %\"transpose_41\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_49\") {perm=(0, 2, 1, 3)}\n",
       "             620 |  # node_view_50\n",
       "                    %\"view_50\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_62\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             621 |  # node_transpose_42\n",
       "                    %\"transpose_42\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_50\") {perm=(0, 2, 1, 3)}\n",
       "             622 |  # node_Reshape_2887\n",
       "                    %\"val_2925\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_41\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "             623 |  # node_Transpose_2888\n",
       "                    %\"val_2926\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_2925\") {perm=(0, 2, 1)}\n",
       "             624 |  # node_Reshape_2890\n",
       "                    %\"val_2928\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_2926\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "             625 |  # node_Mul_2892\n",
       "                    %\"val_2930\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_40\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             626 |  # node_Mul_2895\n",
       "                    %\"val_2933\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_2928\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             627 |  # node_MatMul_2896\n",
       "                    %\"val_2934\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_2930\", %\"val_2933\")\n",
       "             628 |  # node_Softmax_2897\n",
       "                    %\"val_2935\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_2934\") {axis=-1}\n",
       "             629 |  # node_scaled_dot_product_attention_10\n",
       "                    %\"scaled_dot_product_attention_10\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2935\", %\"transpose_42\")\n",
       "             630 |  # node_transpose_43\n",
       "                    %\"transpose_43\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_10\") {perm=(0, 2, 1, 3)}\n",
       "             631 |  # node_view_51\n",
       "                    %\"view_51\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_43\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             632 |  # node_MatMul_2904\n",
       "                    %\"val_2942\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_51\", %\"val_2941\"{...})\n",
       "             633 |  # node_linear_63\n",
       "                    %\"linear_63\"<FLOAT,[8,256,640]>  ::Add(%\"val_2942\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             634 |  # node_add_30\n",
       "                    %\"add_30\"<FLOAT,[8,256,640]>  ::Add(%\"linear_63\", %\"view_47\")\n",
       "             635 |  # node_layer_norm_19\n",
       "                    %\"layer_norm_19\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_30\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             636 |  # node_linear_64\n",
       "                    %\"linear_64\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_19\", %\"val_2945\"{...})\n",
       "             637 |  # node_linear_65\n",
       "                    %\"linear_65\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2946\"{...})\n",
       "             638 |  # node_linear_66\n",
       "                    %\"linear_66\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_2947\"{...})\n",
       "             639 |  # node_view_52\n",
       "                    %\"view_52\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_64\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             640 |  # node_transpose_44\n",
       "                    %\"transpose_44\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_52\") {perm=(0, 2, 1, 3)}\n",
       "             641 |  # node_view_53\n",
       "                    %\"view_53\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_65\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             642 |  # node_transpose_45\n",
       "                    %\"transpose_45\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_53\") {perm=(0, 2, 1, 3)}\n",
       "             643 |  # node_view_54\n",
       "                    %\"view_54\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_66\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "             644 |  # node_transpose_46\n",
       "                    %\"transpose_46\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_54\") {perm=(0, 2, 1, 3)}\n",
       "             645 |  # node_Reshape_2943\n",
       "                    %\"val_2983\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_45\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "             646 |  # node_Transpose_2944\n",
       "                    %\"val_2984\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_2983\") {perm=(0, 2, 1)}\n",
       "             647 |  # node_Reshape_2946\n",
       "                    %\"val_2986\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_2984\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "             648 |  # node_Mul_2948\n",
       "                    %\"val_2988\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_44\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             649 |  # node_Mul_2951\n",
       "                    %\"val_2991\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_2986\", %\"val_2726\"{[0.33437013626098633]})\n",
       "             650 |  # node_MatMul_2952\n",
       "                    %\"val_2992\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_2988\", %\"val_2991\")\n",
       "             651 |  # node_Softmax_2953\n",
       "                    %\"val_2993\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_2992\") {axis=-1}\n",
       "             652 |  # node_scaled_dot_product_attention_11\n",
       "                    %\"scaled_dot_product_attention_11\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_2993\", %\"transpose_46\")\n",
       "             653 |  # node_transpose_47\n",
       "                    %\"transpose_47\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_11\") {perm=(0, 2, 1, 3)}\n",
       "             654 |  # node_view_55\n",
       "                    %\"view_55\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_47\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "             655 |  # node_MatMul_2960\n",
       "                    %\"val_3000\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_55\", %\"val_2999\"{...})\n",
       "             656 |  # node_linear_67\n",
       "                    %\"linear_67\"<FLOAT,[8,256,640]>  ::Add(%\"val_3000\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             657 |  # node_add_31\n",
       "                    %\"add_31\"<FLOAT,[8,256,640]>  ::Add(%\"linear_67\", %\"add_30\")\n",
       "             658 |  # node_layer_norm_20\n",
       "                    %\"layer_norm_20\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_31\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             659 |  # node_MatMul_2962\n",
       "                    %\"val_3004\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_20\", %\"val_3003\"{...})\n",
       "             660 |  # node_linear_68\n",
       "                    %\"linear_68\"<FLOAT,[8,256,5120]>  ::Add(%\"val_3004\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             661 |  # node_Split_7090\n",
       "                    %\"split_3_split_0\"<FLOAT,[8,256,2560]>, %\"split_3_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_68\") {axis=2, num_outputs=2}\n",
       "             662 |  # node_gelu_9\n",
       "                    %\"gelu_9\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_3_split_1\") {approximate='none'}\n",
       "             663 |  # node_mul_6\n",
       "                    %\"mul_6\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_3_split_0\", %\"gelu_9\")\n",
       "             664 |  # node_MatMul_2964\n",
       "                    %\"val_3006\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_6\", %\"val_3005\"{...})\n",
       "             665 |  # node_linear_69\n",
       "                    %\"linear_69\"<FLOAT,[8,256,640]>  ::Add(%\"val_3006\", %\"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             666 |  # node_add_32\n",
       "                    %\"add_32\"<FLOAT,[8,256,640]>  ::Add(%\"linear_69\", %\"add_31\")\n",
       "             667 |  # node_view_56\n",
       "                    %\"view_56\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_32\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "             668 |  # node_permute_8\n",
       "                    %\"permute_8\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_56\") {perm=(0, 3, 1, 2)}\n",
       "             669 |  # node_conv2d_18\n",
       "                    %\"conv2d_18\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_8\", %\"unet.down_blocks.1.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.1.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             670 |  # node_add_33\n",
       "                    %\"add_33\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_18\", %\"add_29\")\n",
       "             671 |  # node_conv2d_19\n",
       "                    %\"conv2d_19\"<FLOAT,[8,640,8,8]>  ::Conv(%\"add_33\", %\"unet.down_blocks.1.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.1.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             672 |  # node_Reshape_2975\n",
       "                    %\"val_3017\"<FLOAT,[8,32,1280]>  ::Reshape(%\"conv2d_19\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             673 |  # node_InstanceNormalization_2982\n",
       "                    %\"val_3024\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3017\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             674 |  # node_Reshape_2984\n",
       "                    %\"val_3026\"<FLOAT,[8,640,8,8]>  ::Reshape(%\"val_3024\", %\"val_3025\"{[8, 640, 8, 8]}) {allowzero=0}\n",
       "             675 |  # node_Mul_2991\n",
       "                    %\"val_3033\"<FLOAT,[8,640,8,8]>  ::Mul(%\"val_3026\", %\"val_3032\"{...})\n",
       "             676 |  # node_group_norm_12\n",
       "                    %\"group_norm_12\"<FLOAT,[8,640,8,8]>  ::Add(%\"val_3033\", %\"val_3034\"{...})\n",
       "             677 |  # node_Sigmoid_2993\n",
       "                    %\"val_3035\"<FLOAT,[8,640,8,8]>  ::Sigmoid(%\"group_norm_12\")\n",
       "             678 |  # node_silu_13\n",
       "                    %\"silu_13\"<FLOAT,[8,640,8,8]>  ::Mul(%\"group_norm_12\", %\"val_3035\")\n",
       "             679 |  # node_conv2d_20\n",
       "                    %\"conv2d_20\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_13\", %\"unet.down_blocks.2.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             680 |  # node_linear_70\n",
       "                    %\"linear_70\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.2.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.2.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             681 |  # node_Unsqueeze_7737\n",
       "                    %\"unsqueeze_11\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_70\", %\"val_6521\"{[2, 3]})\n",
       "             682 |  # node_add_34\n",
       "                    %\"add_34\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_20\", %\"unsqueeze_11\")\n",
       "             683 |  # node_Reshape_2999\n",
       "                    %\"val_3041\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_34\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             684 |  # node_InstanceNormalization_3006\n",
       "                    %\"val_3048\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3041\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             685 |  # node_Reshape_3008\n",
       "                    %\"val_3050\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3048\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             686 |  # node_Mul_3015\n",
       "                    %\"val_3057\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3050\", %\"val_3056\"{...})\n",
       "             687 |  # node_group_norm_13\n",
       "                    %\"group_norm_13\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3057\", %\"val_3058\"{...})\n",
       "             688 |  # node_Sigmoid_3017\n",
       "                    %\"val_3059\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_13\")\n",
       "             689 |  # node_silu_15\n",
       "                    %\"silu_15\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_13\", %\"val_3059\")\n",
       "             690 |  # node_conv2d_21\n",
       "                    %\"conv2d_21\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_15\", %\"unet.down_blocks.2.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             691 |  # node_conv2d_22\n",
       "                    %\"conv2d_22\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"conv2d_19\", %\"unet.down_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"unet.down_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             692 |  # node_add_35\n",
       "                    %\"add_35\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_22\", %\"conv2d_21\")\n",
       "             693 |  # node_Reshape_3022\n",
       "                    %\"val_3064\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_35\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             694 |  # node_InstanceNormalization_3029\n",
       "                    %\"val_3071\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3064\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             695 |  # node_Reshape_3031\n",
       "                    %\"val_3073\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3071\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             696 |  # node_Mul_3038\n",
       "                    %\"val_3080\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3073\", %\"val_3079\"{...})\n",
       "             697 |  # node_group_norm_14\n",
       "                    %\"group_norm_14\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3080\", %\"val_3081\"{...})\n",
       "             698 |  # node_conv2d_23\n",
       "                    %\"conv2d_23\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_14\", %\"unet.down_blocks.2.attentions.0.proj_in.weight\"{...}, %\"unet.down_blocks.2.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             699 |  # node_permute_9\n",
       "                    %\"permute_9\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_23\") {perm=(0, 2, 3, 1)}\n",
       "             700 |  # node_view_57\n",
       "                    %\"view_57\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_9\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "             701 |  # node_layer_norm_21\n",
       "                    %\"layer_norm_21\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_57\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             702 |  # node_linear_71\n",
       "                    %\"linear_71\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3089\"{...})\n",
       "             703 |  # node_linear_72\n",
       "                    %\"linear_72\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3090\"{...})\n",
       "             704 |  # node_linear_73\n",
       "                    %\"linear_73\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_21\", %\"val_3091\"{...})\n",
       "             705 |  # node_view_58\n",
       "                    %\"view_58\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_71\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             706 |  # node_transpose_48\n",
       "                    %\"transpose_48\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_58\") {perm=(0, 2, 1, 3)}\n",
       "             707 |  # node_view_59\n",
       "                    %\"view_59\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_72\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             708 |  # node_transpose_49\n",
       "                    %\"transpose_49\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_59\") {perm=(0, 2, 1, 3)}\n",
       "             709 |  # node_view_60\n",
       "                    %\"view_60\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_73\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             710 |  # node_transpose_50\n",
       "                    %\"transpose_50\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_60\") {perm=(0, 2, 1, 3)}\n",
       "             711 |  # node_Reshape_3083\n",
       "                    %\"val_3127\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_49\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "             712 |  # node_Transpose_3084\n",
       "                    %\"val_3128\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_3127\") {perm=(0, 2, 1)}\n",
       "             713 |  # node_Reshape_3086\n",
       "                    %\"val_3130\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_3128\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "             714 |  # node_Mul_3088\n",
       "                    %\"val_3132\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_48\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             715 |  # node_Mul_3091\n",
       "                    %\"val_3135\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_3130\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             716 |  # node_MatMul_3092\n",
       "                    %\"val_3136\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_3132\", %\"val_3135\")\n",
       "             717 |  # node_Softmax_3093\n",
       "                    %\"val_3137\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_3136\") {axis=-1}\n",
       "             718 |  # node_scaled_dot_product_attention_12\n",
       "                    %\"scaled_dot_product_attention_12\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3137\", %\"transpose_50\")\n",
       "             719 |  # node_transpose_51\n",
       "                    %\"transpose_51\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_12\") {perm=(0, 2, 1, 3)}\n",
       "             720 |  # node_view_61\n",
       "                    %\"view_61\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_51\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             721 |  # node_MatMul_3100\n",
       "                    %\"val_3144\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_61\", %\"val_3143\"{...})\n",
       "             722 |  # node_linear_74\n",
       "                    %\"linear_74\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3144\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             723 |  # node_add_36\n",
       "                    %\"add_36\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_74\", %\"view_57\")\n",
       "             724 |  # node_layer_norm_22\n",
       "                    %\"layer_norm_22\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_36\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             725 |  # node_linear_75\n",
       "                    %\"linear_75\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_22\", %\"val_3147\"{...})\n",
       "             726 |  # node_linear_76\n",
       "                    %\"linear_76\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3148\"{...})\n",
       "             727 |  # node_linear_77\n",
       "                    %\"linear_77\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3149\"{...})\n",
       "             728 |  # node_view_62\n",
       "                    %\"view_62\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_75\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             729 |  # node_transpose_52\n",
       "                    %\"transpose_52\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_62\") {perm=(0, 2, 1, 3)}\n",
       "             730 |  # node_view_63\n",
       "                    %\"view_63\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_76\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             731 |  # node_transpose_53\n",
       "                    %\"transpose_53\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_63\") {perm=(0, 2, 1, 3)}\n",
       "             732 |  # node_view_64\n",
       "                    %\"view_64\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_77\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             733 |  # node_transpose_54\n",
       "                    %\"transpose_54\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_64\") {perm=(0, 2, 1, 3)}\n",
       "             734 |  # node_Reshape_3139\n",
       "                    %\"val_3185\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_53\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             735 |  # node_Transpose_3140\n",
       "                    %\"val_3186\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3185\") {perm=(0, 2, 1)}\n",
       "             736 |  # node_Reshape_3142\n",
       "                    %\"val_3188\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3186\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             737 |  # node_Mul_3144\n",
       "                    %\"val_3190\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_52\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             738 |  # node_Mul_3147\n",
       "                    %\"val_3193\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3188\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             739 |  # node_MatMul_3148\n",
       "                    %\"val_3194\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_3190\", %\"val_3193\")\n",
       "             740 |  # node_Softmax_3149\n",
       "                    %\"val_3195\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_3194\") {axis=-1}\n",
       "             741 |  # node_scaled_dot_product_attention_13\n",
       "                    %\"scaled_dot_product_attention_13\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3195\", %\"transpose_54\")\n",
       "             742 |  # node_transpose_55\n",
       "                    %\"transpose_55\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_13\") {perm=(0, 2, 1, 3)}\n",
       "             743 |  # node_view_65\n",
       "                    %\"view_65\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_55\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             744 |  # node_MatMul_3156\n",
       "                    %\"val_3202\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_65\", %\"val_3201\"{...})\n",
       "             745 |  # node_linear_78\n",
       "                    %\"linear_78\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3202\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             746 |  # node_add_37\n",
       "                    %\"add_37\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_78\", %\"add_36\")\n",
       "             747 |  # node_layer_norm_23\n",
       "                    %\"layer_norm_23\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_37\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             748 |  # node_MatMul_3158\n",
       "                    %\"val_3206\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_23\", %\"val_3205\"{...})\n",
       "             749 |  # node_linear_79\n",
       "                    %\"linear_79\"<FLOAT,[8,64,10240]>  ::Add(%\"val_3206\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             750 |  # node_Split_7123\n",
       "                    %\"split_4_split_0\"<FLOAT,[8,64,5120]>, %\"split_4_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_79\") {axis=2, num_outputs=2}\n",
       "             751 |  # node_gelu_10\n",
       "                    %\"gelu_10\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_4_split_1\") {approximate='none'}\n",
       "             752 |  # node_mul_7\n",
       "                    %\"mul_7\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_4_split_0\", %\"gelu_10\")\n",
       "             753 |  # node_MatMul_3161\n",
       "                    %\"val_3209\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_7\", %\"val_3208\"{...})\n",
       "             754 |  # node_linear_80\n",
       "                    %\"linear_80\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3209\", %\"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             755 |  # node_add_38\n",
       "                    %\"add_38\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_80\", %\"add_37\")\n",
       "             756 |  # node_view_66\n",
       "                    %\"view_66\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_38\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "             757 |  # node_permute_10\n",
       "                    %\"permute_10\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_66\") {perm=(0, 3, 1, 2)}\n",
       "             758 |  # node_conv2d_24\n",
       "                    %\"conv2d_24\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_10\", %\"unet.down_blocks.2.attentions.0.proj_out.weight\"{...}, %\"unet.down_blocks.2.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             759 |  # node_add_39\n",
       "                    %\"add_39\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_24\", %\"add_35\")\n",
       "             760 |  # node_Reshape_3172\n",
       "                    %\"val_3220\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_39\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             761 |  # node_InstanceNormalization_3179\n",
       "                    %\"val_3227\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3220\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             762 |  # node_Reshape_3181\n",
       "                    %\"val_3229\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3227\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             763 |  # node_Mul_3188\n",
       "                    %\"val_3236\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3229\", %\"val_3235\"{...})\n",
       "             764 |  # node_group_norm_15\n",
       "                    %\"group_norm_15\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3236\", %\"val_3237\"{...})\n",
       "             765 |  # node_Sigmoid_3190\n",
       "                    %\"val_3238\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_15\")\n",
       "             766 |  # node_silu_16\n",
       "                    %\"silu_16\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_15\", %\"val_3238\")\n",
       "             767 |  # node_conv2d_25\n",
       "                    %\"conv2d_25\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_16\", %\"unet.down_blocks.2.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             768 |  # node_linear_81\n",
       "                    %\"linear_81\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.2.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.2.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             769 |  # node_Unsqueeze_7742\n",
       "                    %\"unsqueeze_13\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_81\", %\"val_6521\"{[2, 3]})\n",
       "             770 |  # node_add_40\n",
       "                    %\"add_40\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_25\", %\"unsqueeze_13\")\n",
       "             771 |  # node_Reshape_3196\n",
       "                    %\"val_3244\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_40\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             772 |  # node_InstanceNormalization_3203\n",
       "                    %\"val_3251\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3244\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             773 |  # node_Reshape_3205\n",
       "                    %\"val_3253\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3251\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             774 |  # node_Mul_3212\n",
       "                    %\"val_3260\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3253\", %\"val_3259\"{...})\n",
       "             775 |  # node_group_norm_16\n",
       "                    %\"group_norm_16\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3260\", %\"val_3261\"{...})\n",
       "             776 |  # node_Sigmoid_3214\n",
       "                    %\"val_3262\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_16\")\n",
       "             777 |  # node_silu_18\n",
       "                    %\"silu_18\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_16\", %\"val_3262\")\n",
       "             778 |  # node_conv2d_26\n",
       "                    %\"conv2d_26\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_18\", %\"unet.down_blocks.2.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             779 |  # node_add_41\n",
       "                    %\"add_41\"<FLOAT,[8,1280,8,8]>  ::Add(%\"add_39\", %\"conv2d_26\")\n",
       "             780 |  # node_Reshape_3219\n",
       "                    %\"val_3267\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_41\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             781 |  # node_InstanceNormalization_3226\n",
       "                    %\"val_3274\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3267\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             782 |  # node_Reshape_3228\n",
       "                    %\"val_3276\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3274\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "             783 |  # node_Mul_3235\n",
       "                    %\"val_3283\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3276\", %\"val_3282\"{...})\n",
       "             784 |  # node_group_norm_17\n",
       "                    %\"group_norm_17\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3283\", %\"val_3284\"{...})\n",
       "             785 |  # node_conv2d_27\n",
       "                    %\"conv2d_27\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_17\", %\"unet.down_blocks.2.attentions.1.proj_in.weight\"{...}, %\"unet.down_blocks.2.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             786 |  # node_permute_11\n",
       "                    %\"permute_11\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_27\") {perm=(0, 2, 3, 1)}\n",
       "             787 |  # node_view_67\n",
       "                    %\"view_67\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_11\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "             788 |  # node_layer_norm_24\n",
       "                    %\"layer_norm_24\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_67\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             789 |  # node_linear_82\n",
       "                    %\"linear_82\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3292\"{...})\n",
       "             790 |  # node_linear_83\n",
       "                    %\"linear_83\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3293\"{...})\n",
       "             791 |  # node_linear_84\n",
       "                    %\"linear_84\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_24\", %\"val_3294\"{...})\n",
       "             792 |  # node_view_68\n",
       "                    %\"view_68\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_82\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             793 |  # node_transpose_56\n",
       "                    %\"transpose_56\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_68\") {perm=(0, 2, 1, 3)}\n",
       "             794 |  # node_view_69\n",
       "                    %\"view_69\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_83\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             795 |  # node_transpose_57\n",
       "                    %\"transpose_57\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_69\") {perm=(0, 2, 1, 3)}\n",
       "             796 |  # node_view_70\n",
       "                    %\"view_70\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_84\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             797 |  # node_transpose_58\n",
       "                    %\"transpose_58\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_70\") {perm=(0, 2, 1, 3)}\n",
       "             798 |  # node_Reshape_3280\n",
       "                    %\"val_3330\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_57\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "             799 |  # node_Transpose_3281\n",
       "                    %\"val_3331\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_3330\") {perm=(0, 2, 1)}\n",
       "             800 |  # node_Reshape_3283\n",
       "                    %\"val_3333\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_3331\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "             801 |  # node_Mul_3285\n",
       "                    %\"val_3335\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_56\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             802 |  # node_Mul_3288\n",
       "                    %\"val_3338\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_3333\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             803 |  # node_MatMul_3289\n",
       "                    %\"val_3339\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_3335\", %\"val_3338\")\n",
       "             804 |  # node_Softmax_3290\n",
       "                    %\"val_3340\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_3339\") {axis=-1}\n",
       "             805 |  # node_scaled_dot_product_attention_14\n",
       "                    %\"scaled_dot_product_attention_14\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3340\", %\"transpose_58\")\n",
       "             806 |  # node_transpose_59\n",
       "                    %\"transpose_59\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_14\") {perm=(0, 2, 1, 3)}\n",
       "             807 |  # node_view_71\n",
       "                    %\"view_71\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_59\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             808 |  # node_MatMul_3297\n",
       "                    %\"val_3347\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_71\", %\"val_3346\"{...})\n",
       "             809 |  # node_linear_85\n",
       "                    %\"linear_85\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3347\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             810 |  # node_add_42\n",
       "                    %\"add_42\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_85\", %\"view_67\")\n",
       "             811 |  # node_layer_norm_25\n",
       "                    %\"layer_norm_25\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_42\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             812 |  # node_linear_86\n",
       "                    %\"linear_86\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_25\", %\"val_3350\"{...})\n",
       "             813 |  # node_linear_87\n",
       "                    %\"linear_87\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3351\"{...})\n",
       "             814 |  # node_linear_88\n",
       "                    %\"linear_88\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3352\"{...})\n",
       "             815 |  # node_view_72\n",
       "                    %\"view_72\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_86\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             816 |  # node_transpose_60\n",
       "                    %\"transpose_60\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_72\") {perm=(0, 2, 1, 3)}\n",
       "             817 |  # node_view_73\n",
       "                    %\"view_73\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_87\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             818 |  # node_transpose_61\n",
       "                    %\"transpose_61\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_73\") {perm=(0, 2, 1, 3)}\n",
       "             819 |  # node_view_74\n",
       "                    %\"view_74\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_88\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             820 |  # node_transpose_62\n",
       "                    %\"transpose_62\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_74\") {perm=(0, 2, 1, 3)}\n",
       "             821 |  # node_Reshape_3336\n",
       "                    %\"val_3388\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_61\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             822 |  # node_Transpose_3337\n",
       "                    %\"val_3389\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3388\") {perm=(0, 2, 1)}\n",
       "             823 |  # node_Reshape_3339\n",
       "                    %\"val_3391\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3389\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             824 |  # node_Mul_3341\n",
       "                    %\"val_3393\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_60\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             825 |  # node_Mul_3344\n",
       "                    %\"val_3396\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3391\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             826 |  # node_MatMul_3345\n",
       "                    %\"val_3397\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_3393\", %\"val_3396\")\n",
       "             827 |  # node_Softmax_3346\n",
       "                    %\"val_3398\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_3397\") {axis=-1}\n",
       "             828 |  # node_scaled_dot_product_attention_15\n",
       "                    %\"scaled_dot_product_attention_15\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_3398\", %\"transpose_62\")\n",
       "             829 |  # node_transpose_63\n",
       "                    %\"transpose_63\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_15\") {perm=(0, 2, 1, 3)}\n",
       "             830 |  # node_view_75\n",
       "                    %\"view_75\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_63\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             831 |  # node_MatMul_3353\n",
       "                    %\"val_3405\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_75\", %\"val_3404\"{...})\n",
       "             832 |  # node_linear_89\n",
       "                    %\"linear_89\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3405\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             833 |  # node_add_43\n",
       "                    %\"add_43\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_89\", %\"add_42\")\n",
       "             834 |  # node_layer_norm_26\n",
       "                    %\"layer_norm_26\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_43\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             835 |  # node_MatMul_3355\n",
       "                    %\"val_3409\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_26\", %\"val_3408\"{...})\n",
       "             836 |  # node_linear_90\n",
       "                    %\"linear_90\"<FLOAT,[8,64,10240]>  ::Add(%\"val_3409\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             837 |  # node_Split_7156\n",
       "                    %\"split_5_split_0\"<FLOAT,[8,64,5120]>, %\"split_5_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_90\") {axis=2, num_outputs=2}\n",
       "             838 |  # node_gelu_11\n",
       "                    %\"gelu_11\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_5_split_1\") {approximate='none'}\n",
       "             839 |  # node_mul_8\n",
       "                    %\"mul_8\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_5_split_0\", %\"gelu_11\")\n",
       "             840 |  # node_MatMul_3357\n",
       "                    %\"val_3411\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_8\", %\"val_3410\"{...})\n",
       "             841 |  # node_linear_91\n",
       "                    %\"linear_91\"<FLOAT,[8,64,1280]>  ::Add(%\"val_3411\", %\"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             842 |  # node_add_44\n",
       "                    %\"add_44\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_91\", %\"add_43\")\n",
       "             843 |  # node_view_76\n",
       "                    %\"view_76\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_44\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "             844 |  # node_permute_12\n",
       "                    %\"permute_12\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_76\") {perm=(0, 3, 1, 2)}\n",
       "             845 |  # node_conv2d_28\n",
       "                    %\"conv2d_28\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_12\", %\"unet.down_blocks.2.attentions.1.proj_out.weight\"{...}, %\"unet.down_blocks.2.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             846 |  # node_add_45\n",
       "                    %\"add_45\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_28\", %\"add_41\")\n",
       "             847 |  # node_conv2d_29\n",
       "                    %\"conv2d_29\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"add_45\", %\"unet.down_blocks.2.downsamplers.0.conv.weight\"{...}, %\"unet.down_blocks.2.downsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             848 |  # node_Reshape_3368\n",
       "                    %\"val_3422\"<FLOAT,[8,32,640]>  ::Reshape(%\"conv2d_29\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             849 |  # node_InstanceNormalization_3375\n",
       "                    %\"val_3429\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3422\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             850 |  # node_Reshape_3377\n",
       "                    %\"val_3431\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3429\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             851 |  # node_Mul_3384\n",
       "                    %\"val_3438\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3431\", %\"val_3437\"{...})\n",
       "             852 |  # node_group_norm_18\n",
       "                    %\"group_norm_18\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3438\", %\"val_3439\"{...})\n",
       "             853 |  # node_Sigmoid_3386\n",
       "                    %\"val_3440\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_18\")\n",
       "             854 |  # node_silu_19\n",
       "                    %\"silu_19\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_18\", %\"val_3440\")\n",
       "             855 |  # node_conv2d_30\n",
       "                    %\"conv2d_30\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_19\", %\"unet.down_blocks.3.resnets.0.conv1.weight\"{...}, %\"unet.down_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             856 |  # node_linear_92\n",
       "                    %\"linear_92\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.3.resnets.0.time_emb_proj.weight\"{...}, %\"unet.down_blocks.3.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             857 |  # node_Unsqueeze_7747\n",
       "                    %\"unsqueeze_15\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_92\", %\"val_6521\"{[2, 3]})\n",
       "             858 |  # node_add_46\n",
       "                    %\"add_46\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_30\", %\"unsqueeze_15\")\n",
       "             859 |  # node_Reshape_3392\n",
       "                    %\"val_3446\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_46\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             860 |  # node_InstanceNormalization_3399\n",
       "                    %\"val_3453\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3446\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             861 |  # node_Reshape_3401\n",
       "                    %\"val_3455\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3453\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             862 |  # node_Mul_3408\n",
       "                    %\"val_3462\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3455\", %\"val_3461\"{...})\n",
       "             863 |  # node_group_norm_19\n",
       "                    %\"group_norm_19\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3462\", %\"val_3463\"{...})\n",
       "             864 |  # node_Sigmoid_3410\n",
       "                    %\"val_3464\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_19\")\n",
       "             865 |  # node_silu_21\n",
       "                    %\"silu_21\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_19\", %\"val_3464\")\n",
       "             866 |  # node_conv2d_31\n",
       "                    %\"conv2d_31\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_21\", %\"unet.down_blocks.3.resnets.0.conv2.weight\"{...}, %\"unet.down_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             867 |  # node_add_47\n",
       "                    %\"add_47\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_29\", %\"conv2d_31\")\n",
       "             868 |  # node_Reshape_3415\n",
       "                    %\"val_3469\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_47\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             869 |  # node_InstanceNormalization_3422\n",
       "                    %\"val_3476\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3469\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             870 |  # node_Reshape_3424\n",
       "                    %\"val_3478\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3476\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             871 |  # node_Mul_3431\n",
       "                    %\"val_3485\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3478\", %\"val_3484\"{...})\n",
       "             872 |  # node_group_norm_20\n",
       "                    %\"group_norm_20\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3485\", %\"val_3486\"{...})\n",
       "             873 |  # node_Sigmoid_3433\n",
       "                    %\"val_3487\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_20\")\n",
       "             874 |  # node_silu_22\n",
       "                    %\"silu_22\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_20\", %\"val_3487\")\n",
       "             875 |  # node_conv2d_32\n",
       "                    %\"conv2d_32\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_22\", %\"unet.down_blocks.3.resnets.1.conv1.weight\"{...}, %\"unet.down_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             876 |  # node_linear_93\n",
       "                    %\"linear_93\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.down_blocks.3.resnets.1.time_emb_proj.weight\"{...}, %\"unet.down_blocks.3.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             877 |  # node_Unsqueeze_7750\n",
       "                    %\"unsqueeze_17\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_93\", %\"val_6521\"{[2, 3]})\n",
       "             878 |  # node_add_48\n",
       "                    %\"add_48\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_32\", %\"unsqueeze_17\")\n",
       "             879 |  # node_Reshape_3439\n",
       "                    %\"val_3493\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_48\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             880 |  # node_InstanceNormalization_3446\n",
       "                    %\"val_3500\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3493\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             881 |  # node_Reshape_3448\n",
       "                    %\"val_3502\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3500\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             882 |  # node_Mul_3455\n",
       "                    %\"val_3509\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3502\", %\"val_3508\"{...})\n",
       "             883 |  # node_group_norm_21\n",
       "                    %\"group_norm_21\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3509\", %\"val_3510\"{...})\n",
       "             884 |  # node_Sigmoid_3457\n",
       "                    %\"val_3511\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_21\")\n",
       "             885 |  # node_silu_24\n",
       "                    %\"silu_24\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_21\", %\"val_3511\")\n",
       "             886 |  # node_conv2d_33\n",
       "                    %\"conv2d_33\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_24\", %\"unet.down_blocks.3.resnets.1.conv2.weight\"{...}, %\"unet.down_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             887 |  # node_add_49\n",
       "                    %\"add_49\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_47\", %\"conv2d_33\")\n",
       "             888 |  # node_Reshape_3462\n",
       "                    %\"val_3516\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_49\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             889 |  # node_InstanceNormalization_3469\n",
       "                    %\"val_3523\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3516\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             890 |  # node_Reshape_3471\n",
       "                    %\"val_3525\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3523\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             891 |  # node_Mul_3478\n",
       "                    %\"val_3532\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3525\", %\"val_3531\"{...})\n",
       "             892 |  # node_group_norm_22\n",
       "                    %\"group_norm_22\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3532\", %\"val_3533\"{...})\n",
       "             893 |  # node_Sigmoid_3480\n",
       "                    %\"val_3534\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_22\")\n",
       "             894 |  # node_silu_25\n",
       "                    %\"silu_25\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_22\", %\"val_3534\")\n",
       "             895 |  # node_conv2d_34\n",
       "                    %\"conv2d_34\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_25\", %\"unet.mid_block.resnets.0.conv1.weight\"{...}, %\"unet.mid_block.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             896 |  # node_linear_94\n",
       "                    %\"linear_94\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.mid_block.resnets.0.time_emb_proj.weight\"{...}, %\"unet.mid_block.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             897 |  # node_Unsqueeze_7753\n",
       "                    %\"unsqueeze_19\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_94\", %\"val_6521\"{[2, 3]})\n",
       "             898 |  # node_add_50\n",
       "                    %\"add_50\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_34\", %\"unsqueeze_19\")\n",
       "             899 |  # node_Reshape_3486\n",
       "                    %\"val_3540\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_50\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             900 |  # node_InstanceNormalization_3493\n",
       "                    %\"val_3547\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3540\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             901 |  # node_Reshape_3495\n",
       "                    %\"val_3549\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3547\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             902 |  # node_Mul_3502\n",
       "                    %\"val_3556\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3549\", %\"val_3555\"{...})\n",
       "             903 |  # node_group_norm_23\n",
       "                    %\"group_norm_23\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3556\", %\"val_3557\"{...})\n",
       "             904 |  # node_Sigmoid_3504\n",
       "                    %\"val_3558\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_23\")\n",
       "             905 |  # node_silu_27\n",
       "                    %\"silu_27\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_23\", %\"val_3558\")\n",
       "             906 |  # node_conv2d_35\n",
       "                    %\"conv2d_35\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_27\", %\"unet.mid_block.resnets.0.conv2.weight\"{...}, %\"unet.mid_block.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             907 |  # node_add_51\n",
       "                    %\"add_51\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_49\", %\"conv2d_35\")\n",
       "             908 |  # node_Reshape_3509\n",
       "                    %\"val_3563\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_51\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             909 |  # node_InstanceNormalization_3516\n",
       "                    %\"val_3570\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3563\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "             910 |  # node_Reshape_3518\n",
       "                    %\"val_3572\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3570\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             911 |  # node_Mul_3525\n",
       "                    %\"val_3579\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3572\", %\"val_3578\"{...})\n",
       "             912 |  # node_group_norm_24\n",
       "                    %\"group_norm_24\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3579\", %\"val_3580\"{...})\n",
       "             913 |  # node_conv2d_36\n",
       "                    %\"conv2d_36\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"group_norm_24\", %\"unet.mid_block.attentions.0.proj_in.weight\"{...}, %\"unet.mid_block.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             914 |  # node_permute_13\n",
       "                    %\"permute_13\"<FLOAT,[8,4,4,1280]>  ::Transpose(%\"conv2d_36\") {perm=(0, 2, 3, 1)}\n",
       "             915 |  # node_view_77\n",
       "                    %\"view_77\"<FLOAT,[8,16,1280]>  ::Reshape(%\"permute_13\", %\"val_3585\"{[8, 16, 1280]}) {allowzero=1}\n",
       "             916 |  # node_layer_norm_27\n",
       "                    %\"layer_norm_27\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_77\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             917 |  # node_linear_95\n",
       "                    %\"linear_95\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3588\"{...})\n",
       "             918 |  # node_linear_96\n",
       "                    %\"linear_96\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3589\"{...})\n",
       "             919 |  # node_linear_97\n",
       "                    %\"linear_97\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_27\", %\"val_3590\"{...})\n",
       "             920 |  # node_view_78\n",
       "                    %\"view_78\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_95\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             921 |  # node_transpose_64\n",
       "                    %\"transpose_64\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_78\") {perm=(0, 2, 1, 3)}\n",
       "             922 |  # node_view_79\n",
       "                    %\"view_79\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_96\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             923 |  # node_transpose_65\n",
       "                    %\"transpose_65\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_79\") {perm=(0, 2, 1, 3)}\n",
       "             924 |  # node_view_80\n",
       "                    %\"view_80\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_97\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             925 |  # node_transpose_66\n",
       "                    %\"transpose_66\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_80\") {perm=(0, 2, 1, 3)}\n",
       "             926 |  # node_Reshape_3570\n",
       "                    %\"val_3626\"<FLOAT,[64,16,160]>  ::Reshape(%\"transpose_65\", %\"val_3625\"{[-1, 16, 160]}) {allowzero=0}\n",
       "             927 |  # node_Transpose_3571\n",
       "                    %\"val_3627\"<FLOAT,[64,160,16]>  ::Transpose(%\"val_3626\") {perm=(0, 2, 1)}\n",
       "             928 |  # node_Reshape_3573\n",
       "                    %\"val_3629\"<FLOAT,[8,8,160,16]>  ::Reshape(%\"val_3627\", %\"val_3628\"{[8, 8, 160, 16]}) {allowzero=0}\n",
       "             929 |  # node_Mul_3575\n",
       "                    %\"val_3631\"<FLOAT,[8,8,16,160]>  ::Mul(%\"transpose_64\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             930 |  # node_Mul_3578\n",
       "                    %\"val_3634\"<FLOAT,[8,8,160,16]>  ::Mul(%\"val_3629\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             931 |  # node_MatMul_3579\n",
       "                    %\"val_3635\"<FLOAT,[8,8,16,16]>  ::MatMul(%\"val_3631\", %\"val_3634\")\n",
       "             932 |  # node_Softmax_3580\n",
       "                    %\"val_3636\"<FLOAT,[8,8,16,16]>  ::Softmax(%\"val_3635\") {axis=-1}\n",
       "             933 |  # node_scaled_dot_product_attention_16\n",
       "                    %\"scaled_dot_product_attention_16\"<FLOAT,[8,8,16,160]>  ::MatMul(%\"val_3636\", %\"transpose_66\")\n",
       "             934 |  # node_transpose_67\n",
       "                    %\"transpose_67\"<FLOAT,[8,16,8,160]>  ::Transpose(%\"scaled_dot_product_attention_16\") {perm=(0, 2, 1, 3)}\n",
       "             935 |  # node_view_81\n",
       "                    %\"view_81\"<FLOAT,[8,16,1280]>  ::Reshape(%\"transpose_67\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             936 |  # node_MatMul_3587\n",
       "                    %\"val_3643\"<FLOAT,[8,16,1280]>  ::MatMul(%\"view_81\", %\"val_3642\"{...})\n",
       "             937 |  # node_linear_98\n",
       "                    %\"linear_98\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3643\", %\"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "             938 |  # node_add_52\n",
       "                    %\"add_52\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_98\", %\"view_77\")\n",
       "             939 |  # node_layer_norm_28\n",
       "                    %\"layer_norm_28\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_52\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             940 |  # node_linear_99\n",
       "                    %\"linear_99\"<FLOAT,[8,16,1280]>  ::MatMul(%\"layer_norm_28\", %\"val_3646\"{...})\n",
       "             941 |  # node_linear_100\n",
       "                    %\"linear_100\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3647\"{...})\n",
       "             942 |  # node_linear_101\n",
       "                    %\"linear_101\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_3648\"{...})\n",
       "             943 |  # node_view_82\n",
       "                    %\"view_82\"<FLOAT,[8,16,8,160]>  ::Reshape(%\"linear_99\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             944 |  # node_transpose_68\n",
       "                    %\"transpose_68\"<FLOAT,[8,8,16,160]>  ::Transpose(%\"view_82\") {perm=(0, 2, 1, 3)}\n",
       "             945 |  # node_view_83\n",
       "                    %\"view_83\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_100\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             946 |  # node_transpose_69\n",
       "                    %\"transpose_69\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_83\") {perm=(0, 2, 1, 3)}\n",
       "             947 |  # node_view_84\n",
       "                    %\"view_84\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_101\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "             948 |  # node_transpose_70\n",
       "                    %\"transpose_70\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_84\") {perm=(0, 2, 1, 3)}\n",
       "             949 |  # node_Reshape_3626\n",
       "                    %\"val_3684\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_69\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "             950 |  # node_Transpose_3627\n",
       "                    %\"val_3685\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_3684\") {perm=(0, 2, 1)}\n",
       "             951 |  # node_Reshape_3629\n",
       "                    %\"val_3687\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_3685\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "             952 |  # node_Mul_3631\n",
       "                    %\"val_3689\"<FLOAT,[8,8,16,160]>  ::Mul(%\"transpose_68\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             953 |  # node_Mul_3634\n",
       "                    %\"val_3692\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_3687\", %\"val_3131\"{[0.28117066621780396]})\n",
       "             954 |  # node_MatMul_3635\n",
       "                    %\"val_3693\"<FLOAT,[8,8,16,50]>  ::MatMul(%\"val_3689\", %\"val_3692\")\n",
       "             955 |  # node_Softmax_3636\n",
       "                    %\"val_3694\"<FLOAT,[8,8,16,50]>  ::Softmax(%\"val_3693\") {axis=-1}\n",
       "             956 |  # node_scaled_dot_product_attention_17\n",
       "                    %\"scaled_dot_product_attention_17\"<FLOAT,[8,8,16,160]>  ::MatMul(%\"val_3694\", %\"transpose_70\")\n",
       "             957 |  # node_transpose_71\n",
       "                    %\"transpose_71\"<FLOAT,[8,16,8,160]>  ::Transpose(%\"scaled_dot_product_attention_17\") {perm=(0, 2, 1, 3)}\n",
       "             958 |  # node_view_85\n",
       "                    %\"view_85\"<FLOAT,[8,16,1280]>  ::Reshape(%\"transpose_71\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "             959 |  # node_MatMul_3643\n",
       "                    %\"val_3701\"<FLOAT,[8,16,1280]>  ::MatMul(%\"view_85\", %\"val_3700\"{...})\n",
       "             960 |  # node_linear_102\n",
       "                    %\"linear_102\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3701\", %\"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "             961 |  # node_add_53\n",
       "                    %\"add_53\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_102\", %\"add_52\")\n",
       "             962 |  # node_layer_norm_29\n",
       "                    %\"layer_norm_29\"<FLOAT,[8,16,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_53\", %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             963 |  # node_MatMul_3645\n",
       "                    %\"val_3705\"<FLOAT,[8,16,10240]>  ::MatMul(%\"layer_norm_29\", %\"val_3704\"{...})\n",
       "             964 |  # node_linear_103\n",
       "                    %\"linear_103\"<FLOAT,[8,16,10240]>  ::Add(%\"val_3705\", %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "             965 |  # node_Split_7209\n",
       "                    %\"split_6_split_0\"<FLOAT,[8,16,5120]>, %\"split_6_split_1\"<FLOAT,[8,16,5120]>  ::Split(%\"linear_103\") {axis=2, num_outputs=2}\n",
       "             966 |  # node_gelu_12\n",
       "                    %\"gelu_12\"<FLOAT,[8,16,5120]>  ::Gelu(%\"split_6_split_1\") {approximate='none'}\n",
       "             967 |  # node_mul_9\n",
       "                    %\"mul_9\"<FLOAT,[8,16,5120]>  ::Mul(%\"split_6_split_0\", %\"gelu_12\")\n",
       "             968 |  # node_MatMul_3647\n",
       "                    %\"val_3707\"<FLOAT,[8,16,1280]>  ::MatMul(%\"mul_9\", %\"val_3706\"{...})\n",
       "             969 |  # node_linear_104\n",
       "                    %\"linear_104\"<FLOAT,[8,16,1280]>  ::Add(%\"val_3707\", %\"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "             970 |  # node_add_54\n",
       "                    %\"add_54\"<FLOAT,[8,16,1280]>  ::Add(%\"linear_104\", %\"add_53\")\n",
       "             971 |  # node_view_86\n",
       "                    %\"view_86\"<FLOAT,[8,4,4,1280]>  ::Reshape(%\"add_54\", %\"val_3713\"{[8, 4, 4, 1280]}) {allowzero=1}\n",
       "             972 |  # node_permute_14\n",
       "                    %\"permute_14\"<FLOAT,[8,1280,4,4]>  ::Transpose(%\"view_86\") {perm=(0, 3, 1, 2)}\n",
       "             973 |  # node_conv2d_37\n",
       "                    %\"conv2d_37\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"permute_14\", %\"unet.mid_block.attentions.0.proj_out.weight\"{...}, %\"unet.mid_block.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             974 |  # node_add_55\n",
       "                    %\"add_55\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_37\", %\"add_51\")\n",
       "             975 |  # node_Reshape_3658\n",
       "                    %\"val_3718\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_55\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             976 |  # node_InstanceNormalization_3665\n",
       "                    %\"val_3725\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3718\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             977 |  # node_Reshape_3667\n",
       "                    %\"val_3727\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3725\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             978 |  # node_Mul_3674\n",
       "                    %\"val_3734\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3727\", %\"val_3733\"{...})\n",
       "             979 |  # node_group_norm_25\n",
       "                    %\"group_norm_25\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3734\", %\"val_3735\"{...})\n",
       "             980 |  # node_Sigmoid_3676\n",
       "                    %\"val_3736\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_25\")\n",
       "             981 |  # node_silu_28\n",
       "                    %\"silu_28\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_25\", %\"val_3736\")\n",
       "             982 |  # node_conv2d_38\n",
       "                    %\"conv2d_38\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_28\", %\"unet.mid_block.resnets.1.conv1.weight\"{...}, %\"unet.mid_block.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             983 |  # node_linear_105\n",
       "                    %\"linear_105\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.mid_block.resnets.1.time_emb_proj.weight\"{...}, %\"unet.mid_block.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "             984 |  # node_Unsqueeze_7758\n",
       "                    %\"unsqueeze_21\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_105\", %\"val_6521\"{[2, 3]})\n",
       "             985 |  # node_add_56\n",
       "                    %\"add_56\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_38\", %\"unsqueeze_21\")\n",
       "             986 |  # node_Reshape_3682\n",
       "                    %\"val_3742\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_56\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             987 |  # node_InstanceNormalization_3689\n",
       "                    %\"val_3749\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3742\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             988 |  # node_Reshape_3691\n",
       "                    %\"val_3751\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3749\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "             989 |  # node_Mul_3698\n",
       "                    %\"val_3758\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3751\", %\"val_3757\"{...})\n",
       "             990 |  # node_group_norm_26\n",
       "                    %\"group_norm_26\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3758\", %\"val_3759\"{...})\n",
       "             991 |  # node_Sigmoid_3700\n",
       "                    %\"val_3760\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_26\")\n",
       "             992 |  # node_silu_30\n",
       "                    %\"silu_30\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_26\", %\"val_3760\")\n",
       "             993 |  # node_conv2d_39\n",
       "                    %\"conv2d_39\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_30\", %\"unet.mid_block.resnets.1.conv2.weight\"{...}, %\"unet.mid_block.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             994 |  # node_add_57\n",
       "                    %\"add_57\"<FLOAT,[8,1280,4,4]>  ::Add(%\"add_55\", %\"conv2d_39\")\n",
       "             995 |  # node_cat_5\n",
       "                    %\"cat_5\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_57\", %\"add_49\") {axis=1}\n",
       "             996 |  # node_Reshape_3705\n",
       "                    %\"val_3765\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_5\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "             997 |  # node_InstanceNormalization_3712\n",
       "                    %\"val_3772\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3765\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "             998 |  # node_Reshape_3714\n",
       "                    %\"val_3774\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3772\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "             999 |  # node_Mul_3721\n",
       "                    %\"val_3781\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3774\", %\"val_3780\"{...})\n",
       "            1000 |  # node_group_norm_27\n",
       "                    %\"group_norm_27\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3781\", %\"val_3782\"{...})\n",
       "            1001 |  # node_Sigmoid_3723\n",
       "                    %\"val_3783\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_27\")\n",
       "            1002 |  # node_silu_31\n",
       "                    %\"silu_31\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_27\", %\"val_3783\")\n",
       "            1003 |  # node_conv2d_40\n",
       "                    %\"conv2d_40\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_31\", %\"unet.up_blocks.0.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1004 |  # node_linear_106\n",
       "                    %\"linear_106\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1005 |  # node_Unsqueeze_7761\n",
       "                    %\"unsqueeze_23\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_106\", %\"val_6521\"{[2, 3]})\n",
       "            1006 |  # node_add_58\n",
       "                    %\"add_58\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_40\", %\"unsqueeze_23\")\n",
       "            1007 |  # node_Reshape_3729\n",
       "                    %\"val_3789\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_58\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1008 |  # node_InstanceNormalization_3736\n",
       "                    %\"val_3796\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3789\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1009 |  # node_Reshape_3738\n",
       "                    %\"val_3798\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3796\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1010 |  # node_Mul_3745\n",
       "                    %\"val_3805\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3798\", %\"val_3804\"{...})\n",
       "            1011 |  # node_group_norm_28\n",
       "                    %\"group_norm_28\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3805\", %\"val_3806\"{...})\n",
       "            1012 |  # node_Sigmoid_3747\n",
       "                    %\"val_3807\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_28\")\n",
       "            1013 |  # node_silu_33\n",
       "                    %\"silu_33\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_28\", %\"val_3807\")\n",
       "            1014 |  # node_conv2d_41\n",
       "                    %\"conv2d_41\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_33\", %\"unet.up_blocks.0.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1015 |  # node_conv2d_42\n",
       "                    %\"conv2d_42\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_5\", %\"unet.up_blocks.0.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1016 |  # node_add_59\n",
       "                    %\"add_59\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_42\", %\"conv2d_41\")\n",
       "            1017 |  # node_cat_6\n",
       "                    %\"cat_6\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_59\", %\"add_47\") {axis=1}\n",
       "            1018 |  # node_Reshape_3752\n",
       "                    %\"val_3812\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_6\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1019 |  # node_InstanceNormalization_3759\n",
       "                    %\"val_3819\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3812\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1020 |  # node_Reshape_3761\n",
       "                    %\"val_3821\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3819\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "            1021 |  # node_Mul_3768\n",
       "                    %\"val_3828\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3821\", %\"val_3827\"{...})\n",
       "            1022 |  # node_group_norm_29\n",
       "                    %\"group_norm_29\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3828\", %\"val_3829\"{...})\n",
       "            1023 |  # node_Sigmoid_3770\n",
       "                    %\"val_3830\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_29\")\n",
       "            1024 |  # node_silu_34\n",
       "                    %\"silu_34\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_29\", %\"val_3830\")\n",
       "            1025 |  # node_conv2d_43\n",
       "                    %\"conv2d_43\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_34\", %\"unet.up_blocks.0.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1026 |  # node_linear_107\n",
       "                    %\"linear_107\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1027 |  # node_Unsqueeze_7764\n",
       "                    %\"unsqueeze_25\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_107\", %\"val_6521\"{[2, 3]})\n",
       "            1028 |  # node_add_60\n",
       "                    %\"add_60\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_43\", %\"unsqueeze_25\")\n",
       "            1029 |  # node_Reshape_3776\n",
       "                    %\"val_3836\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_60\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1030 |  # node_InstanceNormalization_3783\n",
       "                    %\"val_3843\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3836\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1031 |  # node_Reshape_3785\n",
       "                    %\"val_3845\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3843\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1032 |  # node_Mul_3792\n",
       "                    %\"val_3852\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3845\", %\"val_3851\"{...})\n",
       "            1033 |  # node_group_norm_30\n",
       "                    %\"group_norm_30\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3852\", %\"val_3853\"{...})\n",
       "            1034 |  # node_Sigmoid_3794\n",
       "                    %\"val_3854\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_30\")\n",
       "            1035 |  # node_silu_36\n",
       "                    %\"silu_36\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_30\", %\"val_3854\")\n",
       "            1036 |  # node_conv2d_44\n",
       "                    %\"conv2d_44\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_36\", %\"unet.up_blocks.0.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1037 |  # node_conv2d_45\n",
       "                    %\"conv2d_45\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_6\", %\"unet.up_blocks.0.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1038 |  # node_add_61\n",
       "                    %\"add_61\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_45\", %\"conv2d_44\")\n",
       "            1039 |  # node_cat_7\n",
       "                    %\"cat_7\"<FLOAT,[8,2560,4,4]>  ::Concat(%\"add_61\", %\"conv2d_29\") {axis=1}\n",
       "            1040 |  # node_Reshape_3799\n",
       "                    %\"val_3859\"<FLOAT,[8,32,1280]>  ::Reshape(%\"cat_7\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1041 |  # node_InstanceNormalization_3806\n",
       "                    %\"val_3866\"<FLOAT,[8,32,1280]>  ::InstanceNormalization(%\"val_3859\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1042 |  # node_Reshape_3808\n",
       "                    %\"val_3868\"<FLOAT,[8,2560,4,4]>  ::Reshape(%\"val_3866\", %\"val_3773\"{[8, 2560, 4, 4]}) {allowzero=0}\n",
       "            1043 |  # node_Mul_3815\n",
       "                    %\"val_3875\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"val_3868\", %\"val_3874\"{...})\n",
       "            1044 |  # node_group_norm_31\n",
       "                    %\"group_norm_31\"<FLOAT,[8,2560,4,4]>  ::Add(%\"val_3875\", %\"val_3876\"{...})\n",
       "            1045 |  # node_Sigmoid_3817\n",
       "                    %\"val_3877\"<FLOAT,[8,2560,4,4]>  ::Sigmoid(%\"group_norm_31\")\n",
       "            1046 |  # node_silu_37\n",
       "                    %\"silu_37\"<FLOAT,[8,2560,4,4]>  ::Mul(%\"group_norm_31\", %\"val_3877\")\n",
       "            1047 |  # node_conv2d_46\n",
       "                    %\"conv2d_46\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_37\", %\"unet.up_blocks.0.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1048 |  # node_linear_108\n",
       "                    %\"linear_108\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.0.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.0.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1049 |  # node_Unsqueeze_7767\n",
       "                    %\"unsqueeze_27\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_108\", %\"val_6521\"{[2, 3]})\n",
       "            1050 |  # node_add_62\n",
       "                    %\"add_62\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_46\", %\"unsqueeze_27\")\n",
       "            1051 |  # node_Reshape_3823\n",
       "                    %\"val_3883\"<FLOAT,[8,32,640]>  ::Reshape(%\"add_62\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1052 |  # node_InstanceNormalization_3830\n",
       "                    %\"val_3890\"<FLOAT,[8,32,640]>  ::InstanceNormalization(%\"val_3883\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1053 |  # node_Reshape_3832\n",
       "                    %\"val_3892\"<FLOAT,[8,1280,4,4]>  ::Reshape(%\"val_3890\", %\"val_3430\"{[8, 1280, 4, 4]}) {allowzero=0}\n",
       "            1054 |  # node_Mul_3839\n",
       "                    %\"val_3899\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"val_3892\", %\"val_3898\"{...})\n",
       "            1055 |  # node_group_norm_32\n",
       "                    %\"group_norm_32\"<FLOAT,[8,1280,4,4]>  ::Add(%\"val_3899\", %\"val_3900\"{...})\n",
       "            1056 |  # node_Sigmoid_3841\n",
       "                    %\"val_3901\"<FLOAT,[8,1280,4,4]>  ::Sigmoid(%\"group_norm_32\")\n",
       "            1057 |  # node_silu_39\n",
       "                    %\"silu_39\"<FLOAT,[8,1280,4,4]>  ::Mul(%\"group_norm_32\", %\"val_3901\")\n",
       "            1058 |  # node_conv2d_47\n",
       "                    %\"conv2d_47\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"silu_39\", %\"unet.up_blocks.0.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1059 |  # node_conv2d_48\n",
       "                    %\"conv2d_48\"<FLOAT,[8,1280,4,4]>  ::Conv(%\"cat_7\", %\"unet.up_blocks.0.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.0.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1060 |  # node_add_63\n",
       "                    %\"add_63\"<FLOAT,[8,1280,4,4]>  ::Add(%\"conv2d_48\", %\"conv2d_47\")\n",
       "            1061 |  # node_upsample_nearest2d\n",
       "                    %\"upsample_nearest2d\"<FLOAT,[8,1280,8,8]>  ::Resize(%\"add_63\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1062 |  # node_conv2d_49\n",
       "                    %\"conv2d_49\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"upsample_nearest2d\", %\"unet.up_blocks.0.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.0.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1063 |  # node_cat_8\n",
       "                    %\"cat_8\"<FLOAT,[8,2560,8,8]>  ::Concat(%\"conv2d_49\", %\"add_45\") {axis=1}\n",
       "            1064 |  # node_Reshape_3847\n",
       "                    %\"val_3907\"<FLOAT,[8,32,5120]>  ::Reshape(%\"cat_8\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1065 |  # node_InstanceNormalization_3854\n",
       "                    %\"val_3914\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_3907\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1066 |  # node_Reshape_3856\n",
       "                    %\"val_3916\"<FLOAT,[8,2560,8,8]>  ::Reshape(%\"val_3914\", %\"val_3915\"{[8, 2560, 8, 8]}) {allowzero=0}\n",
       "            1067 |  # node_Mul_3863\n",
       "                    %\"val_3923\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"val_3916\", %\"val_3922\"{...})\n",
       "            1068 |  # node_group_norm_33\n",
       "                    %\"group_norm_33\"<FLOAT,[8,2560,8,8]>  ::Add(%\"val_3923\", %\"val_3924\"{...})\n",
       "            1069 |  # node_Sigmoid_3865\n",
       "                    %\"val_3925\"<FLOAT,[8,2560,8,8]>  ::Sigmoid(%\"group_norm_33\")\n",
       "            1070 |  # node_silu_40\n",
       "                    %\"silu_40\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"group_norm_33\", %\"val_3925\")\n",
       "            1071 |  # node_conv2d_50\n",
       "                    %\"conv2d_50\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_40\", %\"unet.up_blocks.1.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1072 |  # node_linear_109\n",
       "                    %\"linear_109\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1073 |  # node_Unsqueeze_7770\n",
       "                    %\"unsqueeze_29\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_109\", %\"val_6521\"{[2, 3]})\n",
       "            1074 |  # node_add_64\n",
       "                    %\"add_64\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_50\", %\"unsqueeze_29\")\n",
       "            1075 |  # node_Reshape_3871\n",
       "                    %\"val_3931\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_64\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1076 |  # node_InstanceNormalization_3878\n",
       "                    %\"val_3938\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3931\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1077 |  # node_Reshape_3880\n",
       "                    %\"val_3940\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3938\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1078 |  # node_Mul_3887\n",
       "                    %\"val_3947\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3940\", %\"val_3946\"{...})\n",
       "            1079 |  # node_group_norm_34\n",
       "                    %\"group_norm_34\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3947\", %\"val_3948\"{...})\n",
       "            1080 |  # node_Sigmoid_3889\n",
       "                    %\"val_3949\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_34\")\n",
       "            1081 |  # node_silu_42\n",
       "                    %\"silu_42\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_34\", %\"val_3949\")\n",
       "            1082 |  # node_conv2d_51\n",
       "                    %\"conv2d_51\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_42\", %\"unet.up_blocks.1.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1083 |  # node_conv2d_52\n",
       "                    %\"conv2d_52\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_8\", %\"unet.up_blocks.1.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1084 |  # node_add_65\n",
       "                    %\"add_65\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_52\", %\"conv2d_51\")\n",
       "            1085 |  # node_Reshape_3894\n",
       "                    %\"val_3954\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_65\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1086 |  # node_InstanceNormalization_3901\n",
       "                    %\"val_3961\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_3954\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1087 |  # node_Reshape_3903\n",
       "                    %\"val_3963\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_3961\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1088 |  # node_Mul_3910\n",
       "                    %\"val_3970\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_3963\", %\"val_3969\"{...})\n",
       "            1089 |  # node_group_norm_35\n",
       "                    %\"group_norm_35\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_3970\", %\"val_3971\"{...})\n",
       "            1090 |  # node_conv2d_53\n",
       "                    %\"conv2d_53\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_35\", %\"unet.up_blocks.1.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1091 |  # node_permute_15\n",
       "                    %\"permute_15\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_53\") {perm=(0, 2, 3, 1)}\n",
       "            1092 |  # node_view_87\n",
       "                    %\"view_87\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_15\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1093 |  # node_layer_norm_30\n",
       "                    %\"layer_norm_30\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_87\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1094 |  # node_linear_110\n",
       "                    %\"linear_110\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3979\"{...})\n",
       "            1095 |  # node_linear_111\n",
       "                    %\"linear_111\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3980\"{...})\n",
       "            1096 |  # node_linear_112\n",
       "                    %\"linear_112\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_30\", %\"val_3981\"{...})\n",
       "            1097 |  # node_view_88\n",
       "                    %\"view_88\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_110\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1098 |  # node_transpose_72\n",
       "                    %\"transpose_72\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_88\") {perm=(0, 2, 1, 3)}\n",
       "            1099 |  # node_view_89\n",
       "                    %\"view_89\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_111\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1100 |  # node_transpose_73\n",
       "                    %\"transpose_73\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_89\") {perm=(0, 2, 1, 3)}\n",
       "            1101 |  # node_view_90\n",
       "                    %\"view_90\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_112\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1102 |  # node_transpose_74\n",
       "                    %\"transpose_74\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_90\") {perm=(0, 2, 1, 3)}\n",
       "            1103 |  # node_Reshape_3955\n",
       "                    %\"val_4017\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_73\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1104 |  # node_Transpose_3956\n",
       "                    %\"val_4018\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4017\") {perm=(0, 2, 1)}\n",
       "            1105 |  # node_Reshape_3958\n",
       "                    %\"val_4020\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4018\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1106 |  # node_Mul_3960\n",
       "                    %\"val_4022\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_72\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1107 |  # node_Mul_3963\n",
       "                    %\"val_4025\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4020\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1108 |  # node_MatMul_3964\n",
       "                    %\"val_4026\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4022\", %\"val_4025\")\n",
       "            1109 |  # node_Softmax_3965\n",
       "                    %\"val_4027\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4026\") {axis=-1}\n",
       "            1110 |  # node_scaled_dot_product_attention_18\n",
       "                    %\"scaled_dot_product_attention_18\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4027\", %\"transpose_74\")\n",
       "            1111 |  # node_transpose_75\n",
       "                    %\"transpose_75\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_18\") {perm=(0, 2, 1, 3)}\n",
       "            1112 |  # node_view_91\n",
       "                    %\"view_91\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_75\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1113 |  # node_MatMul_3972\n",
       "                    %\"val_4034\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_91\", %\"val_4033\"{...})\n",
       "            1114 |  # node_linear_113\n",
       "                    %\"linear_113\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4034\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1115 |  # node_add_66\n",
       "                    %\"add_66\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_113\", %\"view_87\")\n",
       "            1116 |  # node_layer_norm_31\n",
       "                    %\"layer_norm_31\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_66\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1117 |  # node_linear_114\n",
       "                    %\"linear_114\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_31\", %\"val_4037\"{...})\n",
       "            1118 |  # node_linear_115\n",
       "                    %\"linear_115\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4038\"{...})\n",
       "            1119 |  # node_linear_116\n",
       "                    %\"linear_116\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4039\"{...})\n",
       "            1120 |  # node_view_92\n",
       "                    %\"view_92\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_114\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1121 |  # node_transpose_76\n",
       "                    %\"transpose_76\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_92\") {perm=(0, 2, 1, 3)}\n",
       "            1122 |  # node_view_93\n",
       "                    %\"view_93\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_115\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1123 |  # node_transpose_77\n",
       "                    %\"transpose_77\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_93\") {perm=(0, 2, 1, 3)}\n",
       "            1124 |  # node_view_94\n",
       "                    %\"view_94\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_116\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1125 |  # node_transpose_78\n",
       "                    %\"transpose_78\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_94\") {perm=(0, 2, 1, 3)}\n",
       "            1126 |  # node_Reshape_4011\n",
       "                    %\"val_4075\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_77\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1127 |  # node_Transpose_4012\n",
       "                    %\"val_4076\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4075\") {perm=(0, 2, 1)}\n",
       "            1128 |  # node_Reshape_4014\n",
       "                    %\"val_4078\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4076\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1129 |  # node_Mul_4016\n",
       "                    %\"val_4080\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_76\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1130 |  # node_Mul_4019\n",
       "                    %\"val_4083\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4078\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1131 |  # node_MatMul_4020\n",
       "                    %\"val_4084\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4080\", %\"val_4083\")\n",
       "            1132 |  # node_Softmax_4021\n",
       "                    %\"val_4085\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4084\") {axis=-1}\n",
       "            1133 |  # node_scaled_dot_product_attention_19\n",
       "                    %\"scaled_dot_product_attention_19\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4085\", %\"transpose_78\")\n",
       "            1134 |  # node_transpose_79\n",
       "                    %\"transpose_79\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_19\") {perm=(0, 2, 1, 3)}\n",
       "            1135 |  # node_view_95\n",
       "                    %\"view_95\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_79\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1136 |  # node_MatMul_4028\n",
       "                    %\"val_4092\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_95\", %\"val_4091\"{...})\n",
       "            1137 |  # node_linear_117\n",
       "                    %\"linear_117\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4092\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1138 |  # node_add_67\n",
       "                    %\"add_67\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_117\", %\"add_66\")\n",
       "            1139 |  # node_layer_norm_32\n",
       "                    %\"layer_norm_32\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_67\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1140 |  # node_MatMul_4030\n",
       "                    %\"val_4096\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_32\", %\"val_4095\"{...})\n",
       "            1141 |  # node_linear_118\n",
       "                    %\"linear_118\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4096\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1142 |  # node_Split_7282\n",
       "                    %\"split_7_split_0\"<FLOAT,[8,64,5120]>, %\"split_7_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_118\") {axis=2, num_outputs=2}\n",
       "            1143 |  # node_gelu_13\n",
       "                    %\"gelu_13\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_7_split_1\") {approximate='none'}\n",
       "            1144 |  # node_mul_10\n",
       "                    %\"mul_10\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_7_split_0\", %\"gelu_13\")\n",
       "            1145 |  # node_MatMul_4032\n",
       "                    %\"val_4098\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_10\", %\"val_4097\"{...})\n",
       "            1146 |  # node_linear_119\n",
       "                    %\"linear_119\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4098\", %\"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1147 |  # node_add_68\n",
       "                    %\"add_68\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_119\", %\"add_67\")\n",
       "            1148 |  # node_view_96\n",
       "                    %\"view_96\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_68\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1149 |  # node_permute_16\n",
       "                    %\"permute_16\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_96\") {perm=(0, 3, 1, 2)}\n",
       "            1150 |  # node_conv2d_54\n",
       "                    %\"conv2d_54\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_16\", %\"unet.up_blocks.1.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1151 |  # node_add_69\n",
       "                    %\"add_69\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_54\", %\"add_65\")\n",
       "            1152 |  # node_cat_9\n",
       "                    %\"cat_9\"<FLOAT,[8,2560,8,8]>  ::Concat(%\"add_69\", %\"add_39\") {axis=1}\n",
       "            1153 |  # node_Reshape_4043\n",
       "                    %\"val_4109\"<FLOAT,[8,32,5120]>  ::Reshape(%\"cat_9\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1154 |  # node_InstanceNormalization_4050\n",
       "                    %\"val_4116\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4109\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1155 |  # node_Reshape_4052\n",
       "                    %\"val_4118\"<FLOAT,[8,2560,8,8]>  ::Reshape(%\"val_4116\", %\"val_3915\"{[8, 2560, 8, 8]}) {allowzero=0}\n",
       "            1156 |  # node_Mul_4059\n",
       "                    %\"val_4125\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"val_4118\", %\"val_4124\"{...})\n",
       "            1157 |  # node_group_norm_36\n",
       "                    %\"group_norm_36\"<FLOAT,[8,2560,8,8]>  ::Add(%\"val_4125\", %\"val_4126\"{...})\n",
       "            1158 |  # node_Sigmoid_4061\n",
       "                    %\"val_4127\"<FLOAT,[8,2560,8,8]>  ::Sigmoid(%\"group_norm_36\")\n",
       "            1159 |  # node_silu_43\n",
       "                    %\"silu_43\"<FLOAT,[8,2560,8,8]>  ::Mul(%\"group_norm_36\", %\"val_4127\")\n",
       "            1160 |  # node_conv2d_55\n",
       "                    %\"conv2d_55\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_43\", %\"unet.up_blocks.1.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1161 |  # node_linear_120\n",
       "                    %\"linear_120\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1162 |  # node_Unsqueeze_7775\n",
       "                    %\"unsqueeze_31\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_120\", %\"val_6521\"{[2, 3]})\n",
       "            1163 |  # node_add_70\n",
       "                    %\"add_70\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_55\", %\"unsqueeze_31\")\n",
       "            1164 |  # node_Reshape_4067\n",
       "                    %\"val_4133\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_70\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1165 |  # node_InstanceNormalization_4074\n",
       "                    %\"val_4140\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4133\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1166 |  # node_Reshape_4076\n",
       "                    %\"val_4142\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4140\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1167 |  # node_Mul_4083\n",
       "                    %\"val_4149\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4142\", %\"val_4148\"{...})\n",
       "            1168 |  # node_group_norm_37\n",
       "                    %\"group_norm_37\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4149\", %\"val_4150\"{...})\n",
       "            1169 |  # node_Sigmoid_4085\n",
       "                    %\"val_4151\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_37\")\n",
       "            1170 |  # node_silu_45\n",
       "                    %\"silu_45\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_37\", %\"val_4151\")\n",
       "            1171 |  # node_conv2d_56\n",
       "                    %\"conv2d_56\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_45\", %\"unet.up_blocks.1.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1172 |  # node_conv2d_57\n",
       "                    %\"conv2d_57\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_9\", %\"unet.up_blocks.1.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1173 |  # node_add_71\n",
       "                    %\"add_71\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_57\", %\"conv2d_56\")\n",
       "            1174 |  # node_Reshape_4090\n",
       "                    %\"val_4156\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_71\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1175 |  # node_InstanceNormalization_4097\n",
       "                    %\"val_4163\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4156\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1176 |  # node_Reshape_4099\n",
       "                    %\"val_4165\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4163\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1177 |  # node_Mul_4106\n",
       "                    %\"val_4172\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4165\", %\"val_4171\"{...})\n",
       "            1178 |  # node_group_norm_38\n",
       "                    %\"group_norm_38\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4172\", %\"val_4173\"{...})\n",
       "            1179 |  # node_conv2d_58\n",
       "                    %\"conv2d_58\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_38\", %\"unet.up_blocks.1.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1180 |  # node_permute_17\n",
       "                    %\"permute_17\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_58\") {perm=(0, 2, 3, 1)}\n",
       "            1181 |  # node_view_97\n",
       "                    %\"view_97\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_17\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1182 |  # node_layer_norm_33\n",
       "                    %\"layer_norm_33\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_97\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1183 |  # node_linear_121\n",
       "                    %\"linear_121\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4181\"{...})\n",
       "            1184 |  # node_linear_122\n",
       "                    %\"linear_122\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4182\"{...})\n",
       "            1185 |  # node_linear_123\n",
       "                    %\"linear_123\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_33\", %\"val_4183\"{...})\n",
       "            1186 |  # node_view_98\n",
       "                    %\"view_98\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_121\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1187 |  # node_transpose_80\n",
       "                    %\"transpose_80\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_98\") {perm=(0, 2, 1, 3)}\n",
       "            1188 |  # node_view_99\n",
       "                    %\"view_99\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_122\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1189 |  # node_transpose_81\n",
       "                    %\"transpose_81\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_99\") {perm=(0, 2, 1, 3)}\n",
       "            1190 |  # node_view_100\n",
       "                    %\"view_100\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_123\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1191 |  # node_transpose_82\n",
       "                    %\"transpose_82\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_100\") {perm=(0, 2, 1, 3)}\n",
       "            1192 |  # node_Reshape_4151\n",
       "                    %\"val_4219\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_81\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1193 |  # node_Transpose_4152\n",
       "                    %\"val_4220\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4219\") {perm=(0, 2, 1)}\n",
       "            1194 |  # node_Reshape_4154\n",
       "                    %\"val_4222\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4220\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1195 |  # node_Mul_4156\n",
       "                    %\"val_4224\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_80\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1196 |  # node_Mul_4159\n",
       "                    %\"val_4227\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4222\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1197 |  # node_MatMul_4160\n",
       "                    %\"val_4228\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4224\", %\"val_4227\")\n",
       "            1198 |  # node_Softmax_4161\n",
       "                    %\"val_4229\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4228\") {axis=-1}\n",
       "            1199 |  # node_scaled_dot_product_attention_20\n",
       "                    %\"scaled_dot_product_attention_20\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4229\", %\"transpose_82\")\n",
       "            1200 |  # node_transpose_83\n",
       "                    %\"transpose_83\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_20\") {perm=(0, 2, 1, 3)}\n",
       "            1201 |  # node_view_101\n",
       "                    %\"view_101\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_83\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1202 |  # node_MatMul_4168\n",
       "                    %\"val_4236\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_101\", %\"val_4235\"{...})\n",
       "            1203 |  # node_linear_124\n",
       "                    %\"linear_124\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4236\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1204 |  # node_add_72\n",
       "                    %\"add_72\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_124\", %\"view_97\")\n",
       "            1205 |  # node_layer_norm_34\n",
       "                    %\"layer_norm_34\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_72\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1206 |  # node_linear_125\n",
       "                    %\"linear_125\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_34\", %\"val_4239\"{...})\n",
       "            1207 |  # node_linear_126\n",
       "                    %\"linear_126\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4240\"{...})\n",
       "            1208 |  # node_linear_127\n",
       "                    %\"linear_127\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4241\"{...})\n",
       "            1209 |  # node_view_102\n",
       "                    %\"view_102\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_125\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1210 |  # node_transpose_84\n",
       "                    %\"transpose_84\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_102\") {perm=(0, 2, 1, 3)}\n",
       "            1211 |  # node_view_103\n",
       "                    %\"view_103\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_126\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1212 |  # node_transpose_85\n",
       "                    %\"transpose_85\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_103\") {perm=(0, 2, 1, 3)}\n",
       "            1213 |  # node_view_104\n",
       "                    %\"view_104\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_127\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1214 |  # node_transpose_86\n",
       "                    %\"transpose_86\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_104\") {perm=(0, 2, 1, 3)}\n",
       "            1215 |  # node_Reshape_4207\n",
       "                    %\"val_4277\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_85\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1216 |  # node_Transpose_4208\n",
       "                    %\"val_4278\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4277\") {perm=(0, 2, 1)}\n",
       "            1217 |  # node_Reshape_4210\n",
       "                    %\"val_4280\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4278\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1218 |  # node_Mul_4212\n",
       "                    %\"val_4282\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_84\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1219 |  # node_Mul_4215\n",
       "                    %\"val_4285\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4280\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1220 |  # node_MatMul_4216\n",
       "                    %\"val_4286\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4282\", %\"val_4285\")\n",
       "            1221 |  # node_Softmax_4217\n",
       "                    %\"val_4287\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4286\") {axis=-1}\n",
       "            1222 |  # node_scaled_dot_product_attention_21\n",
       "                    %\"scaled_dot_product_attention_21\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4287\", %\"transpose_86\")\n",
       "            1223 |  # node_transpose_87\n",
       "                    %\"transpose_87\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_21\") {perm=(0, 2, 1, 3)}\n",
       "            1224 |  # node_view_105\n",
       "                    %\"view_105\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_87\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1225 |  # node_MatMul_4224\n",
       "                    %\"val_4294\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_105\", %\"val_4293\"{...})\n",
       "            1226 |  # node_linear_128\n",
       "                    %\"linear_128\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4294\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1227 |  # node_add_73\n",
       "                    %\"add_73\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_128\", %\"add_72\")\n",
       "            1228 |  # node_layer_norm_35\n",
       "                    %\"layer_norm_35\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_73\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1229 |  # node_MatMul_4226\n",
       "                    %\"val_4298\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_35\", %\"val_4297\"{...})\n",
       "            1230 |  # node_linear_129\n",
       "                    %\"linear_129\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4298\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1231 |  # node_Split_7315\n",
       "                    %\"split_8_split_0\"<FLOAT,[8,64,5120]>, %\"split_8_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_129\") {axis=2, num_outputs=2}\n",
       "            1232 |  # node_gelu_14\n",
       "                    %\"gelu_14\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_8_split_1\") {approximate='none'}\n",
       "            1233 |  # node_mul_11\n",
       "                    %\"mul_11\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_8_split_0\", %\"gelu_14\")\n",
       "            1234 |  # node_MatMul_4228\n",
       "                    %\"val_4300\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_11\", %\"val_4299\"{...})\n",
       "            1235 |  # node_linear_130\n",
       "                    %\"linear_130\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4300\", %\"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1236 |  # node_add_74\n",
       "                    %\"add_74\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_130\", %\"add_73\")\n",
       "            1237 |  # node_view_106\n",
       "                    %\"view_106\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_74\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1238 |  # node_permute_18\n",
       "                    %\"permute_18\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_106\") {perm=(0, 3, 1, 2)}\n",
       "            1239 |  # node_conv2d_59\n",
       "                    %\"conv2d_59\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_18\", %\"unet.up_blocks.1.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1240 |  # node_add_75\n",
       "                    %\"add_75\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_59\", %\"add_71\")\n",
       "            1241 |  # node_cat_10\n",
       "                    %\"cat_10\"<FLOAT,[8,1920,8,8]>  ::Concat(%\"add_75\", %\"conv2d_19\") {axis=1}\n",
       "            1242 |  # node_Reshape_4239\n",
       "                    %\"val_4311\"<FLOAT,[8,32,3840]>  ::Reshape(%\"cat_10\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1243 |  # node_InstanceNormalization_4246\n",
       "                    %\"val_4318\"<FLOAT,[8,32,3840]>  ::InstanceNormalization(%\"val_4311\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1244 |  # node_Reshape_4248\n",
       "                    %\"val_4320\"<FLOAT,[8,1920,8,8]>  ::Reshape(%\"val_4318\", %\"val_4319\"{[8, 1920, 8, 8]}) {allowzero=0}\n",
       "            1245 |  # node_Mul_4255\n",
       "                    %\"val_4327\"<FLOAT,[8,1920,8,8]>  ::Mul(%\"val_4320\", %\"val_4326\"{...})\n",
       "            1246 |  # node_group_norm_39\n",
       "                    %\"group_norm_39\"<FLOAT,[8,1920,8,8]>  ::Add(%\"val_4327\", %\"val_4328\"{...})\n",
       "            1247 |  # node_Sigmoid_4257\n",
       "                    %\"val_4329\"<FLOAT,[8,1920,8,8]>  ::Sigmoid(%\"group_norm_39\")\n",
       "            1248 |  # node_silu_46\n",
       "                    %\"silu_46\"<FLOAT,[8,1920,8,8]>  ::Mul(%\"group_norm_39\", %\"val_4329\")\n",
       "            1249 |  # node_conv2d_60\n",
       "                    %\"conv2d_60\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_46\", %\"unet.up_blocks.1.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1250 |  # node_linear_131\n",
       "                    %\"linear_131\"<FLOAT,[8,1280]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.1.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.1.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1251 |  # node_Unsqueeze_7780\n",
       "                    %\"unsqueeze_33\"<FLOAT,[8,1280,1,1]>  ::Unsqueeze(%\"linear_131\", %\"val_6521\"{[2, 3]})\n",
       "            1252 |  # node_add_76\n",
       "                    %\"add_76\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_60\", %\"unsqueeze_33\")\n",
       "            1253 |  # node_Reshape_4263\n",
       "                    %\"val_4335\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_76\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1254 |  # node_InstanceNormalization_4270\n",
       "                    %\"val_4342\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4335\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1255 |  # node_Reshape_4272\n",
       "                    %\"val_4344\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4342\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1256 |  # node_Mul_4279\n",
       "                    %\"val_4351\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4344\", %\"val_4350\"{...})\n",
       "            1257 |  # node_group_norm_40\n",
       "                    %\"group_norm_40\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4351\", %\"val_4352\"{...})\n",
       "            1258 |  # node_Sigmoid_4281\n",
       "                    %\"val_4353\"<FLOAT,[8,1280,8,8]>  ::Sigmoid(%\"group_norm_40\")\n",
       "            1259 |  # node_silu_48\n",
       "                    %\"silu_48\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"group_norm_40\", %\"val_4353\")\n",
       "            1260 |  # node_conv2d_61\n",
       "                    %\"conv2d_61\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"silu_48\", %\"unet.up_blocks.1.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1261 |  # node_conv2d_62\n",
       "                    %\"conv2d_62\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"cat_10\", %\"unet.up_blocks.1.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.1.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1262 |  # node_add_77\n",
       "                    %\"add_77\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_62\", %\"conv2d_61\")\n",
       "            1263 |  # node_Reshape_4286\n",
       "                    %\"val_4358\"<FLOAT,[8,32,2560]>  ::Reshape(%\"add_77\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1264 |  # node_InstanceNormalization_4293\n",
       "                    %\"val_4365\"<FLOAT,[8,32,2560]>  ::InstanceNormalization(%\"val_4358\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1265 |  # node_Reshape_4295\n",
       "                    %\"val_4367\"<FLOAT,[8,1280,8,8]>  ::Reshape(%\"val_4365\", %\"val_3049\"{[8, 1280, 8, 8]}) {allowzero=0}\n",
       "            1266 |  # node_Mul_4302\n",
       "                    %\"val_4374\"<FLOAT,[8,1280,8,8]>  ::Mul(%\"val_4367\", %\"val_4373\"{...})\n",
       "            1267 |  # node_group_norm_41\n",
       "                    %\"group_norm_41\"<FLOAT,[8,1280,8,8]>  ::Add(%\"val_4374\", %\"val_4375\"{...})\n",
       "            1268 |  # node_conv2d_63\n",
       "                    %\"conv2d_63\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"group_norm_41\", %\"unet.up_blocks.1.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.1.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1269 |  # node_permute_19\n",
       "                    %\"permute_19\"<FLOAT,[8,8,8,1280]>  ::Transpose(%\"conv2d_63\") {perm=(0, 2, 3, 1)}\n",
       "            1270 |  # node_view_107\n",
       "                    %\"view_107\"<FLOAT,[8,64,1280]>  ::Reshape(%\"permute_19\", %\"val_3086\"{[8, 64, 1280]}) {allowzero=1}\n",
       "            1271 |  # node_layer_norm_36\n",
       "                    %\"layer_norm_36\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_107\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1272 |  # node_linear_132\n",
       "                    %\"linear_132\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4383\"{...})\n",
       "            1273 |  # node_linear_133\n",
       "                    %\"linear_133\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4384\"{...})\n",
       "            1274 |  # node_linear_134\n",
       "                    %\"linear_134\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_36\", %\"val_4385\"{...})\n",
       "            1275 |  # node_view_108\n",
       "                    %\"view_108\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_132\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1276 |  # node_transpose_88\n",
       "                    %\"transpose_88\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_108\") {perm=(0, 2, 1, 3)}\n",
       "            1277 |  # node_view_109\n",
       "                    %\"view_109\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_133\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1278 |  # node_transpose_89\n",
       "                    %\"transpose_89\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_109\") {perm=(0, 2, 1, 3)}\n",
       "            1279 |  # node_view_110\n",
       "                    %\"view_110\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_134\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1280 |  # node_transpose_90\n",
       "                    %\"transpose_90\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_110\") {perm=(0, 2, 1, 3)}\n",
       "            1281 |  # node_Reshape_4347\n",
       "                    %\"val_4421\"<FLOAT,[64,64,160]>  ::Reshape(%\"transpose_89\", %\"val_3126\"{[-1, 64, 160]}) {allowzero=0}\n",
       "            1282 |  # node_Transpose_4348\n",
       "                    %\"val_4422\"<FLOAT,[64,160,64]>  ::Transpose(%\"val_4421\") {perm=(0, 2, 1)}\n",
       "            1283 |  # node_Reshape_4350\n",
       "                    %\"val_4424\"<FLOAT,[8,8,160,64]>  ::Reshape(%\"val_4422\", %\"val_3129\"{[8, 8, 160, 64]}) {allowzero=0}\n",
       "            1284 |  # node_Mul_4352\n",
       "                    %\"val_4426\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_88\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1285 |  # node_Mul_4355\n",
       "                    %\"val_4429\"<FLOAT,[8,8,160,64]>  ::Mul(%\"val_4424\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1286 |  # node_MatMul_4356\n",
       "                    %\"val_4430\"<FLOAT,[8,8,64,64]>  ::MatMul(%\"val_4426\", %\"val_4429\")\n",
       "            1287 |  # node_Softmax_4357\n",
       "                    %\"val_4431\"<FLOAT,[8,8,64,64]>  ::Softmax(%\"val_4430\") {axis=-1}\n",
       "            1288 |  # node_scaled_dot_product_attention_22\n",
       "                    %\"scaled_dot_product_attention_22\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4431\", %\"transpose_90\")\n",
       "            1289 |  # node_transpose_91\n",
       "                    %\"transpose_91\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_22\") {perm=(0, 2, 1, 3)}\n",
       "            1290 |  # node_view_111\n",
       "                    %\"view_111\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_91\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1291 |  # node_MatMul_4364\n",
       "                    %\"val_4438\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_111\", %\"val_4437\"{...})\n",
       "            1292 |  # node_linear_135\n",
       "                    %\"linear_135\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4438\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1293 |  # node_add_78\n",
       "                    %\"add_78\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_135\", %\"view_107\")\n",
       "            1294 |  # node_layer_norm_37\n",
       "                    %\"layer_norm_37\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_78\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1295 |  # node_linear_136\n",
       "                    %\"linear_136\"<FLOAT,[8,64,1280]>  ::MatMul(%\"layer_norm_37\", %\"val_4441\"{...})\n",
       "            1296 |  # node_linear_137\n",
       "                    %\"linear_137\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4442\"{...})\n",
       "            1297 |  # node_linear_138\n",
       "                    %\"linear_138\"<FLOAT,[8,50,1280]>  ::MatMul(%\"add_9\", %\"val_4443\"{...})\n",
       "            1298 |  # node_view_112\n",
       "                    %\"view_112\"<FLOAT,[8,64,8,160]>  ::Reshape(%\"linear_136\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1299 |  # node_transpose_92\n",
       "                    %\"transpose_92\"<FLOAT,[8,8,64,160]>  ::Transpose(%\"view_112\") {perm=(0, 2, 1, 3)}\n",
       "            1300 |  # node_view_113\n",
       "                    %\"view_113\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_137\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1301 |  # node_transpose_93\n",
       "                    %\"transpose_93\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_113\") {perm=(0, 2, 1, 3)}\n",
       "            1302 |  # node_view_114\n",
       "                    %\"view_114\"<FLOAT,[8,50,8,160]>  ::Reshape(%\"linear_138\", %\"val_3097\"{[8, -1, 8, 160]}) {allowzero=1}\n",
       "            1303 |  # node_transpose_94\n",
       "                    %\"transpose_94\"<FLOAT,[8,8,50,160]>  ::Transpose(%\"view_114\") {perm=(0, 2, 1, 3)}\n",
       "            1304 |  # node_Reshape_4403\n",
       "                    %\"val_4479\"<FLOAT,[64,50,160]>  ::Reshape(%\"transpose_93\", %\"val_3184\"{[-1, 50, 160]}) {allowzero=0}\n",
       "            1305 |  # node_Transpose_4404\n",
       "                    %\"val_4480\"<FLOAT,[64,160,50]>  ::Transpose(%\"val_4479\") {perm=(0, 2, 1)}\n",
       "            1306 |  # node_Reshape_4406\n",
       "                    %\"val_4482\"<FLOAT,[8,8,160,50]>  ::Reshape(%\"val_4480\", %\"val_3187\"{[8, 8, 160, 50]}) {allowzero=0}\n",
       "            1307 |  # node_Mul_4408\n",
       "                    %\"val_4484\"<FLOAT,[8,8,64,160]>  ::Mul(%\"transpose_92\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1308 |  # node_Mul_4411\n",
       "                    %\"val_4487\"<FLOAT,[8,8,160,50]>  ::Mul(%\"val_4482\", %\"val_3131\"{[0.28117066621780396]})\n",
       "            1309 |  # node_MatMul_4412\n",
       "                    %\"val_4488\"<FLOAT,[8,8,64,50]>  ::MatMul(%\"val_4484\", %\"val_4487\")\n",
       "            1310 |  # node_Softmax_4413\n",
       "                    %\"val_4489\"<FLOAT,[8,8,64,50]>  ::Softmax(%\"val_4488\") {axis=-1}\n",
       "            1311 |  # node_scaled_dot_product_attention_23\n",
       "                    %\"scaled_dot_product_attention_23\"<FLOAT,[8,8,64,160]>  ::MatMul(%\"val_4489\", %\"transpose_94\")\n",
       "            1312 |  # node_transpose_95\n",
       "                    %\"transpose_95\"<FLOAT,[8,64,8,160]>  ::Transpose(%\"scaled_dot_product_attention_23\") {perm=(0, 2, 1, 3)}\n",
       "            1313 |  # node_view_115\n",
       "                    %\"view_115\"<FLOAT,[8,64,1280]>  ::Reshape(%\"transpose_95\", %\"val_3142\"{[8, -1, 1280]}) {allowzero=1}\n",
       "            1314 |  # node_MatMul_4420\n",
       "                    %\"val_4496\"<FLOAT,[8,64,1280]>  ::MatMul(%\"view_115\", %\"val_4495\"{...})\n",
       "            1315 |  # node_linear_139\n",
       "                    %\"linear_139\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4496\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1316 |  # node_add_79\n",
       "                    %\"add_79\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_139\", %\"add_78\")\n",
       "            1317 |  # node_layer_norm_38\n",
       "                    %\"layer_norm_38\"<FLOAT,[8,64,1280]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_79\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1318 |  # node_MatMul_4422\n",
       "                    %\"val_4500\"<FLOAT,[8,64,10240]>  ::MatMul(%\"layer_norm_38\", %\"val_4499\"{...})\n",
       "            1319 |  # node_linear_140\n",
       "                    %\"linear_140\"<FLOAT,[8,64,10240]>  ::Add(%\"val_4500\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1320 |  # node_Split_7348\n",
       "                    %\"split_9_split_0\"<FLOAT,[8,64,5120]>, %\"split_9_split_1\"<FLOAT,[8,64,5120]>  ::Split(%\"linear_140\") {axis=2, num_outputs=2}\n",
       "            1321 |  # node_gelu_15\n",
       "                    %\"gelu_15\"<FLOAT,[8,64,5120]>  ::Gelu(%\"split_9_split_1\") {approximate='none'}\n",
       "            1322 |  # node_mul_12\n",
       "                    %\"mul_12\"<FLOAT,[8,64,5120]>  ::Mul(%\"split_9_split_0\", %\"gelu_15\")\n",
       "            1323 |  # node_MatMul_4424\n",
       "                    %\"val_4502\"<FLOAT,[8,64,1280]>  ::MatMul(%\"mul_12\", %\"val_4501\"{...})\n",
       "            1324 |  # node_linear_141\n",
       "                    %\"linear_141\"<FLOAT,[8,64,1280]>  ::Add(%\"val_4502\", %\"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1325 |  # node_add_80\n",
       "                    %\"add_80\"<FLOAT,[8,64,1280]>  ::Add(%\"linear_141\", %\"add_79\")\n",
       "            1326 |  # node_view_116\n",
       "                    %\"view_116\"<FLOAT,[8,8,8,1280]>  ::Reshape(%\"add_80\", %\"val_3215\"{[8, 8, 8, 1280]}) {allowzero=1}\n",
       "            1327 |  # node_permute_20\n",
       "                    %\"permute_20\"<FLOAT,[8,1280,8,8]>  ::Transpose(%\"view_116\") {perm=(0, 3, 1, 2)}\n",
       "            1328 |  # node_conv2d_64\n",
       "                    %\"conv2d_64\"<FLOAT,[8,1280,8,8]>  ::Conv(%\"permute_20\", %\"unet.up_blocks.1.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.1.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1329 |  # node_add_81\n",
       "                    %\"add_81\"<FLOAT,[8,1280,8,8]>  ::Add(%\"conv2d_64\", %\"add_77\")\n",
       "            1330 |  # node_upsample_nearest2d_1\n",
       "                    %\"upsample_nearest2d_1\"<FLOAT,[8,1280,16,16]>  ::Resize(%\"add_81\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1331 |  # node_conv2d_65\n",
       "                    %\"conv2d_65\"<FLOAT,[8,1280,16,16]>  ::Conv(%\"upsample_nearest2d_1\", %\"unet.up_blocks.1.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.1.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1332 |  # node_cat_11\n",
       "                    %\"cat_11\"<FLOAT,[8,1920,16,16]>  ::Concat(%\"conv2d_65\", %\"add_33\") {axis=1}\n",
       "            1333 |  # node_Reshape_4436\n",
       "                    %\"val_4514\"<FLOAT,[8,32,15360]>  ::Reshape(%\"cat_11\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1334 |  # node_InstanceNormalization_4443\n",
       "                    %\"val_4521\"<FLOAT,[8,32,15360]>  ::InstanceNormalization(%\"val_4514\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1335 |  # node_Reshape_4445\n",
       "                    %\"val_4523\"<FLOAT,[8,1920,16,16]>  ::Reshape(%\"val_4521\", %\"val_4522\"{[8, 1920, 16, 16]}) {allowzero=0}\n",
       "            1336 |  # node_Mul_4452\n",
       "                    %\"val_4530\"<FLOAT,[8,1920,16,16]>  ::Mul(%\"val_4523\", %\"val_4529\"{...})\n",
       "            1337 |  # node_group_norm_42\n",
       "                    %\"group_norm_42\"<FLOAT,[8,1920,16,16]>  ::Add(%\"val_4530\", %\"val_4531\"{...})\n",
       "            1338 |  # node_Sigmoid_4454\n",
       "                    %\"val_4532\"<FLOAT,[8,1920,16,16]>  ::Sigmoid(%\"group_norm_42\")\n",
       "            1339 |  # node_silu_49\n",
       "                    %\"silu_49\"<FLOAT,[8,1920,16,16]>  ::Mul(%\"group_norm_42\", %\"val_4532\")\n",
       "            1340 |  # node_conv2d_66\n",
       "                    %\"conv2d_66\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_49\", %\"unet.up_blocks.2.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1341 |  # node_linear_142\n",
       "                    %\"linear_142\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1342 |  # node_Unsqueeze_7785\n",
       "                    %\"unsqueeze_35\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_142\", %\"val_6521\"{[2, 3]})\n",
       "            1343 |  # node_add_82\n",
       "                    %\"add_82\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_66\", %\"unsqueeze_35\")\n",
       "            1344 |  # node_Reshape_4460\n",
       "                    %\"val_4538\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_82\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1345 |  # node_InstanceNormalization_4467\n",
       "                    %\"val_4545\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4538\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1346 |  # node_Reshape_4469\n",
       "                    %\"val_4547\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4545\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1347 |  # node_Mul_4476\n",
       "                    %\"val_4554\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4547\", %\"val_4553\"{...})\n",
       "            1348 |  # node_group_norm_43\n",
       "                    %\"group_norm_43\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4554\", %\"val_4555\"{...})\n",
       "            1349 |  # node_Sigmoid_4478\n",
       "                    %\"val_4556\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_43\")\n",
       "            1350 |  # node_silu_51\n",
       "                    %\"silu_51\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_43\", %\"val_4556\")\n",
       "            1351 |  # node_conv2d_67\n",
       "                    %\"conv2d_67\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_51\", %\"unet.up_blocks.2.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1352 |  # node_conv2d_68\n",
       "                    %\"conv2d_68\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_11\", %\"unet.up_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1353 |  # node_add_83\n",
       "                    %\"add_83\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_68\", %\"conv2d_67\")\n",
       "            1354 |  # node_Reshape_4483\n",
       "                    %\"val_4561\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_83\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1355 |  # node_InstanceNormalization_4490\n",
       "                    %\"val_4568\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4561\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1356 |  # node_Reshape_4492\n",
       "                    %\"val_4570\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4568\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1357 |  # node_Mul_4499\n",
       "                    %\"val_4577\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4570\", %\"val_4576\"{...})\n",
       "            1358 |  # node_group_norm_44\n",
       "                    %\"group_norm_44\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4577\", %\"val_4578\"{...})\n",
       "            1359 |  # node_conv2d_69\n",
       "                    %\"conv2d_69\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_44\", %\"unet.up_blocks.2.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1360 |  # node_permute_21\n",
       "                    %\"permute_21\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_69\") {perm=(0, 2, 3, 1)}\n",
       "            1361 |  # node_view_117\n",
       "                    %\"view_117\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_21\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1362 |  # node_layer_norm_39\n",
       "                    %\"layer_norm_39\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_117\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1363 |  # node_linear_143\n",
       "                    %\"linear_143\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4586\"{...})\n",
       "            1364 |  # node_linear_144\n",
       "                    %\"linear_144\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4587\"{...})\n",
       "            1365 |  # node_linear_145\n",
       "                    %\"linear_145\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_39\", %\"val_4588\"{...})\n",
       "            1366 |  # node_view_118\n",
       "                    %\"view_118\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_143\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1367 |  # node_transpose_96\n",
       "                    %\"transpose_96\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_118\") {perm=(0, 2, 1, 3)}\n",
       "            1368 |  # node_view_119\n",
       "                    %\"view_119\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_144\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1369 |  # node_transpose_97\n",
       "                    %\"transpose_97\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_119\") {perm=(0, 2, 1, 3)}\n",
       "            1370 |  # node_view_120\n",
       "                    %\"view_120\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_145\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1371 |  # node_transpose_98\n",
       "                    %\"transpose_98\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_120\") {perm=(0, 2, 1, 3)}\n",
       "            1372 |  # node_Reshape_4544\n",
       "                    %\"val_4624\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_97\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1373 |  # node_Transpose_4545\n",
       "                    %\"val_4625\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_4624\") {perm=(0, 2, 1)}\n",
       "            1374 |  # node_Reshape_4547\n",
       "                    %\"val_4627\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_4625\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1375 |  # node_Mul_4549\n",
       "                    %\"val_4629\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_96\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1376 |  # node_Mul_4552\n",
       "                    %\"val_4632\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_4627\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1377 |  # node_MatMul_4553\n",
       "                    %\"val_4633\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_4629\", %\"val_4632\")\n",
       "            1378 |  # node_Softmax_4554\n",
       "                    %\"val_4634\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_4633\") {axis=-1}\n",
       "            1379 |  # node_scaled_dot_product_attention_24\n",
       "                    %\"scaled_dot_product_attention_24\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4634\", %\"transpose_98\")\n",
       "            1380 |  # node_transpose_99\n",
       "                    %\"transpose_99\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_24\") {perm=(0, 2, 1, 3)}\n",
       "            1381 |  # node_view_121\n",
       "                    %\"view_121\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_99\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1382 |  # node_MatMul_4561\n",
       "                    %\"val_4641\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_121\", %\"val_4640\"{...})\n",
       "            1383 |  # node_linear_146\n",
       "                    %\"linear_146\"<FLOAT,[8,256,640]>  ::Add(%\"val_4641\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1384 |  # node_add_84\n",
       "                    %\"add_84\"<FLOAT,[8,256,640]>  ::Add(%\"linear_146\", %\"view_117\")\n",
       "            1385 |  # node_layer_norm_40\n",
       "                    %\"layer_norm_40\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_84\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1386 |  # node_linear_147\n",
       "                    %\"linear_147\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_40\", %\"val_4644\"{...})\n",
       "            1387 |  # node_linear_148\n",
       "                    %\"linear_148\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4645\"{...})\n",
       "            1388 |  # node_linear_149\n",
       "                    %\"linear_149\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4646\"{...})\n",
       "            1389 |  # node_view_122\n",
       "                    %\"view_122\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_147\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1390 |  # node_transpose_100\n",
       "                    %\"transpose_100\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_122\") {perm=(0, 2, 1, 3)}\n",
       "            1391 |  # node_view_123\n",
       "                    %\"view_123\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_148\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1392 |  # node_transpose_101\n",
       "                    %\"transpose_101\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_123\") {perm=(0, 2, 1, 3)}\n",
       "            1393 |  # node_view_124\n",
       "                    %\"view_124\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_149\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1394 |  # node_transpose_102\n",
       "                    %\"transpose_102\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_124\") {perm=(0, 2, 1, 3)}\n",
       "            1395 |  # node_Reshape_4600\n",
       "                    %\"val_4682\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_101\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1396 |  # node_Transpose_4601\n",
       "                    %\"val_4683\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_4682\") {perm=(0, 2, 1)}\n",
       "            1397 |  # node_Reshape_4603\n",
       "                    %\"val_4685\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_4683\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1398 |  # node_Mul_4605\n",
       "                    %\"val_4687\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_100\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1399 |  # node_Mul_4608\n",
       "                    %\"val_4690\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_4685\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1400 |  # node_MatMul_4609\n",
       "                    %\"val_4691\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_4687\", %\"val_4690\")\n",
       "            1401 |  # node_Softmax_4610\n",
       "                    %\"val_4692\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_4691\") {axis=-1}\n",
       "            1402 |  # node_scaled_dot_product_attention_25\n",
       "                    %\"scaled_dot_product_attention_25\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4692\", %\"transpose_102\")\n",
       "            1403 |  # node_transpose_103\n",
       "                    %\"transpose_103\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_25\") {perm=(0, 2, 1, 3)}\n",
       "            1404 |  # node_view_125\n",
       "                    %\"view_125\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_103\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1405 |  # node_MatMul_4617\n",
       "                    %\"val_4699\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_125\", %\"val_4698\"{...})\n",
       "            1406 |  # node_linear_150\n",
       "                    %\"linear_150\"<FLOAT,[8,256,640]>  ::Add(%\"val_4699\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1407 |  # node_add_85\n",
       "                    %\"add_85\"<FLOAT,[8,256,640]>  ::Add(%\"linear_150\", %\"add_84\")\n",
       "            1408 |  # node_layer_norm_41\n",
       "                    %\"layer_norm_41\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_85\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1409 |  # node_MatMul_4619\n",
       "                    %\"val_4703\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_41\", %\"val_4702\"{...})\n",
       "            1410 |  # node_linear_151\n",
       "                    %\"linear_151\"<FLOAT,[8,256,5120]>  ::Add(%\"val_4703\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1411 |  # node_Split_7381\n",
       "                    %\"split_10_split_0\"<FLOAT,[8,256,2560]>, %\"split_10_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_151\") {axis=2, num_outputs=2}\n",
       "            1412 |  # node_gelu_16\n",
       "                    %\"gelu_16\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_10_split_1\") {approximate='none'}\n",
       "            1413 |  # node_mul_13\n",
       "                    %\"mul_13\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_10_split_0\", %\"gelu_16\")\n",
       "            1414 |  # node_MatMul_4621\n",
       "                    %\"val_4705\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_13\", %\"val_4704\"{...})\n",
       "            1415 |  # node_linear_152\n",
       "                    %\"linear_152\"<FLOAT,[8,256,640]>  ::Add(%\"val_4705\", %\"unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1416 |  # node_add_86\n",
       "                    %\"add_86\"<FLOAT,[8,256,640]>  ::Add(%\"linear_152\", %\"add_85\")\n",
       "            1417 |  # node_view_126\n",
       "                    %\"view_126\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_86\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1418 |  # node_permute_22\n",
       "                    %\"permute_22\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_126\") {perm=(0, 3, 1, 2)}\n",
       "            1419 |  # node_conv2d_70\n",
       "                    %\"conv2d_70\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_22\", %\"unet.up_blocks.2.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1420 |  # node_add_87\n",
       "                    %\"add_87\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_70\", %\"add_83\")\n",
       "            1421 |  # node_cat_12\n",
       "                    %\"cat_12\"<FLOAT,[8,1280,16,16]>  ::Concat(%\"add_87\", %\"add_27\") {axis=1}\n",
       "            1422 |  # node_Reshape_4632\n",
       "                    %\"val_4716\"<FLOAT,[8,32,10240]>  ::Reshape(%\"cat_12\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1423 |  # node_InstanceNormalization_4639\n",
       "                    %\"val_4723\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_4716\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1424 |  # node_Reshape_4641\n",
       "                    %\"val_4725\"<FLOAT,[8,1280,16,16]>  ::Reshape(%\"val_4723\", %\"val_4724\"{[8, 1280, 16, 16]}) {allowzero=0}\n",
       "            1425 |  # node_Mul_4648\n",
       "                    %\"val_4732\"<FLOAT,[8,1280,16,16]>  ::Mul(%\"val_4725\", %\"val_4731\"{...})\n",
       "            1426 |  # node_group_norm_45\n",
       "                    %\"group_norm_45\"<FLOAT,[8,1280,16,16]>  ::Add(%\"val_4732\", %\"val_4733\"{...})\n",
       "            1427 |  # node_Sigmoid_4650\n",
       "                    %\"val_4734\"<FLOAT,[8,1280,16,16]>  ::Sigmoid(%\"group_norm_45\")\n",
       "            1428 |  # node_silu_52\n",
       "                    %\"silu_52\"<FLOAT,[8,1280,16,16]>  ::Mul(%\"group_norm_45\", %\"val_4734\")\n",
       "            1429 |  # node_conv2d_71\n",
       "                    %\"conv2d_71\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_52\", %\"unet.up_blocks.2.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1430 |  # node_linear_153\n",
       "                    %\"linear_153\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1431 |  # node_Unsqueeze_7790\n",
       "                    %\"unsqueeze_37\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_153\", %\"val_6521\"{[2, 3]})\n",
       "            1432 |  # node_add_88\n",
       "                    %\"add_88\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_71\", %\"unsqueeze_37\")\n",
       "            1433 |  # node_Reshape_4656\n",
       "                    %\"val_4740\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_88\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1434 |  # node_InstanceNormalization_4663\n",
       "                    %\"val_4747\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4740\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1435 |  # node_Reshape_4665\n",
       "                    %\"val_4749\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4747\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1436 |  # node_Mul_4672\n",
       "                    %\"val_4756\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4749\", %\"val_4755\"{...})\n",
       "            1437 |  # node_group_norm_46\n",
       "                    %\"group_norm_46\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4756\", %\"val_4757\"{...})\n",
       "            1438 |  # node_Sigmoid_4674\n",
       "                    %\"val_4758\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_46\")\n",
       "            1439 |  # node_silu_54\n",
       "                    %\"silu_54\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_46\", %\"val_4758\")\n",
       "            1440 |  # node_conv2d_72\n",
       "                    %\"conv2d_72\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_54\", %\"unet.up_blocks.2.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1441 |  # node_conv2d_73\n",
       "                    %\"conv2d_73\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_12\", %\"unet.up_blocks.2.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1442 |  # node_add_89\n",
       "                    %\"add_89\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_73\", %\"conv2d_72\")\n",
       "            1443 |  # node_Reshape_4679\n",
       "                    %\"val_4763\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_89\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1444 |  # node_InstanceNormalization_4686\n",
       "                    %\"val_4770\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4763\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1445 |  # node_Reshape_4688\n",
       "                    %\"val_4772\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4770\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1446 |  # node_Mul_4695\n",
       "                    %\"val_4779\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4772\", %\"val_4778\"{...})\n",
       "            1447 |  # node_group_norm_47\n",
       "                    %\"group_norm_47\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4779\", %\"val_4780\"{...})\n",
       "            1448 |  # node_conv2d_74\n",
       "                    %\"conv2d_74\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_47\", %\"unet.up_blocks.2.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1449 |  # node_permute_23\n",
       "                    %\"permute_23\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_74\") {perm=(0, 2, 3, 1)}\n",
       "            1450 |  # node_view_127\n",
       "                    %\"view_127\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_23\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1451 |  # node_layer_norm_42\n",
       "                    %\"layer_norm_42\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_127\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1452 |  # node_linear_154\n",
       "                    %\"linear_154\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4788\"{...})\n",
       "            1453 |  # node_linear_155\n",
       "                    %\"linear_155\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4789\"{...})\n",
       "            1454 |  # node_linear_156\n",
       "                    %\"linear_156\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_42\", %\"val_4790\"{...})\n",
       "            1455 |  # node_view_128\n",
       "                    %\"view_128\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_154\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1456 |  # node_transpose_104\n",
       "                    %\"transpose_104\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_128\") {perm=(0, 2, 1, 3)}\n",
       "            1457 |  # node_view_129\n",
       "                    %\"view_129\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_155\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1458 |  # node_transpose_105\n",
       "                    %\"transpose_105\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_129\") {perm=(0, 2, 1, 3)}\n",
       "            1459 |  # node_view_130\n",
       "                    %\"view_130\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_156\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1460 |  # node_transpose_106\n",
       "                    %\"transpose_106\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_130\") {perm=(0, 2, 1, 3)}\n",
       "            1461 |  # node_Reshape_4740\n",
       "                    %\"val_4826\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_105\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1462 |  # node_Transpose_4741\n",
       "                    %\"val_4827\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_4826\") {perm=(0, 2, 1)}\n",
       "            1463 |  # node_Reshape_4743\n",
       "                    %\"val_4829\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_4827\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1464 |  # node_Mul_4745\n",
       "                    %\"val_4831\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_104\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1465 |  # node_Mul_4748\n",
       "                    %\"val_4834\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_4829\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1466 |  # node_MatMul_4749\n",
       "                    %\"val_4835\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_4831\", %\"val_4834\")\n",
       "            1467 |  # node_Softmax_4750\n",
       "                    %\"val_4836\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_4835\") {axis=-1}\n",
       "            1468 |  # node_scaled_dot_product_attention_26\n",
       "                    %\"scaled_dot_product_attention_26\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4836\", %\"transpose_106\")\n",
       "            1469 |  # node_transpose_107\n",
       "                    %\"transpose_107\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_26\") {perm=(0, 2, 1, 3)}\n",
       "            1470 |  # node_view_131\n",
       "                    %\"view_131\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_107\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1471 |  # node_MatMul_4757\n",
       "                    %\"val_4843\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_131\", %\"val_4842\"{...})\n",
       "            1472 |  # node_linear_157\n",
       "                    %\"linear_157\"<FLOAT,[8,256,640]>  ::Add(%\"val_4843\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1473 |  # node_add_90\n",
       "                    %\"add_90\"<FLOAT,[8,256,640]>  ::Add(%\"linear_157\", %\"view_127\")\n",
       "            1474 |  # node_layer_norm_43\n",
       "                    %\"layer_norm_43\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_90\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1475 |  # node_linear_158\n",
       "                    %\"linear_158\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_43\", %\"val_4846\"{...})\n",
       "            1476 |  # node_linear_159\n",
       "                    %\"linear_159\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4847\"{...})\n",
       "            1477 |  # node_linear_160\n",
       "                    %\"linear_160\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_4848\"{...})\n",
       "            1478 |  # node_view_132\n",
       "                    %\"view_132\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_158\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1479 |  # node_transpose_108\n",
       "                    %\"transpose_108\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_132\") {perm=(0, 2, 1, 3)}\n",
       "            1480 |  # node_view_133\n",
       "                    %\"view_133\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_159\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1481 |  # node_transpose_109\n",
       "                    %\"transpose_109\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_133\") {perm=(0, 2, 1, 3)}\n",
       "            1482 |  # node_view_134\n",
       "                    %\"view_134\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_160\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1483 |  # node_transpose_110\n",
       "                    %\"transpose_110\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_134\") {perm=(0, 2, 1, 3)}\n",
       "            1484 |  # node_Reshape_4796\n",
       "                    %\"val_4884\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_109\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1485 |  # node_Transpose_4797\n",
       "                    %\"val_4885\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_4884\") {perm=(0, 2, 1)}\n",
       "            1486 |  # node_Reshape_4799\n",
       "                    %\"val_4887\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_4885\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1487 |  # node_Mul_4801\n",
       "                    %\"val_4889\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_108\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1488 |  # node_Mul_4804\n",
       "                    %\"val_4892\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_4887\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1489 |  # node_MatMul_4805\n",
       "                    %\"val_4893\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_4889\", %\"val_4892\")\n",
       "            1490 |  # node_Softmax_4806\n",
       "                    %\"val_4894\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_4893\") {axis=-1}\n",
       "            1491 |  # node_scaled_dot_product_attention_27\n",
       "                    %\"scaled_dot_product_attention_27\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_4894\", %\"transpose_110\")\n",
       "            1492 |  # node_transpose_111\n",
       "                    %\"transpose_111\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_27\") {perm=(0, 2, 1, 3)}\n",
       "            1493 |  # node_view_135\n",
       "                    %\"view_135\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_111\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1494 |  # node_MatMul_4813\n",
       "                    %\"val_4901\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_135\", %\"val_4900\"{...})\n",
       "            1495 |  # node_linear_161\n",
       "                    %\"linear_161\"<FLOAT,[8,256,640]>  ::Add(%\"val_4901\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1496 |  # node_add_91\n",
       "                    %\"add_91\"<FLOAT,[8,256,640]>  ::Add(%\"linear_161\", %\"add_90\")\n",
       "            1497 |  # node_layer_norm_44\n",
       "                    %\"layer_norm_44\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_91\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1498 |  # node_MatMul_4815\n",
       "                    %\"val_4905\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_44\", %\"val_4904\"{...})\n",
       "            1499 |  # node_linear_162\n",
       "                    %\"linear_162\"<FLOAT,[8,256,5120]>  ::Add(%\"val_4905\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1500 |  # node_Split_7414\n",
       "                    %\"split_11_split_0\"<FLOAT,[8,256,2560]>, %\"split_11_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_162\") {axis=2, num_outputs=2}\n",
       "            1501 |  # node_gelu_17\n",
       "                    %\"gelu_17\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_11_split_1\") {approximate='none'}\n",
       "            1502 |  # node_mul_14\n",
       "                    %\"mul_14\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_11_split_0\", %\"gelu_17\")\n",
       "            1503 |  # node_MatMul_4817\n",
       "                    %\"val_4907\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_14\", %\"val_4906\"{...})\n",
       "            1504 |  # node_linear_163\n",
       "                    %\"linear_163\"<FLOAT,[8,256,640]>  ::Add(%\"val_4907\", %\"unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1505 |  # node_add_92\n",
       "                    %\"add_92\"<FLOAT,[8,256,640]>  ::Add(%\"linear_163\", %\"add_91\")\n",
       "            1506 |  # node_view_136\n",
       "                    %\"view_136\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_92\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1507 |  # node_permute_24\n",
       "                    %\"permute_24\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_136\") {perm=(0, 3, 1, 2)}\n",
       "            1508 |  # node_conv2d_75\n",
       "                    %\"conv2d_75\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_24\", %\"unet.up_blocks.2.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1509 |  # node_add_93\n",
       "                    %\"add_93\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_75\", %\"add_89\")\n",
       "            1510 |  # node_cat_13\n",
       "                    %\"cat_13\"<FLOAT,[8,960,16,16]>  ::Concat(%\"add_93\", %\"conv2d_9\") {axis=1}\n",
       "            1511 |  # node_Reshape_4828\n",
       "                    %\"val_4918\"<FLOAT,[8,32,7680]>  ::Reshape(%\"cat_13\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1512 |  # node_InstanceNormalization_4835\n",
       "                    %\"val_4925\"<FLOAT,[8,32,7680]>  ::InstanceNormalization(%\"val_4918\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1513 |  # node_Reshape_4837\n",
       "                    %\"val_4927\"<FLOAT,[8,960,16,16]>  ::Reshape(%\"val_4925\", %\"val_4926\"{[8, 960, 16, 16]}) {allowzero=0}\n",
       "            1514 |  # node_Mul_4844\n",
       "                    %\"val_4934\"<FLOAT,[8,960,16,16]>  ::Mul(%\"val_4927\", %\"val_4933\"{...})\n",
       "            1515 |  # node_group_norm_48\n",
       "                    %\"group_norm_48\"<FLOAT,[8,960,16,16]>  ::Add(%\"val_4934\", %\"val_4935\"{...})\n",
       "            1516 |  # node_Sigmoid_4846\n",
       "                    %\"val_4936\"<FLOAT,[8,960,16,16]>  ::Sigmoid(%\"group_norm_48\")\n",
       "            1517 |  # node_silu_55\n",
       "                    %\"silu_55\"<FLOAT,[8,960,16,16]>  ::Mul(%\"group_norm_48\", %\"val_4936\")\n",
       "            1518 |  # node_conv2d_76\n",
       "                    %\"conv2d_76\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_55\", %\"unet.up_blocks.2.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1519 |  # node_linear_164\n",
       "                    %\"linear_164\"<FLOAT,[8,640]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.2.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.2.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1520 |  # node_Unsqueeze_7795\n",
       "                    %\"unsqueeze_39\"<FLOAT,[8,640,1,1]>  ::Unsqueeze(%\"linear_164\", %\"val_6521\"{[2, 3]})\n",
       "            1521 |  # node_add_94\n",
       "                    %\"add_94\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_76\", %\"unsqueeze_39\")\n",
       "            1522 |  # node_Reshape_4852\n",
       "                    %\"val_4942\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_94\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1523 |  # node_InstanceNormalization_4859\n",
       "                    %\"val_4949\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4942\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1524 |  # node_Reshape_4861\n",
       "                    %\"val_4951\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4949\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1525 |  # node_Mul_4868\n",
       "                    %\"val_4958\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4951\", %\"val_4957\"{...})\n",
       "            1526 |  # node_group_norm_49\n",
       "                    %\"group_norm_49\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4958\", %\"val_4959\"{...})\n",
       "            1527 |  # node_Sigmoid_4870\n",
       "                    %\"val_4960\"<FLOAT,[8,640,16,16]>  ::Sigmoid(%\"group_norm_49\")\n",
       "            1528 |  # node_silu_57\n",
       "                    %\"silu_57\"<FLOAT,[8,640,16,16]>  ::Mul(%\"group_norm_49\", %\"val_4960\")\n",
       "            1529 |  # node_conv2d_77\n",
       "                    %\"conv2d_77\"<FLOAT,[8,640,16,16]>  ::Conv(%\"silu_57\", %\"unet.up_blocks.2.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1530 |  # node_conv2d_78\n",
       "                    %\"conv2d_78\"<FLOAT,[8,640,16,16]>  ::Conv(%\"cat_13\", %\"unet.up_blocks.2.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.2.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1531 |  # node_add_95\n",
       "                    %\"add_95\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_78\", %\"conv2d_77\")\n",
       "            1532 |  # node_Reshape_4875\n",
       "                    %\"val_4965\"<FLOAT,[8,32,5120]>  ::Reshape(%\"add_95\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1533 |  # node_InstanceNormalization_4882\n",
       "                    %\"val_4972\"<FLOAT,[8,32,5120]>  ::InstanceNormalization(%\"val_4965\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1534 |  # node_Reshape_4884\n",
       "                    %\"val_4974\"<FLOAT,[8,640,16,16]>  ::Reshape(%\"val_4972\", %\"val_2644\"{[8, 640, 16, 16]}) {allowzero=0}\n",
       "            1535 |  # node_Mul_4891\n",
       "                    %\"val_4981\"<FLOAT,[8,640,16,16]>  ::Mul(%\"val_4974\", %\"val_4980\"{...})\n",
       "            1536 |  # node_group_norm_50\n",
       "                    %\"group_norm_50\"<FLOAT,[8,640,16,16]>  ::Add(%\"val_4981\", %\"val_4982\"{...})\n",
       "            1537 |  # node_conv2d_79\n",
       "                    %\"conv2d_79\"<FLOAT,[8,640,16,16]>  ::Conv(%\"group_norm_50\", %\"unet.up_blocks.2.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.2.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1538 |  # node_permute_25\n",
       "                    %\"permute_25\"<FLOAT,[8,16,16,640]>  ::Transpose(%\"conv2d_79\") {perm=(0, 2, 3, 1)}\n",
       "            1539 |  # node_view_137\n",
       "                    %\"view_137\"<FLOAT,[8,256,640]>  ::Reshape(%\"permute_25\", %\"val_2681\"{[8, 256, 640]}) {allowzero=1}\n",
       "            1540 |  # node_layer_norm_45\n",
       "                    %\"layer_norm_45\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_137\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1541 |  # node_linear_165\n",
       "                    %\"linear_165\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4990\"{...})\n",
       "            1542 |  # node_linear_166\n",
       "                    %\"linear_166\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4991\"{...})\n",
       "            1543 |  # node_linear_167\n",
       "                    %\"linear_167\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_45\", %\"val_4992\"{...})\n",
       "            1544 |  # node_view_138\n",
       "                    %\"view_138\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_165\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1545 |  # node_transpose_112\n",
       "                    %\"transpose_112\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_138\") {perm=(0, 2, 1, 3)}\n",
       "            1546 |  # node_view_139\n",
       "                    %\"view_139\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_166\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1547 |  # node_transpose_113\n",
       "                    %\"transpose_113\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_139\") {perm=(0, 2, 1, 3)}\n",
       "            1548 |  # node_view_140\n",
       "                    %\"view_140\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_167\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1549 |  # node_transpose_114\n",
       "                    %\"transpose_114\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_140\") {perm=(0, 2, 1, 3)}\n",
       "            1550 |  # node_Reshape_4936\n",
       "                    %\"val_5028\"<FLOAT,[64,256,80]>  ::Reshape(%\"transpose_113\", %\"val_2721\"{[-1, 256, 80]}) {allowzero=0}\n",
       "            1551 |  # node_Transpose_4937\n",
       "                    %\"val_5029\"<FLOAT,[64,80,256]>  ::Transpose(%\"val_5028\") {perm=(0, 2, 1)}\n",
       "            1552 |  # node_Reshape_4939\n",
       "                    %\"val_5031\"<FLOAT,[8,8,80,256]>  ::Reshape(%\"val_5029\", %\"val_2724\"{[8, 8, 80, 256]}) {allowzero=0}\n",
       "            1553 |  # node_Mul_4941\n",
       "                    %\"val_5033\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_112\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1554 |  # node_Mul_4944\n",
       "                    %\"val_5036\"<FLOAT,[8,8,80,256]>  ::Mul(%\"val_5031\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1555 |  # node_MatMul_4945\n",
       "                    %\"val_5037\"<FLOAT,[8,8,256,256]>  ::MatMul(%\"val_5033\", %\"val_5036\")\n",
       "            1556 |  # node_Softmax_4946\n",
       "                    %\"val_5038\"<FLOAT,[8,8,256,256]>  ::Softmax(%\"val_5037\") {axis=-1}\n",
       "            1557 |  # node_scaled_dot_product_attention_28\n",
       "                    %\"scaled_dot_product_attention_28\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_5038\", %\"transpose_114\")\n",
       "            1558 |  # node_transpose_115\n",
       "                    %\"transpose_115\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_28\") {perm=(0, 2, 1, 3)}\n",
       "            1559 |  # node_view_141\n",
       "                    %\"view_141\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_115\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1560 |  # node_MatMul_4953\n",
       "                    %\"val_5045\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_141\", %\"val_5044\"{...})\n",
       "            1561 |  # node_linear_168\n",
       "                    %\"linear_168\"<FLOAT,[8,256,640]>  ::Add(%\"val_5045\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1562 |  # node_add_96\n",
       "                    %\"add_96\"<FLOAT,[8,256,640]>  ::Add(%\"linear_168\", %\"view_137\")\n",
       "            1563 |  # node_layer_norm_46\n",
       "                    %\"layer_norm_46\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_96\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1564 |  # node_linear_169\n",
       "                    %\"linear_169\"<FLOAT,[8,256,640]>  ::MatMul(%\"layer_norm_46\", %\"val_5048\"{...})\n",
       "            1565 |  # node_linear_170\n",
       "                    %\"linear_170\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_5049\"{...})\n",
       "            1566 |  # node_linear_171\n",
       "                    %\"linear_171\"<FLOAT,[8,50,640]>  ::MatMul(%\"add_9\", %\"val_5050\"{...})\n",
       "            1567 |  # node_view_142\n",
       "                    %\"view_142\"<FLOAT,[8,256,8,80]>  ::Reshape(%\"linear_169\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1568 |  # node_transpose_116\n",
       "                    %\"transpose_116\"<FLOAT,[8,8,256,80]>  ::Transpose(%\"view_142\") {perm=(0, 2, 1, 3)}\n",
       "            1569 |  # node_view_143\n",
       "                    %\"view_143\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_170\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1570 |  # node_transpose_117\n",
       "                    %\"transpose_117\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_143\") {perm=(0, 2, 1, 3)}\n",
       "            1571 |  # node_view_144\n",
       "                    %\"view_144\"<FLOAT,[8,50,8,80]>  ::Reshape(%\"linear_171\", %\"val_2692\"{[8, -1, 8, 80]}) {allowzero=1}\n",
       "            1572 |  # node_transpose_118\n",
       "                    %\"transpose_118\"<FLOAT,[8,8,50,80]>  ::Transpose(%\"view_144\") {perm=(0, 2, 1, 3)}\n",
       "            1573 |  # node_Reshape_4992\n",
       "                    %\"val_5086\"<FLOAT,[64,50,80]>  ::Reshape(%\"transpose_117\", %\"val_2779\"{[-1, 50, 80]}) {allowzero=0}\n",
       "            1574 |  # node_Transpose_4993\n",
       "                    %\"val_5087\"<FLOAT,[64,80,50]>  ::Transpose(%\"val_5086\") {perm=(0, 2, 1)}\n",
       "            1575 |  # node_Reshape_4995\n",
       "                    %\"val_5089\"<FLOAT,[8,8,80,50]>  ::Reshape(%\"val_5087\", %\"val_2782\"{[8, 8, 80, 50]}) {allowzero=0}\n",
       "            1576 |  # node_Mul_4997\n",
       "                    %\"val_5091\"<FLOAT,[8,8,256,80]>  ::Mul(%\"transpose_116\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1577 |  # node_Mul_5000\n",
       "                    %\"val_5094\"<FLOAT,[8,8,80,50]>  ::Mul(%\"val_5089\", %\"val_2726\"{[0.33437013626098633]})\n",
       "            1578 |  # node_MatMul_5001\n",
       "                    %\"val_5095\"<FLOAT,[8,8,256,50]>  ::MatMul(%\"val_5091\", %\"val_5094\")\n",
       "            1579 |  # node_Softmax_5002\n",
       "                    %\"val_5096\"<FLOAT,[8,8,256,50]>  ::Softmax(%\"val_5095\") {axis=-1}\n",
       "            1580 |  # node_scaled_dot_product_attention_29\n",
       "                    %\"scaled_dot_product_attention_29\"<FLOAT,[8,8,256,80]>  ::MatMul(%\"val_5096\", %\"transpose_118\")\n",
       "            1581 |  # node_transpose_119\n",
       "                    %\"transpose_119\"<FLOAT,[8,256,8,80]>  ::Transpose(%\"scaled_dot_product_attention_29\") {perm=(0, 2, 1, 3)}\n",
       "            1582 |  # node_view_145\n",
       "                    %\"view_145\"<FLOAT,[8,256,640]>  ::Reshape(%\"transpose_119\", %\"val_2737\"{[8, -1, 640]}) {allowzero=1}\n",
       "            1583 |  # node_MatMul_5009\n",
       "                    %\"val_5103\"<FLOAT,[8,256,640]>  ::MatMul(%\"view_145\", %\"val_5102\"{...})\n",
       "            1584 |  # node_linear_172\n",
       "                    %\"linear_172\"<FLOAT,[8,256,640]>  ::Add(%\"val_5103\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1585 |  # node_add_97\n",
       "                    %\"add_97\"<FLOAT,[8,256,640]>  ::Add(%\"linear_172\", %\"add_96\")\n",
       "            1586 |  # node_layer_norm_47\n",
       "                    %\"layer_norm_47\"<FLOAT,[8,256,640]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_97\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1587 |  # node_MatMul_5011\n",
       "                    %\"val_5107\"<FLOAT,[8,256,5120]>  ::MatMul(%\"layer_norm_47\", %\"val_5106\"{...})\n",
       "            1588 |  # node_linear_173\n",
       "                    %\"linear_173\"<FLOAT,[8,256,5120]>  ::Add(%\"val_5107\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1589 |  # node_Split_7447\n",
       "                    %\"split_12_split_0\"<FLOAT,[8,256,2560]>, %\"split_12_split_1\"<FLOAT,[8,256,2560]>  ::Split(%\"linear_173\") {axis=2, num_outputs=2}\n",
       "            1590 |  # node_gelu_18\n",
       "                    %\"gelu_18\"<FLOAT,[8,256,2560]>  ::Gelu(%\"split_12_split_1\") {approximate='none'}\n",
       "            1591 |  # node_mul_15\n",
       "                    %\"mul_15\"<FLOAT,[8,256,2560]>  ::Mul(%\"split_12_split_0\", %\"gelu_18\")\n",
       "            1592 |  # node_MatMul_5013\n",
       "                    %\"val_5109\"<FLOAT,[8,256,640]>  ::MatMul(%\"mul_15\", %\"val_5108\"{...})\n",
       "            1593 |  # node_linear_174\n",
       "                    %\"linear_174\"<FLOAT,[8,256,640]>  ::Add(%\"val_5109\", %\"unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1594 |  # node_add_98\n",
       "                    %\"add_98\"<FLOAT,[8,256,640]>  ::Add(%\"linear_174\", %\"add_97\")\n",
       "            1595 |  # node_view_146\n",
       "                    %\"view_146\"<FLOAT,[8,16,16,640]>  ::Reshape(%\"add_98\", %\"val_2810\"{[8, 16, 16, 640]}) {allowzero=1}\n",
       "            1596 |  # node_permute_26\n",
       "                    %\"permute_26\"<FLOAT,[8,640,16,16]>  ::Transpose(%\"view_146\") {perm=(0, 3, 1, 2)}\n",
       "            1597 |  # node_conv2d_80\n",
       "                    %\"conv2d_80\"<FLOAT,[8,640,16,16]>  ::Conv(%\"permute_26\", %\"unet.up_blocks.2.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.2.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1598 |  # node_add_99\n",
       "                    %\"add_99\"<FLOAT,[8,640,16,16]>  ::Add(%\"conv2d_80\", %\"add_95\")\n",
       "            1599 |  # node_upsample_nearest2d_2\n",
       "                    %\"upsample_nearest2d_2\"<FLOAT,[8,640,32,32]>  ::Resize(%\"add_99\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1600 |  # node_conv2d_81\n",
       "                    %\"conv2d_81\"<FLOAT,[8,640,32,32]>  ::Conv(%\"upsample_nearest2d_2\", %\"unet.up_blocks.2.upsamplers.0.conv.weight\"{...}, %\"unet.up_blocks.2.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1601 |  # node_cat_14\n",
       "                    %\"cat_14\"<FLOAT,[8,960,32,32]>  ::Concat(%\"conv2d_81\", %\"add_21\") {axis=1}\n",
       "            1602 |  # node_Reshape_5025\n",
       "                    %\"val_5121\"<FLOAT,[8,32,30720]>  ::Reshape(%\"cat_14\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1603 |  # node_InstanceNormalization_5032\n",
       "                    %\"val_5128\"<FLOAT,[8,32,30720]>  ::InstanceNormalization(%\"val_5121\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1604 |  # node_Reshape_5034\n",
       "                    %\"val_5130\"<FLOAT,[8,960,32,32]>  ::Reshape(%\"val_5128\", %\"val_5129\"{[8, 960, 32, 32]}) {allowzero=0}\n",
       "            1605 |  # node_Mul_5041\n",
       "                    %\"val_5137\"<FLOAT,[8,960,32,32]>  ::Mul(%\"val_5130\", %\"val_5136\"{...})\n",
       "            1606 |  # node_group_norm_51\n",
       "                    %\"group_norm_51\"<FLOAT,[8,960,32,32]>  ::Add(%\"val_5137\", %\"val_5138\"{...})\n",
       "            1607 |  # node_Sigmoid_5043\n",
       "                    %\"val_5139\"<FLOAT,[8,960,32,32]>  ::Sigmoid(%\"group_norm_51\")\n",
       "            1608 |  # node_silu_58\n",
       "                    %\"silu_58\"<FLOAT,[8,960,32,32]>  ::Mul(%\"group_norm_51\", %\"val_5139\")\n",
       "            1609 |  # node_conv2d_82\n",
       "                    %\"conv2d_82\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_58\", %\"unet.up_blocks.3.resnets.0.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1610 |  # node_linear_175\n",
       "                    %\"linear_175\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.0.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.0.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1611 |  # node_Unsqueeze_7800\n",
       "                    %\"unsqueeze_41\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_175\", %\"val_6521\"{[2, 3]})\n",
       "            1612 |  # node_add_100\n",
       "                    %\"add_100\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_82\", %\"unsqueeze_41\")\n",
       "            1613 |  # node_Reshape_5049\n",
       "                    %\"val_5145\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_100\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1614 |  # node_InstanceNormalization_5056\n",
       "                    %\"val_5152\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5145\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1615 |  # node_Reshape_5058\n",
       "                    %\"val_5154\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5152\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1616 |  # node_Mul_5065\n",
       "                    %\"val_5161\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5154\", %\"val_5160\"{...})\n",
       "            1617 |  # node_group_norm_52\n",
       "                    %\"group_norm_52\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5161\", %\"val_5162\"{...})\n",
       "            1618 |  # node_Sigmoid_5067\n",
       "                    %\"val_5163\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_52\")\n",
       "            1619 |  # node_silu_60\n",
       "                    %\"silu_60\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_52\", %\"val_5163\")\n",
       "            1620 |  # node_conv2d_83\n",
       "                    %\"conv2d_83\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_60\", %\"unet.up_blocks.3.resnets.0.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1621 |  # node_conv2d_84\n",
       "                    %\"conv2d_84\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_14\", %\"unet.up_blocks.3.resnets.0.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1622 |  # node_add_101\n",
       "                    %\"add_101\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_84\", %\"conv2d_83\")\n",
       "            1623 |  # node_Reshape_5072\n",
       "                    %\"val_5168\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_101\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1624 |  # node_InstanceNormalization_5079\n",
       "                    %\"val_5175\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5168\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1625 |  # node_Reshape_5081\n",
       "                    %\"val_5177\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5175\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1626 |  # node_Mul_5088\n",
       "                    %\"val_5184\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5177\", %\"val_5183\"{...})\n",
       "            1627 |  # node_group_norm_53\n",
       "                    %\"group_norm_53\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5184\", %\"val_5185\"{...})\n",
       "            1628 |  # node_conv2d_85\n",
       "                    %\"conv2d_85\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_53\", %\"unet.up_blocks.3.attentions.0.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.0.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1629 |  # node_permute_27\n",
       "                    %\"permute_27\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_85\") {perm=(0, 2, 3, 1)}\n",
       "            1630 |  # node_view_147\n",
       "                    %\"view_147\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_27\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1631 |  # node_layer_norm_48\n",
       "                    %\"layer_norm_48\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_147\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1632 |  # node_linear_176\n",
       "                    %\"linear_176\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5193\"{...})\n",
       "            1633 |  # node_linear_177\n",
       "                    %\"linear_177\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5194\"{...})\n",
       "            1634 |  # node_linear_178\n",
       "                    %\"linear_178\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_48\", %\"val_5195\"{...})\n",
       "            1635 |  # node_view_148\n",
       "                    %\"view_148\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_176\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1636 |  # node_transpose_120\n",
       "                    %\"transpose_120\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_148\") {perm=(0, 2, 1, 3)}\n",
       "            1637 |  # node_view_149\n",
       "                    %\"view_149\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_177\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1638 |  # node_transpose_121\n",
       "                    %\"transpose_121\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_149\") {perm=(0, 2, 1, 3)}\n",
       "            1639 |  # node_view_150\n",
       "                    %\"view_150\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_178\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1640 |  # node_transpose_122\n",
       "                    %\"transpose_122\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_150\") {perm=(0, 2, 1, 3)}\n",
       "            1641 |  # node_Reshape_5133\n",
       "                    %\"val_5231\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_121\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1642 |  # node_Transpose_5134\n",
       "                    %\"val_5232\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5231\") {perm=(0, 2, 1)}\n",
       "            1643 |  # node_Reshape_5136\n",
       "                    %\"val_5234\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5232\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1644 |  # node_Mul_5138\n",
       "                    %\"val_5236\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_120\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1645 |  # node_Mul_5141\n",
       "                    %\"val_5239\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5234\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1646 |  # node_MatMul_5142\n",
       "                    %\"val_5240\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5236\", %\"val_5239\")\n",
       "            1647 |  # node_Softmax_5143\n",
       "                    %\"val_5241\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5240\") {axis=-1}\n",
       "            1648 |  # node_scaled_dot_product_attention_30\n",
       "                    %\"scaled_dot_product_attention_30\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5241\", %\"transpose_122\")\n",
       "            1649 |  # node_transpose_123\n",
       "                    %\"transpose_123\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_30\") {perm=(0, 2, 1, 3)}\n",
       "            1650 |  # node_view_151\n",
       "                    %\"view_151\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_123\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1651 |  # node_MatMul_5150\n",
       "                    %\"val_5248\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_151\", %\"val_5247\"{...})\n",
       "            1652 |  # node_linear_179\n",
       "                    %\"linear_179\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5248\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1653 |  # node_add_102\n",
       "                    %\"add_102\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_179\", %\"view_147\")\n",
       "            1654 |  # node_layer_norm_49\n",
       "                    %\"layer_norm_49\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_102\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1655 |  # node_linear_180\n",
       "                    %\"linear_180\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_49\", %\"val_5251\"{...})\n",
       "            1656 |  # node_linear_181\n",
       "                    %\"linear_181\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5252\"{...})\n",
       "            1657 |  # node_linear_182\n",
       "                    %\"linear_182\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5253\"{...})\n",
       "            1658 |  # node_view_152\n",
       "                    %\"view_152\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_180\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1659 |  # node_transpose_124\n",
       "                    %\"transpose_124\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_152\") {perm=(0, 2, 1, 3)}\n",
       "            1660 |  # node_view_153\n",
       "                    %\"view_153\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_181\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1661 |  # node_transpose_125\n",
       "                    %\"transpose_125\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_153\") {perm=(0, 2, 1, 3)}\n",
       "            1662 |  # node_view_154\n",
       "                    %\"view_154\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_182\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1663 |  # node_transpose_126\n",
       "                    %\"transpose_126\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_154\") {perm=(0, 2, 1, 3)}\n",
       "            1664 |  # node_Reshape_5189\n",
       "                    %\"val_5289\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_125\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1665 |  # node_Transpose_5190\n",
       "                    %\"val_5290\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5289\") {perm=(0, 2, 1)}\n",
       "            1666 |  # node_Reshape_5192\n",
       "                    %\"val_5292\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5290\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1667 |  # node_Mul_5194\n",
       "                    %\"val_5294\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_124\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1668 |  # node_Mul_5197\n",
       "                    %\"val_5297\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5292\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1669 |  # node_MatMul_5198\n",
       "                    %\"val_5298\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5294\", %\"val_5297\")\n",
       "            1670 |  # node_Softmax_5199\n",
       "                    %\"val_5299\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5298\") {axis=-1}\n",
       "            1671 |  # node_scaled_dot_product_attention_31\n",
       "                    %\"scaled_dot_product_attention_31\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5299\", %\"transpose_126\")\n",
       "            1672 |  # node_transpose_127\n",
       "                    %\"transpose_127\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_31\") {perm=(0, 2, 1, 3)}\n",
       "            1673 |  # node_view_155\n",
       "                    %\"view_155\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_127\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1674 |  # node_MatMul_5206\n",
       "                    %\"val_5306\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_155\", %\"val_5305\"{...})\n",
       "            1675 |  # node_linear_183\n",
       "                    %\"linear_183\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5306\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1676 |  # node_add_103\n",
       "                    %\"add_103\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_183\", %\"add_102\")\n",
       "            1677 |  # node_layer_norm_50\n",
       "                    %\"layer_norm_50\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_103\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1678 |  # node_MatMul_5208\n",
       "                    %\"val_5310\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_50\", %\"val_5309\"{...})\n",
       "            1679 |  # node_linear_184\n",
       "                    %\"linear_184\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5310\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1680 |  # node_Split_7480\n",
       "                    %\"split_13_split_0\"<FLOAT,[8,1024,1280]>, %\"split_13_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_184\") {axis=2, num_outputs=2}\n",
       "            1681 |  # node_gelu_19\n",
       "                    %\"gelu_19\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_13_split_1\") {approximate='none'}\n",
       "            1682 |  # node_mul_16\n",
       "                    %\"mul_16\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_13_split_0\", %\"gelu_19\")\n",
       "            1683 |  # node_MatMul_5210\n",
       "                    %\"val_5312\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_16\", %\"val_5311\"{...})\n",
       "            1684 |  # node_linear_185\n",
       "                    %\"linear_185\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5312\", %\"unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1685 |  # node_add_104\n",
       "                    %\"add_104\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_185\", %\"add_103\")\n",
       "            1686 |  # node_view_156\n",
       "                    %\"view_156\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_104\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1687 |  # node_permute_28\n",
       "                    %\"permute_28\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_156\") {perm=(0, 3, 1, 2)}\n",
       "            1688 |  # node_conv2d_86\n",
       "                    %\"conv2d_86\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_28\", %\"unet.up_blocks.3.attentions.0.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.0.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1689 |  # node_add_105\n",
       "                    %\"add_105\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_86\", %\"add_101\")\n",
       "            1690 |  # node_cat_15\n",
       "                    %\"cat_15\"<FLOAT,[8,640,32,32]>  ::Concat(%\"add_105\", %\"add_15\") {axis=1}\n",
       "            1691 |  # node_Reshape_5221\n",
       "                    %\"val_5323\"<FLOAT,[8,32,20480]>  ::Reshape(%\"cat_15\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1692 |  # node_InstanceNormalization_5228\n",
       "                    %\"val_5330\"<FLOAT,[8,32,20480]>  ::InstanceNormalization(%\"val_5323\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1693 |  # node_Reshape_5230\n",
       "                    %\"val_5332\"<FLOAT,[8,640,32,32]>  ::Reshape(%\"val_5330\", %\"val_5331\"{[8, 640, 32, 32]}) {allowzero=0}\n",
       "            1694 |  # node_Mul_5237\n",
       "                    %\"val_5339\"<FLOAT,[8,640,32,32]>  ::Mul(%\"val_5332\", %\"val_5338\"{...})\n",
       "            1695 |  # node_group_norm_54\n",
       "                    %\"group_norm_54\"<FLOAT,[8,640,32,32]>  ::Add(%\"val_5339\", %\"val_5340\"{...})\n",
       "            1696 |  # node_Sigmoid_5239\n",
       "                    %\"val_5341\"<FLOAT,[8,640,32,32]>  ::Sigmoid(%\"group_norm_54\")\n",
       "            1697 |  # node_silu_61\n",
       "                    %\"silu_61\"<FLOAT,[8,640,32,32]>  ::Mul(%\"group_norm_54\", %\"val_5341\")\n",
       "            1698 |  # node_conv2d_87\n",
       "                    %\"conv2d_87\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_61\", %\"unet.up_blocks.3.resnets.1.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1699 |  # node_linear_186\n",
       "                    %\"linear_186\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.1.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.1.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1700 |  # node_Unsqueeze_7805\n",
       "                    %\"unsqueeze_43\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_186\", %\"val_6521\"{[2, 3]})\n",
       "            1701 |  # node_add_106\n",
       "                    %\"add_106\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_87\", %\"unsqueeze_43\")\n",
       "            1702 |  # node_Reshape_5245\n",
       "                    %\"val_5347\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_106\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1703 |  # node_InstanceNormalization_5252\n",
       "                    %\"val_5354\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5347\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1704 |  # node_Reshape_5254\n",
       "                    %\"val_5356\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5354\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1705 |  # node_Mul_5261\n",
       "                    %\"val_5363\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5356\", %\"val_5362\"{...})\n",
       "            1706 |  # node_group_norm_55\n",
       "                    %\"group_norm_55\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5363\", %\"val_5364\"{...})\n",
       "            1707 |  # node_Sigmoid_5263\n",
       "                    %\"val_5365\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_55\")\n",
       "            1708 |  # node_silu_63\n",
       "                    %\"silu_63\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_55\", %\"val_5365\")\n",
       "            1709 |  # node_conv2d_88\n",
       "                    %\"conv2d_88\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_63\", %\"unet.up_blocks.3.resnets.1.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1710 |  # node_conv2d_89\n",
       "                    %\"conv2d_89\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_15\", %\"unet.up_blocks.3.resnets.1.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.1.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1711 |  # node_add_107\n",
       "                    %\"add_107\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_89\", %\"conv2d_88\")\n",
       "            1712 |  # node_Reshape_5268\n",
       "                    %\"val_5370\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_107\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1713 |  # node_InstanceNormalization_5275\n",
       "                    %\"val_5377\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5370\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1714 |  # node_Reshape_5277\n",
       "                    %\"val_5379\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5377\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1715 |  # node_Mul_5284\n",
       "                    %\"val_5386\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5379\", %\"val_5385\"{...})\n",
       "            1716 |  # node_group_norm_56\n",
       "                    %\"group_norm_56\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5386\", %\"val_5387\"{...})\n",
       "            1717 |  # node_conv2d_90\n",
       "                    %\"conv2d_90\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_56\", %\"unet.up_blocks.3.attentions.1.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.1.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1718 |  # node_permute_29\n",
       "                    %\"permute_29\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_90\") {perm=(0, 2, 3, 1)}\n",
       "            1719 |  # node_view_157\n",
       "                    %\"view_157\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_29\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1720 |  # node_layer_norm_51\n",
       "                    %\"layer_norm_51\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_157\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1721 |  # node_linear_187\n",
       "                    %\"linear_187\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5395\"{...})\n",
       "            1722 |  # node_linear_188\n",
       "                    %\"linear_188\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5396\"{...})\n",
       "            1723 |  # node_linear_189\n",
       "                    %\"linear_189\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_51\", %\"val_5397\"{...})\n",
       "            1724 |  # node_view_158\n",
       "                    %\"view_158\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_187\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1725 |  # node_transpose_128\n",
       "                    %\"transpose_128\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_158\") {perm=(0, 2, 1, 3)}\n",
       "            1726 |  # node_view_159\n",
       "                    %\"view_159\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_188\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1727 |  # node_transpose_129\n",
       "                    %\"transpose_129\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_159\") {perm=(0, 2, 1, 3)}\n",
       "            1728 |  # node_view_160\n",
       "                    %\"view_160\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_189\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1729 |  # node_transpose_130\n",
       "                    %\"transpose_130\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_160\") {perm=(0, 2, 1, 3)}\n",
       "            1730 |  # node_Reshape_5329\n",
       "                    %\"val_5433\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_129\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1731 |  # node_Transpose_5330\n",
       "                    %\"val_5434\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5433\") {perm=(0, 2, 1)}\n",
       "            1732 |  # node_Reshape_5332\n",
       "                    %\"val_5436\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5434\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1733 |  # node_Mul_5334\n",
       "                    %\"val_5438\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_128\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1734 |  # node_Mul_5337\n",
       "                    %\"val_5441\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5436\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1735 |  # node_MatMul_5338\n",
       "                    %\"val_5442\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5438\", %\"val_5441\")\n",
       "            1736 |  # node_Softmax_5339\n",
       "                    %\"val_5443\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5442\") {axis=-1}\n",
       "            1737 |  # node_scaled_dot_product_attention_32\n",
       "                    %\"scaled_dot_product_attention_32\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5443\", %\"transpose_130\")\n",
       "            1738 |  # node_transpose_131\n",
       "                    %\"transpose_131\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_32\") {perm=(0, 2, 1, 3)}\n",
       "            1739 |  # node_view_161\n",
       "                    %\"view_161\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_131\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1740 |  # node_MatMul_5346\n",
       "                    %\"val_5450\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_161\", %\"val_5449\"{...})\n",
       "            1741 |  # node_linear_190\n",
       "                    %\"linear_190\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5450\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1742 |  # node_add_108\n",
       "                    %\"add_108\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_190\", %\"view_157\")\n",
       "            1743 |  # node_layer_norm_52\n",
       "                    %\"layer_norm_52\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_108\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1744 |  # node_linear_191\n",
       "                    %\"linear_191\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_52\", %\"val_5453\"{...})\n",
       "            1745 |  # node_linear_192\n",
       "                    %\"linear_192\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5454\"{...})\n",
       "            1746 |  # node_linear_193\n",
       "                    %\"linear_193\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5455\"{...})\n",
       "            1747 |  # node_view_162\n",
       "                    %\"view_162\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_191\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1748 |  # node_transpose_132\n",
       "                    %\"transpose_132\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_162\") {perm=(0, 2, 1, 3)}\n",
       "            1749 |  # node_view_163\n",
       "                    %\"view_163\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_192\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1750 |  # node_transpose_133\n",
       "                    %\"transpose_133\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_163\") {perm=(0, 2, 1, 3)}\n",
       "            1751 |  # node_view_164\n",
       "                    %\"view_164\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_193\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1752 |  # node_transpose_134\n",
       "                    %\"transpose_134\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_164\") {perm=(0, 2, 1, 3)}\n",
       "            1753 |  # node_Reshape_5385\n",
       "                    %\"val_5491\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_133\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1754 |  # node_Transpose_5386\n",
       "                    %\"val_5492\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5491\") {perm=(0, 2, 1)}\n",
       "            1755 |  # node_Reshape_5388\n",
       "                    %\"val_5494\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5492\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1756 |  # node_Mul_5390\n",
       "                    %\"val_5496\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_132\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1757 |  # node_Mul_5393\n",
       "                    %\"val_5499\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5494\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1758 |  # node_MatMul_5394\n",
       "                    %\"val_5500\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5496\", %\"val_5499\")\n",
       "            1759 |  # node_Softmax_5395\n",
       "                    %\"val_5501\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5500\") {axis=-1}\n",
       "            1760 |  # node_scaled_dot_product_attention_33\n",
       "                    %\"scaled_dot_product_attention_33\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5501\", %\"transpose_134\")\n",
       "            1761 |  # node_transpose_135\n",
       "                    %\"transpose_135\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_33\") {perm=(0, 2, 1, 3)}\n",
       "            1762 |  # node_view_165\n",
       "                    %\"view_165\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_135\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1763 |  # node_MatMul_5402\n",
       "                    %\"val_5508\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_165\", %\"val_5507\"{...})\n",
       "            1764 |  # node_linear_194\n",
       "                    %\"linear_194\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5508\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1765 |  # node_add_109\n",
       "                    %\"add_109\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_194\", %\"add_108\")\n",
       "            1766 |  # node_layer_norm_53\n",
       "                    %\"layer_norm_53\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_109\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1767 |  # node_MatMul_5404\n",
       "                    %\"val_5512\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_53\", %\"val_5511\"{...})\n",
       "            1768 |  # node_linear_195\n",
       "                    %\"linear_195\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5512\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1769 |  # node_Split_7513\n",
       "                    %\"split_14_split_0\"<FLOAT,[8,1024,1280]>, %\"split_14_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_195\") {axis=2, num_outputs=2}\n",
       "            1770 |  # node_gelu_20\n",
       "                    %\"gelu_20\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_14_split_1\") {approximate='none'}\n",
       "            1771 |  # node_mul_17\n",
       "                    %\"mul_17\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_14_split_0\", %\"gelu_20\")\n",
       "            1772 |  # node_MatMul_5406\n",
       "                    %\"val_5514\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_17\", %\"val_5513\"{...})\n",
       "            1773 |  # node_linear_196\n",
       "                    %\"linear_196\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5514\", %\"unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1774 |  # node_add_110\n",
       "                    %\"add_110\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_196\", %\"add_109\")\n",
       "            1775 |  # node_view_166\n",
       "                    %\"view_166\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_110\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1776 |  # node_permute_30\n",
       "                    %\"permute_30\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_166\") {perm=(0, 3, 1, 2)}\n",
       "            1777 |  # node_conv2d_91\n",
       "                    %\"conv2d_91\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_30\", %\"unet.up_blocks.3.attentions.1.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.1.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1778 |  # node_add_111\n",
       "                    %\"add_111\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_91\", %\"add_107\")\n",
       "            1779 |  # node_cat_16\n",
       "                    %\"cat_16\"<FLOAT,[8,640,32,32]>  ::Concat(%\"add_111\", %\"conv2d\") {axis=1}\n",
       "            1780 |  # node_Reshape_5417\n",
       "                    %\"val_5525\"<FLOAT,[8,32,20480]>  ::Reshape(%\"cat_16\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1781 |  # node_InstanceNormalization_5424\n",
       "                    %\"val_5532\"<FLOAT,[8,32,20480]>  ::InstanceNormalization(%\"val_5525\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1782 |  # node_Reshape_5426\n",
       "                    %\"val_5534\"<FLOAT,[8,640,32,32]>  ::Reshape(%\"val_5532\", %\"val_5331\"{[8, 640, 32, 32]}) {allowzero=0}\n",
       "            1783 |  # node_Mul_5433\n",
       "                    %\"val_5541\"<FLOAT,[8,640,32,32]>  ::Mul(%\"val_5534\", %\"val_5540\"{...})\n",
       "            1784 |  # node_group_norm_57\n",
       "                    %\"group_norm_57\"<FLOAT,[8,640,32,32]>  ::Add(%\"val_5541\", %\"val_5542\"{...})\n",
       "            1785 |  # node_Sigmoid_5435\n",
       "                    %\"val_5543\"<FLOAT,[8,640,32,32]>  ::Sigmoid(%\"group_norm_57\")\n",
       "            1786 |  # node_silu_64\n",
       "                    %\"silu_64\"<FLOAT,[8,640,32,32]>  ::Mul(%\"group_norm_57\", %\"val_5543\")\n",
       "            1787 |  # node_conv2d_92\n",
       "                    %\"conv2d_92\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_64\", %\"unet.up_blocks.3.resnets.2.conv1.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1788 |  # node_linear_197\n",
       "                    %\"linear_197\"<FLOAT,[8,320]>  ::Gemm(%\"silu_2\", %\"unet.up_blocks.3.resnets.2.time_emb_proj.weight\"{...}, %\"unet.up_blocks.3.resnets.2.time_emb_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            1789 |  # node_Unsqueeze_7810\n",
       "                    %\"unsqueeze_45\"<FLOAT,[8,320,1,1]>  ::Unsqueeze(%\"linear_197\", %\"val_6521\"{[2, 3]})\n",
       "            1790 |  # node_add_112\n",
       "                    %\"add_112\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_92\", %\"unsqueeze_45\")\n",
       "            1791 |  # node_Reshape_5441\n",
       "                    %\"val_5549\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_112\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1792 |  # node_InstanceNormalization_5448\n",
       "                    %\"val_5556\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5549\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1793 |  # node_Reshape_5450\n",
       "                    %\"val_5558\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5556\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1794 |  # node_Mul_5457\n",
       "                    %\"val_5565\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5558\", %\"val_5564\"{...})\n",
       "            1795 |  # node_group_norm_58\n",
       "                    %\"group_norm_58\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5565\", %\"val_5566\"{...})\n",
       "            1796 |  # node_Sigmoid_5459\n",
       "                    %\"val_5567\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_58\")\n",
       "            1797 |  # node_silu_66\n",
       "                    %\"silu_66\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_58\", %\"val_5567\")\n",
       "            1798 |  # node_conv2d_93\n",
       "                    %\"conv2d_93\"<FLOAT,[8,320,32,32]>  ::Conv(%\"silu_66\", %\"unet.up_blocks.3.resnets.2.conv2.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1799 |  # node_conv2d_94\n",
       "                    %\"conv2d_94\"<FLOAT,[8,320,32,32]>  ::Conv(%\"cat_16\", %\"unet.up_blocks.3.resnets.2.conv_shortcut.weight\"{...}, %\"unet.up_blocks.3.resnets.2.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1800 |  # node_add_113\n",
       "                    %\"add_113\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_94\", %\"conv2d_93\")\n",
       "            1801 |  # node_Reshape_5464\n",
       "                    %\"val_5572\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_113\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1802 |  # node_InstanceNormalization_5471\n",
       "                    %\"val_5579\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5572\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1803 |  # node_Reshape_5473\n",
       "                    %\"val_5581\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5579\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1804 |  # node_Mul_5480\n",
       "                    %\"val_5588\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5581\", %\"val_5587\"{...})\n",
       "            1805 |  # node_group_norm_59\n",
       "                    %\"group_norm_59\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5588\", %\"val_5589\"{...})\n",
       "            1806 |  # node_conv2d_95\n",
       "                    %\"conv2d_95\"<FLOAT,[8,320,32,32]>  ::Conv(%\"group_norm_59\", %\"unet.up_blocks.3.attentions.2.proj_in.weight\"{...}, %\"unet.up_blocks.3.attentions.2.proj_in.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1807 |  # node_permute_31\n",
       "                    %\"permute_31\"<FLOAT,[8,32,32,320]>  ::Transpose(%\"conv2d_95\") {perm=(0, 2, 3, 1)}\n",
       "            1808 |  # node_view_167\n",
       "                    %\"view_167\"<FLOAT,[8,1024,320]>  ::Reshape(%\"permute_31\", %\"val_2276\"{[8, 1024, 320]}) {allowzero=1}\n",
       "            1809 |  # node_layer_norm_54\n",
       "                    %\"layer_norm_54\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"view_167\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1810 |  # node_linear_198\n",
       "                    %\"linear_198\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5597\"{...})\n",
       "            1811 |  # node_linear_199\n",
       "                    %\"linear_199\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5598\"{...})\n",
       "            1812 |  # node_linear_200\n",
       "                    %\"linear_200\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_54\", %\"val_5599\"{...})\n",
       "            1813 |  # node_view_168\n",
       "                    %\"view_168\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_198\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1814 |  # node_transpose_136\n",
       "                    %\"transpose_136\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_168\") {perm=(0, 2, 1, 3)}\n",
       "            1815 |  # node_view_169\n",
       "                    %\"view_169\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_199\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1816 |  # node_transpose_137\n",
       "                    %\"transpose_137\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_169\") {perm=(0, 2, 1, 3)}\n",
       "            1817 |  # node_view_170\n",
       "                    %\"view_170\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_200\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1818 |  # node_transpose_138\n",
       "                    %\"transpose_138\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_170\") {perm=(0, 2, 1, 3)}\n",
       "            1819 |  # node_Reshape_5525\n",
       "                    %\"val_5635\"<FLOAT,[64,1024,40]>  ::Reshape(%\"transpose_137\", %\"val_2316\"{[-1, 1024, 40]}) {allowzero=0}\n",
       "            1820 |  # node_Transpose_5526\n",
       "                    %\"val_5636\"<FLOAT,[64,40,1024]>  ::Transpose(%\"val_5635\") {perm=(0, 2, 1)}\n",
       "            1821 |  # node_Reshape_5528\n",
       "                    %\"val_5638\"<FLOAT,[8,8,40,1024]>  ::Reshape(%\"val_5636\", %\"val_2319\"{[8, 8, 40, 1024]}) {allowzero=0}\n",
       "            1822 |  # node_Mul_5530\n",
       "                    %\"val_5640\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_136\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1823 |  # node_Mul_5533\n",
       "                    %\"val_5643\"<FLOAT,[8,8,40,1024]>  ::Mul(%\"val_5638\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1824 |  # node_MatMul_5534\n",
       "                    %\"val_5644\"<FLOAT,[8,8,1024,1024]>  ::MatMul(%\"val_5640\", %\"val_5643\")\n",
       "            1825 |  # node_Softmax_5535\n",
       "                    %\"val_5645\"<FLOAT,[8,8,1024,1024]>  ::Softmax(%\"val_5644\") {axis=-1}\n",
       "            1826 |  # node_scaled_dot_product_attention_34\n",
       "                    %\"scaled_dot_product_attention_34\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5645\", %\"transpose_138\")\n",
       "            1827 |  # node_transpose_139\n",
       "                    %\"transpose_139\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_34\") {perm=(0, 2, 1, 3)}\n",
       "            1828 |  # node_view_171\n",
       "                    %\"view_171\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_139\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1829 |  # node_MatMul_5542\n",
       "                    %\"val_5652\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_171\", %\"val_5651\"{...})\n",
       "            1830 |  # node_linear_201\n",
       "                    %\"linear_201\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5652\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\"{...})\n",
       "            1831 |  # node_add_114\n",
       "                    %\"add_114\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_201\", %\"view_167\")\n",
       "            1832 |  # node_layer_norm_55\n",
       "                    %\"layer_norm_55\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_114\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1833 |  # node_linear_202\n",
       "                    %\"linear_202\"<FLOAT,[8,1024,320]>  ::MatMul(%\"layer_norm_55\", %\"val_5655\"{...})\n",
       "            1834 |  # node_linear_203\n",
       "                    %\"linear_203\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5656\"{...})\n",
       "            1835 |  # node_linear_204\n",
       "                    %\"linear_204\"<FLOAT,[8,50,320]>  ::MatMul(%\"add_9\", %\"val_5657\"{...})\n",
       "            1836 |  # node_view_172\n",
       "                    %\"view_172\"<FLOAT,[8,1024,8,40]>  ::Reshape(%\"linear_202\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1837 |  # node_transpose_140\n",
       "                    %\"transpose_140\"<FLOAT,[8,8,1024,40]>  ::Transpose(%\"view_172\") {perm=(0, 2, 1, 3)}\n",
       "            1838 |  # node_view_173\n",
       "                    %\"view_173\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_203\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1839 |  # node_transpose_141\n",
       "                    %\"transpose_141\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_173\") {perm=(0, 2, 1, 3)}\n",
       "            1840 |  # node_view_174\n",
       "                    %\"view_174\"<FLOAT,[8,50,8,40]>  ::Reshape(%\"linear_204\", %\"val_2287\"{[8, -1, 8, 40]}) {allowzero=1}\n",
       "            1841 |  # node_transpose_142\n",
       "                    %\"transpose_142\"<FLOAT,[8,8,50,40]>  ::Transpose(%\"view_174\") {perm=(0, 2, 1, 3)}\n",
       "            1842 |  # node_Reshape_5581\n",
       "                    %\"val_5693\"<FLOAT,[64,50,40]>  ::Reshape(%\"transpose_141\", %\"val_2374\"{[-1, 50, 40]}) {allowzero=0}\n",
       "            1843 |  # node_Transpose_5582\n",
       "                    %\"val_5694\"<FLOAT,[64,40,50]>  ::Transpose(%\"val_5693\") {perm=(0, 2, 1)}\n",
       "            1844 |  # node_Reshape_5584\n",
       "                    %\"val_5696\"<FLOAT,[8,8,40,50]>  ::Reshape(%\"val_5694\", %\"val_2377\"{[8, 8, 40, 50]}) {allowzero=0}\n",
       "            1845 |  # node_Mul_5586\n",
       "                    %\"val_5698\"<FLOAT,[8,8,1024,40]>  ::Mul(%\"transpose_140\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1846 |  # node_Mul_5589\n",
       "                    %\"val_5701\"<FLOAT,[8,8,40,50]>  ::Mul(%\"val_5696\", %\"val_2321\"{[0.3976353704929352]})\n",
       "            1847 |  # node_MatMul_5590\n",
       "                    %\"val_5702\"<FLOAT,[8,8,1024,50]>  ::MatMul(%\"val_5698\", %\"val_5701\")\n",
       "            1848 |  # node_Softmax_5591\n",
       "                    %\"val_5703\"<FLOAT,[8,8,1024,50]>  ::Softmax(%\"val_5702\") {axis=-1}\n",
       "            1849 |  # node_scaled_dot_product_attention_35\n",
       "                    %\"scaled_dot_product_attention_35\"<FLOAT,[8,8,1024,40]>  ::MatMul(%\"val_5703\", %\"transpose_142\")\n",
       "            1850 |  # node_transpose_143\n",
       "                    %\"transpose_143\"<FLOAT,[8,1024,8,40]>  ::Transpose(%\"scaled_dot_product_attention_35\") {perm=(0, 2, 1, 3)}\n",
       "            1851 |  # node_view_175\n",
       "                    %\"view_175\"<FLOAT,[8,1024,320]>  ::Reshape(%\"transpose_143\", %\"val_2332\"{[8, -1, 320]}) {allowzero=1}\n",
       "            1852 |  # node_MatMul_5598\n",
       "                    %\"val_5710\"<FLOAT,[8,1024,320]>  ::MatMul(%\"view_175\", %\"val_5709\"{...})\n",
       "            1853 |  # node_linear_205\n",
       "                    %\"linear_205\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5710\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\"{...})\n",
       "            1854 |  # node_add_115\n",
       "                    %\"add_115\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_205\", %\"add_114\")\n",
       "            1855 |  # node_layer_norm_56\n",
       "                    %\"layer_norm_56\"<FLOAT,[8,1024,320]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_115\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\"{...}, %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1856 |  # node_MatMul_5600\n",
       "                    %\"val_5714\"<FLOAT,[8,1024,2560]>  ::MatMul(%\"layer_norm_56\", %\"val_5713\"{...})\n",
       "            1857 |  # node_linear_206\n",
       "                    %\"linear_206\"<FLOAT,[8,1024,2560]>  ::Add(%\"val_5714\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\"{...})\n",
       "            1858 |  # node_Split_7546\n",
       "                    %\"split_15_split_0\"<FLOAT,[8,1024,1280]>, %\"split_15_split_1\"<FLOAT,[8,1024,1280]>  ::Split(%\"linear_206\") {axis=2, num_outputs=2}\n",
       "            1859 |  # node_gelu_21\n",
       "                    %\"gelu_21\"<FLOAT,[8,1024,1280]>  ::Gelu(%\"split_15_split_1\") {approximate='none'}\n",
       "            1860 |  # node_mul_18\n",
       "                    %\"mul_18\"<FLOAT,[8,1024,1280]>  ::Mul(%\"split_15_split_0\", %\"gelu_21\")\n",
       "            1861 |  # node_MatMul_5602\n",
       "                    %\"val_5716\"<FLOAT,[8,1024,320]>  ::MatMul(%\"mul_18\", %\"val_5715\"{...})\n",
       "            1862 |  # node_linear_207\n",
       "                    %\"linear_207\"<FLOAT,[8,1024,320]>  ::Add(%\"val_5716\", %\"unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\"{...})\n",
       "            1863 |  # node_add_116\n",
       "                    %\"add_116\"<FLOAT,[8,1024,320]>  ::Add(%\"linear_207\", %\"add_115\")\n",
       "            1864 |  # node_view_176\n",
       "                    %\"view_176\"<FLOAT,[8,32,32,320]>  ::Reshape(%\"add_116\", %\"val_2405\"{[8, 32, 32, 320]}) {allowzero=1}\n",
       "            1865 |  # node_permute_32\n",
       "                    %\"permute_32\"<FLOAT,[8,320,32,32]>  ::Transpose(%\"view_176\") {perm=(0, 3, 1, 2)}\n",
       "            1866 |  # node_conv2d_96\n",
       "                    %\"conv2d_96\"<FLOAT,[8,320,32,32]>  ::Conv(%\"permute_32\", %\"unet.up_blocks.3.attentions.2.proj_out.weight\"{...}, %\"unet.up_blocks.3.attentions.2.proj_out.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1867 |  # node_add_117\n",
       "                    %\"add_117\"<FLOAT,[8,320,32,32]>  ::Add(%\"conv2d_96\", %\"add_113\")\n",
       "            1868 |  # node_Reshape_5613\n",
       "                    %\"val_5727\"<FLOAT,[8,32,10240]>  ::Reshape(%\"add_117\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1869 |  # node_InstanceNormalization_5620\n",
       "                    %\"val_5734\"<FLOAT,[8,32,10240]>  ::InstanceNormalization(%\"val_5727\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-05}\n",
       "            1870 |  # node_Reshape_5622\n",
       "                    %\"val_5736\"<FLOAT,[8,320,32,32]>  ::Reshape(%\"val_5734\", %\"val_2212\"{[8, 320, 32, 32]}) {allowzero=0}\n",
       "            1871 |  # node_Mul_5629\n",
       "                    %\"val_5743\"<FLOAT,[8,320,32,32]>  ::Mul(%\"val_5736\", %\"val_5742\"{...})\n",
       "            1872 |  # node_group_norm_60\n",
       "                    %\"group_norm_60\"<FLOAT,[8,320,32,32]>  ::Add(%\"val_5743\", %\"val_5744\"{...})\n",
       "            1873 |  # node_Sigmoid_5631\n",
       "                    %\"val_5745\"<FLOAT,[8,320,32,32]>  ::Sigmoid(%\"group_norm_60\")\n",
       "            1874 |  # node_silu_67\n",
       "                    %\"silu_67\"<FLOAT,[8,320,32,32]>  ::Mul(%\"group_norm_60\", %\"val_5745\")\n",
       "            1875 |  # node_conv2d_97\n",
       "                    %\"conv2d_97\"<FLOAT,[8,4,32,32]>  ::Conv(%\"silu_67\", %\"unet.conv_out.weight\"{...}, %\"unet.conv_out.bias\"{[-0.006085909903049469, -0.00038231629878282547, 0.006092327646911144, -0.01816507801413536]}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1876 |  # node_mul_19\n",
       "                    %\"mul_19\"<FLOAT,[8,4,32,32]>  ::Mul(%\"conv2d_97\", %\"val_5746\"{5.489980697631836})\n",
       "            1877 |  # node_conv2d_98\n",
       "                    %\"conv2d_98\"<FLOAT,[8,4,32,32]>  ::Conv(%\"mul_19\", %\"vae.post_quant_conv.weight\"{...}, %\"vae.post_quant_conv.bias\"{[0.032071199268102646, -0.08428296446800232, -0.24323134124279022, 0.13154643774032593]}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1878 |  # node_conv2d_99\n",
       "                    %\"conv2d_99\"<FLOAT,[8,512,32,32]>  ::Conv(%\"conv2d_98\", %\"vae.decoder.conv_in.weight\"{...}, %\"vae.decoder.conv_in.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1879 |  # node_Reshape_5637\n",
       "                    %\"val_5751\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_99\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1880 |  # node_InstanceNormalization_5644\n",
       "                    %\"val_5758\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5751\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1881 |  # node_Reshape_5646\n",
       "                    %\"val_5760\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5758\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1882 |  # node_Mul_5653\n",
       "                    %\"val_5767\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5760\", %\"val_5766\"{...})\n",
       "            1883 |  # node_group_norm_61\n",
       "                    %\"group_norm_61\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5767\", %\"val_5768\"{...})\n",
       "            1884 |  # node_Sigmoid_5655\n",
       "                    %\"val_5769\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_61\")\n",
       "            1885 |  # node_silu_68\n",
       "                    %\"silu_68\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_61\", %\"val_5769\")\n",
       "            1886 |  # node_conv2d_100\n",
       "                    %\"conv2d_100\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_68\", %\"vae.decoder.mid_block.resnets.0.conv1.weight\"{...}, %\"vae.decoder.mid_block.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1887 |  # node_Reshape_5660\n",
       "                    %\"val_5774\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_100\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1888 |  # node_InstanceNormalization_5667\n",
       "                    %\"val_5781\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5774\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1889 |  # node_Reshape_5669\n",
       "                    %\"val_5783\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5781\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1890 |  # node_Mul_5676\n",
       "                    %\"val_5790\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5783\", %\"val_5789\"{...})\n",
       "            1891 |  # node_group_norm_62\n",
       "                    %\"group_norm_62\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5790\", %\"val_5791\"{...})\n",
       "            1892 |  # node_Sigmoid_5678\n",
       "                    %\"val_5792\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_62\")\n",
       "            1893 |  # node_silu_69\n",
       "                    %\"silu_69\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_62\", %\"val_5792\")\n",
       "            1894 |  # node_conv2d_101\n",
       "                    %\"conv2d_101\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_69\", %\"vae.decoder.mid_block.resnets.0.conv2.weight\"{...}, %\"vae.decoder.mid_block.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1895 |  # node_add_118\n",
       "                    %\"add_118\"<FLOAT,[8,512,32,32]>  ::Add(%\"conv2d_99\", %\"conv2d_101\")\n",
       "            1896 |  # node_view_177\n",
       "                    %\"view_177\"<FLOAT,[8,512,1024]>  ::Reshape(%\"add_118\", %\"val_5797\"{[8, 512, 1024]}) {allowzero=1}\n",
       "            1897 |  # node_Reshape_5688\n",
       "                    %\"val_5802\"<FLOAT,[8,32,16384]>  ::Reshape(%\"view_177\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1898 |  # node_InstanceNormalization_5695\n",
       "                    %\"val_5809\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5802\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1899 |  # node_Reshape_5697\n",
       "                    %\"val_5811\"<FLOAT,[8,512,1024]>  ::Reshape(%\"val_5809\", %\"val_5797\"{[8, 512, 1024]}) {allowzero=0}\n",
       "            1900 |  # node_Mul_5705\n",
       "                    %\"val_5819\"<FLOAT,[8,512,1024]>  ::Mul(%\"val_5811\", %\"val_5818\"{...})\n",
       "            1901 |  # node_group_norm_63\n",
       "                    %\"group_norm_63\"<FLOAT,[8,512,1024]>  ::Add(%\"val_5819\", %\"val_5820\"{...})\n",
       "            1902 |  # node_transpose_146\n",
       "                    %\"transpose_146\"<FLOAT,[8,1024,512]>  ::Transpose(%\"group_norm_63\") {perm=(0, 2, 1)}\n",
       "            1903 |  # node_MatMul_5708\n",
       "                    %\"val_5822\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5821\"{...})\n",
       "            1904 |  # node_linear_208\n",
       "                    %\"linear_208\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5822\", %\"vae.decoder.mid_block.attentions.0.to_q.bias\"{...})\n",
       "            1905 |  # node_MatMul_5710\n",
       "                    %\"val_5824\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5823\"{...})\n",
       "            1906 |  # node_linear_209\n",
       "                    %\"linear_209\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5824\", %\"vae.decoder.mid_block.attentions.0.to_k.bias\"{...})\n",
       "            1907 |  # node_MatMul_5712\n",
       "                    %\"val_5826\"<FLOAT,[8,1024,512]>  ::MatMul(%\"transpose_146\", %\"val_5825\"{...})\n",
       "            1908 |  # node_linear_210\n",
       "                    %\"linear_210\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5826\", %\"vae.decoder.mid_block.attentions.0.to_v.bias\"{...})\n",
       "            1909 |  # node_view_178\n",
       "                    %\"view_178\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_208\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1910 |  # node_transpose_147\n",
       "                    %\"transpose_147\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_178\") {perm=(0, 2, 1, 3)}\n",
       "            1911 |  # node_view_179\n",
       "                    %\"view_179\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_209\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1912 |  # node_transpose_148\n",
       "                    %\"transpose_148\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_179\") {perm=(0, 2, 1, 3)}\n",
       "            1913 |  # node_view_180\n",
       "                    %\"view_180\"<FLOAT,[8,1024,1,512]>  ::Reshape(%\"linear_210\", %\"val_5832\"{[8, -1, 1, 512]}) {allowzero=1}\n",
       "            1914 |  # node_transpose_149\n",
       "                    %\"transpose_149\"<FLOAT,[8,1,1024,512]>  ::Transpose(%\"view_180\") {perm=(0, 2, 1, 3)}\n",
       "            1915 |  # node_Reshape_5748\n",
       "                    %\"val_5862\"<FLOAT,[8,1024,512]>  ::Reshape(%\"transpose_148\", %\"val_5861\"{[-1, 1024, 512]}) {allowzero=0}\n",
       "            1916 |  # node_Transpose_5749\n",
       "                    %\"val_5863\"<FLOAT,[8,512,1024]>  ::Transpose(%\"val_5862\") {perm=(0, 2, 1)}\n",
       "            1917 |  # node_Reshape_5751\n",
       "                    %\"val_5865\"<FLOAT,[8,1,512,1024]>  ::Reshape(%\"val_5863\", %\"val_5864\"{[8, 1, 512, 1024]}) {allowzero=0}\n",
       "            1918 |  # node_Mul_5753\n",
       "                    %\"val_5867\"<FLOAT,[8,1,1024,512]>  ::Mul(%\"transpose_147\", %\"val_5866\"{[0.21022410690784454]})\n",
       "            1919 |  # node_Mul_5756\n",
       "                    %\"val_5870\"<FLOAT,[8,1,512,1024]>  ::Mul(%\"val_5865\", %\"val_5866\"{[0.21022410690784454]})\n",
       "            1920 |  # node_MatMul_5757\n",
       "                    %\"val_5871\"<FLOAT,[8,1,1024,1024]>  ::MatMul(%\"val_5867\", %\"val_5870\")\n",
       "            1921 |  # node_Softmax_5758\n",
       "                    %\"val_5872\"<FLOAT,[8,1,1024,1024]>  ::Softmax(%\"val_5871\") {axis=-1}\n",
       "            1922 |  # node_scaled_dot_product_attention_36\n",
       "                    %\"scaled_dot_product_attention_36\"<FLOAT,[8,1,1024,512]>  ::MatMul(%\"val_5872\", %\"transpose_149\")\n",
       "            1923 |  # node_transpose_150\n",
       "                    %\"transpose_150\"<FLOAT,[8,1024,1,512]>  ::Transpose(%\"scaled_dot_product_attention_36\") {perm=(0, 2, 1, 3)}\n",
       "            1924 |  # node_view_181\n",
       "                    %\"view_181\"<FLOAT,[8,1024,512]>  ::Reshape(%\"transpose_150\", %\"val_5877\"{[8, -1, 512]}) {allowzero=1}\n",
       "            1925 |  # node_MatMul_5765\n",
       "                    %\"val_5879\"<FLOAT,[8,1024,512]>  ::MatMul(%\"view_181\", %\"val_5878\"{...})\n",
       "            1926 |  # node_linear_211\n",
       "                    %\"linear_211\"<FLOAT,[8,1024,512]>  ::Add(%\"val_5879\", %\"vae.decoder.mid_block.attentions.0.to_out.0.bias\"{...})\n",
       "            1927 |  # node_transpose_151\n",
       "                    %\"transpose_151\"<FLOAT,[8,512,1024]>  ::Transpose(%\"linear_211\") {perm=(0, 2, 1)}\n",
       "            1928 |  # node_view_182\n",
       "                    %\"view_182\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"transpose_151\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=1}\n",
       "            1929 |  # node_add_119\n",
       "                    %\"add_119\"<FLOAT,[8,512,32,32]>  ::Add(%\"view_182\", %\"add_118\")\n",
       "            1930 |  # node_Reshape_5776\n",
       "                    %\"val_5890\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_119\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1931 |  # node_InstanceNormalization_5783\n",
       "                    %\"val_5897\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5890\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1932 |  # node_Reshape_5785\n",
       "                    %\"val_5899\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5897\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1933 |  # node_Mul_5792\n",
       "                    %\"val_5906\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5899\", %\"val_5905\"{...})\n",
       "            1934 |  # node_group_norm_64\n",
       "                    %\"group_norm_64\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5906\", %\"val_5907\"{...})\n",
       "            1935 |  # node_Sigmoid_5794\n",
       "                    %\"val_5908\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_64\")\n",
       "            1936 |  # node_silu_70\n",
       "                    %\"silu_70\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_64\", %\"val_5908\")\n",
       "            1937 |  # node_conv2d_102\n",
       "                    %\"conv2d_102\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_70\", %\"vae.decoder.mid_block.resnets.1.conv1.weight\"{...}, %\"vae.decoder.mid_block.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1938 |  # node_Reshape_5799\n",
       "                    %\"val_5913\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_102\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1939 |  # node_InstanceNormalization_5806\n",
       "                    %\"val_5920\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5913\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1940 |  # node_Reshape_5808\n",
       "                    %\"val_5922\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5920\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1941 |  # node_Mul_5815\n",
       "                    %\"val_5929\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5922\", %\"val_5928\"{...})\n",
       "            1942 |  # node_group_norm_65\n",
       "                    %\"group_norm_65\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5929\", %\"val_5930\"{...})\n",
       "            1943 |  # node_Sigmoid_5817\n",
       "                    %\"val_5931\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_65\")\n",
       "            1944 |  # node_silu_71\n",
       "                    %\"silu_71\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_65\", %\"val_5931\")\n",
       "            1945 |  # node_conv2d_103\n",
       "                    %\"conv2d_103\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_71\", %\"vae.decoder.mid_block.resnets.1.conv2.weight\"{...}, %\"vae.decoder.mid_block.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1946 |  # node_add_120\n",
       "                    %\"add_120\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_119\", %\"conv2d_103\")\n",
       "            1947 |  # node_Reshape_5822\n",
       "                    %\"val_5936\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_120\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1948 |  # node_InstanceNormalization_5829\n",
       "                    %\"val_5943\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5936\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1949 |  # node_Reshape_5831\n",
       "                    %\"val_5945\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5943\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1950 |  # node_Mul_5838\n",
       "                    %\"val_5952\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5945\", %\"val_5951\"{...})\n",
       "            1951 |  # node_group_norm_66\n",
       "                    %\"group_norm_66\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5952\", %\"val_5953\"{...})\n",
       "            1952 |  # node_Sigmoid_5840\n",
       "                    %\"val_5954\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_66\")\n",
       "            1953 |  # node_silu_72\n",
       "                    %\"silu_72\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_66\", %\"val_5954\")\n",
       "            1954 |  # node_conv2d_104\n",
       "                    %\"conv2d_104\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_72\", %\"vae.decoder.up_blocks.0.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1955 |  # node_Reshape_5845\n",
       "                    %\"val_5959\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_104\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1956 |  # node_InstanceNormalization_5852\n",
       "                    %\"val_5966\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5959\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1957 |  # node_Reshape_5854\n",
       "                    %\"val_5968\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5966\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1958 |  # node_Mul_5861\n",
       "                    %\"val_5975\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5968\", %\"val_5974\"{...})\n",
       "            1959 |  # node_group_norm_67\n",
       "                    %\"group_norm_67\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5975\", %\"val_5976\"{...})\n",
       "            1960 |  # node_Sigmoid_5863\n",
       "                    %\"val_5977\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_67\")\n",
       "            1961 |  # node_silu_73\n",
       "                    %\"silu_73\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_67\", %\"val_5977\")\n",
       "            1962 |  # node_conv2d_105\n",
       "                    %\"conv2d_105\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_73\", %\"vae.decoder.up_blocks.0.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1963 |  # node_add_121\n",
       "                    %\"add_121\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_120\", %\"conv2d_105\")\n",
       "            1964 |  # node_Reshape_5868\n",
       "                    %\"val_5982\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_121\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1965 |  # node_InstanceNormalization_5875\n",
       "                    %\"val_5989\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_5982\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1966 |  # node_Reshape_5877\n",
       "                    %\"val_5991\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_5989\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1967 |  # node_Mul_5884\n",
       "                    %\"val_5998\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_5991\", %\"val_5997\"{...})\n",
       "            1968 |  # node_group_norm_68\n",
       "                    %\"group_norm_68\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_5998\", %\"val_5999\"{...})\n",
       "            1969 |  # node_Sigmoid_5886\n",
       "                    %\"val_6000\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_68\")\n",
       "            1970 |  # node_silu_74\n",
       "                    %\"silu_74\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_68\", %\"val_6000\")\n",
       "            1971 |  # node_conv2d_106\n",
       "                    %\"conv2d_106\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_74\", %\"vae.decoder.up_blocks.0.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1972 |  # node_Reshape_5891\n",
       "                    %\"val_6005\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_106\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1973 |  # node_InstanceNormalization_5898\n",
       "                    %\"val_6012\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6005\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1974 |  # node_Reshape_5900\n",
       "                    %\"val_6014\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6012\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1975 |  # node_Mul_5907\n",
       "                    %\"val_6021\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6014\", %\"val_6020\"{...})\n",
       "            1976 |  # node_group_norm_69\n",
       "                    %\"group_norm_69\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6021\", %\"val_6022\"{...})\n",
       "            1977 |  # node_Sigmoid_5909\n",
       "                    %\"val_6023\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_69\")\n",
       "            1978 |  # node_silu_75\n",
       "                    %\"silu_75\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_69\", %\"val_6023\")\n",
       "            1979 |  # node_conv2d_107\n",
       "                    %\"conv2d_107\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_75\", %\"vae.decoder.up_blocks.0.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1980 |  # node_add_122\n",
       "                    %\"add_122\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_121\", %\"conv2d_107\")\n",
       "            1981 |  # node_Reshape_5914\n",
       "                    %\"val_6028\"<FLOAT,[8,32,16384]>  ::Reshape(%\"add_122\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1982 |  # node_InstanceNormalization_5921\n",
       "                    %\"val_6035\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6028\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1983 |  # node_Reshape_5923\n",
       "                    %\"val_6037\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6035\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1984 |  # node_Mul_5930\n",
       "                    %\"val_6044\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6037\", %\"val_6043\"{...})\n",
       "            1985 |  # node_group_norm_70\n",
       "                    %\"group_norm_70\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6044\", %\"val_6045\"{...})\n",
       "            1986 |  # node_Sigmoid_5932\n",
       "                    %\"val_6046\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_70\")\n",
       "            1987 |  # node_silu_76\n",
       "                    %\"silu_76\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_70\", %\"val_6046\")\n",
       "            1988 |  # node_conv2d_108\n",
       "                    %\"conv2d_108\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_76\", %\"vae.decoder.up_blocks.0.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1989 |  # node_Reshape_5937\n",
       "                    %\"val_6051\"<FLOAT,[8,32,16384]>  ::Reshape(%\"conv2d_108\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            1990 |  # node_InstanceNormalization_5944\n",
       "                    %\"val_6058\"<FLOAT,[8,32,16384]>  ::InstanceNormalization(%\"val_6051\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            1991 |  # node_Reshape_5946\n",
       "                    %\"val_6060\"<FLOAT,[8,512,32,32]>  ::Reshape(%\"val_6058\", %\"val_5885\"{[8, 512, 32, 32]}) {allowzero=0}\n",
       "            1992 |  # node_Mul_5953\n",
       "                    %\"val_6067\"<FLOAT,[8,512,32,32]>  ::Mul(%\"val_6060\", %\"val_6066\"{...})\n",
       "            1993 |  # node_group_norm_71\n",
       "                    %\"group_norm_71\"<FLOAT,[8,512,32,32]>  ::Add(%\"val_6067\", %\"val_6068\"{...})\n",
       "            1994 |  # node_Sigmoid_5955\n",
       "                    %\"val_6069\"<FLOAT,[8,512,32,32]>  ::Sigmoid(%\"group_norm_71\")\n",
       "            1995 |  # node_silu_77\n",
       "                    %\"silu_77\"<FLOAT,[8,512,32,32]>  ::Mul(%\"group_norm_71\", %\"val_6069\")\n",
       "            1996 |  # node_conv2d_109\n",
       "                    %\"conv2d_109\"<FLOAT,[8,512,32,32]>  ::Conv(%\"silu_77\", %\"vae.decoder.up_blocks.0.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.0.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1997 |  # node_add_123\n",
       "                    %\"add_123\"<FLOAT,[8,512,32,32]>  ::Add(%\"add_122\", %\"conv2d_109\")\n",
       "            1998 |  # node_upsample_nearest2d_3\n",
       "                    %\"upsample_nearest2d_3\"<FLOAT,[8,512,64,64]>  ::Resize(%\"add_123\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            1999 |  # node_conv2d_110\n",
       "                    %\"conv2d_110\"<FLOAT,[8,512,64,64]>  ::Conv(%\"upsample_nearest2d_3\", %\"vae.decoder.up_blocks.0.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.0.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2000 |  # node_Reshape_5961\n",
       "                    %\"val_6075\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_110\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2001 |  # node_InstanceNormalization_5968\n",
       "                    %\"val_6082\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6075\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2002 |  # node_Reshape_5970\n",
       "                    %\"val_6084\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6082\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2003 |  # node_Mul_5977\n",
       "                    %\"val_6091\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6084\", %\"val_6090\"{...})\n",
       "            2004 |  # node_group_norm_72\n",
       "                    %\"group_norm_72\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6091\", %\"val_6092\"{...})\n",
       "            2005 |  # node_Sigmoid_5979\n",
       "                    %\"val_6093\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_72\")\n",
       "            2006 |  # node_silu_78\n",
       "                    %\"silu_78\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_72\", %\"val_6093\")\n",
       "            2007 |  # node_conv2d_111\n",
       "                    %\"conv2d_111\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_78\", %\"vae.decoder.up_blocks.1.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2008 |  # node_Reshape_5984\n",
       "                    %\"val_6098\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_111\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2009 |  # node_InstanceNormalization_5991\n",
       "                    %\"val_6105\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6098\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2010 |  # node_Reshape_5993\n",
       "                    %\"val_6107\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6105\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2011 |  # node_Mul_6000\n",
       "                    %\"val_6114\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6107\", %\"val_6113\"{...})\n",
       "            2012 |  # node_group_norm_73\n",
       "                    %\"group_norm_73\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6114\", %\"val_6115\"{...})\n",
       "            2013 |  # node_Sigmoid_6002\n",
       "                    %\"val_6116\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_73\")\n",
       "            2014 |  # node_silu_79\n",
       "                    %\"silu_79\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_73\", %\"val_6116\")\n",
       "            2015 |  # node_conv2d_112\n",
       "                    %\"conv2d_112\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_79\", %\"vae.decoder.up_blocks.1.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2016 |  # node_add_124\n",
       "                    %\"add_124\"<FLOAT,[8,512,64,64]>  ::Add(%\"conv2d_110\", %\"conv2d_112\")\n",
       "            2017 |  # node_Reshape_6007\n",
       "                    %\"val_6121\"<FLOAT,[8,32,65536]>  ::Reshape(%\"add_124\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2018 |  # node_InstanceNormalization_6014\n",
       "                    %\"val_6128\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6121\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2019 |  # node_Reshape_6016\n",
       "                    %\"val_6130\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6128\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2020 |  # node_Mul_6023\n",
       "                    %\"val_6137\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6130\", %\"val_6136\"{...})\n",
       "            2021 |  # node_group_norm_74\n",
       "                    %\"group_norm_74\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6137\", %\"val_6138\"{...})\n",
       "            2022 |  # node_Sigmoid_6025\n",
       "                    %\"val_6139\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_74\")\n",
       "            2023 |  # node_silu_80\n",
       "                    %\"silu_80\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_74\", %\"val_6139\")\n",
       "            2024 |  # node_conv2d_113\n",
       "                    %\"conv2d_113\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_80\", %\"vae.decoder.up_blocks.1.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2025 |  # node_Reshape_6030\n",
       "                    %\"val_6144\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_113\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2026 |  # node_InstanceNormalization_6037\n",
       "                    %\"val_6151\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6144\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2027 |  # node_Reshape_6039\n",
       "                    %\"val_6153\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6151\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2028 |  # node_Mul_6046\n",
       "                    %\"val_6160\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6153\", %\"val_6159\"{...})\n",
       "            2029 |  # node_group_norm_75\n",
       "                    %\"group_norm_75\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6160\", %\"val_6161\"{...})\n",
       "            2030 |  # node_Sigmoid_6048\n",
       "                    %\"val_6162\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_75\")\n",
       "            2031 |  # node_silu_81\n",
       "                    %\"silu_81\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_75\", %\"val_6162\")\n",
       "            2032 |  # node_conv2d_114\n",
       "                    %\"conv2d_114\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_81\", %\"vae.decoder.up_blocks.1.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2033 |  # node_add_125\n",
       "                    %\"add_125\"<FLOAT,[8,512,64,64]>  ::Add(%\"add_124\", %\"conv2d_114\")\n",
       "            2034 |  # node_Reshape_6053\n",
       "                    %\"val_6167\"<FLOAT,[8,32,65536]>  ::Reshape(%\"add_125\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2035 |  # node_InstanceNormalization_6060\n",
       "                    %\"val_6174\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6167\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2036 |  # node_Reshape_6062\n",
       "                    %\"val_6176\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6174\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2037 |  # node_Mul_6069\n",
       "                    %\"val_6183\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6176\", %\"val_6182\"{...})\n",
       "            2038 |  # node_group_norm_76\n",
       "                    %\"group_norm_76\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6183\", %\"val_6184\"{...})\n",
       "            2039 |  # node_Sigmoid_6071\n",
       "                    %\"val_6185\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_76\")\n",
       "            2040 |  # node_silu_82\n",
       "                    %\"silu_82\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_76\", %\"val_6185\")\n",
       "            2041 |  # node_conv2d_115\n",
       "                    %\"conv2d_115\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_82\", %\"vae.decoder.up_blocks.1.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2042 |  # node_Reshape_6076\n",
       "                    %\"val_6190\"<FLOAT,[8,32,65536]>  ::Reshape(%\"conv2d_115\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2043 |  # node_InstanceNormalization_6083\n",
       "                    %\"val_6197\"<FLOAT,[8,32,65536]>  ::InstanceNormalization(%\"val_6190\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2044 |  # node_Reshape_6085\n",
       "                    %\"val_6199\"<FLOAT,[8,512,64,64]>  ::Reshape(%\"val_6197\", %\"val_6083\"{[8, 512, 64, 64]}) {allowzero=0}\n",
       "            2045 |  # node_Mul_6092\n",
       "                    %\"val_6206\"<FLOAT,[8,512,64,64]>  ::Mul(%\"val_6199\", %\"val_6205\"{...})\n",
       "            2046 |  # node_group_norm_77\n",
       "                    %\"group_norm_77\"<FLOAT,[8,512,64,64]>  ::Add(%\"val_6206\", %\"val_6207\"{...})\n",
       "            2047 |  # node_Sigmoid_6094\n",
       "                    %\"val_6208\"<FLOAT,[8,512,64,64]>  ::Sigmoid(%\"group_norm_77\")\n",
       "            2048 |  # node_silu_83\n",
       "                    %\"silu_83\"<FLOAT,[8,512,64,64]>  ::Mul(%\"group_norm_77\", %\"val_6208\")\n",
       "            2049 |  # node_conv2d_116\n",
       "                    %\"conv2d_116\"<FLOAT,[8,512,64,64]>  ::Conv(%\"silu_83\", %\"vae.decoder.up_blocks.1.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.1.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2050 |  # node_add_126\n",
       "                    %\"add_126\"<FLOAT,[8,512,64,64]>  ::Add(%\"add_125\", %\"conv2d_116\")\n",
       "            2051 |  # node_upsample_nearest2d_4\n",
       "                    %\"upsample_nearest2d_4\"<FLOAT,[8,512,128,128]>  ::Resize(%\"add_126\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            2052 |  # node_conv2d_117\n",
       "                    %\"conv2d_117\"<FLOAT,[8,512,128,128]>  ::Conv(%\"upsample_nearest2d_4\", %\"vae.decoder.up_blocks.1.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.1.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2053 |  # node_Reshape_6100\n",
       "                    %\"val_6214\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_117\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2054 |  # node_InstanceNormalization_6107\n",
       "                    %\"val_6221\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6214\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2055 |  # node_Reshape_6109\n",
       "                    %\"val_6223\"<FLOAT,[8,512,128,128]>  ::Reshape(%\"val_6221\", %\"val_6222\"{[8, 512, 128, 128]}) {allowzero=0}\n",
       "            2056 |  # node_Mul_6116\n",
       "                    %\"val_6230\"<FLOAT,[8,512,128,128]>  ::Mul(%\"val_6223\", %\"val_6229\"{...})\n",
       "            2057 |  # node_group_norm_78\n",
       "                    %\"group_norm_78\"<FLOAT,[8,512,128,128]>  ::Add(%\"val_6230\", %\"val_6231\"{...})\n",
       "            2058 |  # node_Sigmoid_6118\n",
       "                    %\"val_6232\"<FLOAT,[8,512,128,128]>  ::Sigmoid(%\"group_norm_78\")\n",
       "            2059 |  # node_silu_84\n",
       "                    %\"silu_84\"<FLOAT,[8,512,128,128]>  ::Mul(%\"group_norm_78\", %\"val_6232\")\n",
       "            2060 |  # node_conv2d_118\n",
       "                    %\"conv2d_118\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_84\", %\"vae.decoder.up_blocks.2.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2061 |  # node_Reshape_6123\n",
       "                    %\"val_6237\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_118\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2062 |  # node_InstanceNormalization_6130\n",
       "                    %\"val_6244\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6237\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2063 |  # node_Reshape_6132\n",
       "                    %\"val_6246\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6244\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2064 |  # node_Mul_6139\n",
       "                    %\"val_6253\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6246\", %\"val_6252\"{...})\n",
       "            2065 |  # node_group_norm_79\n",
       "                    %\"group_norm_79\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6253\", %\"val_6254\"{...})\n",
       "            2066 |  # node_Sigmoid_6141\n",
       "                    %\"val_6255\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_79\")\n",
       "            2067 |  # node_silu_85\n",
       "                    %\"silu_85\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_79\", %\"val_6255\")\n",
       "            2068 |  # node_conv2d_119\n",
       "                    %\"conv2d_119\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_85\", %\"vae.decoder.up_blocks.2.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2069 |  # node_conv2d_120\n",
       "                    %\"conv2d_120\"<FLOAT,[8,256,128,128]>  ::Conv(%\"conv2d_117\", %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2070 |  # node_add_127\n",
       "                    %\"add_127\"<FLOAT,[8,256,128,128]>  ::Add(%\"conv2d_120\", %\"conv2d_119\")\n",
       "            2071 |  # node_Reshape_6146\n",
       "                    %\"val_6260\"<FLOAT,[8,32,131072]>  ::Reshape(%\"add_127\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2072 |  # node_InstanceNormalization_6153\n",
       "                    %\"val_6267\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6260\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2073 |  # node_Reshape_6155\n",
       "                    %\"val_6269\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6267\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2074 |  # node_Mul_6162\n",
       "                    %\"val_6276\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6269\", %\"val_6275\"{...})\n",
       "            2075 |  # node_group_norm_80\n",
       "                    %\"group_norm_80\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6276\", %\"val_6277\"{...})\n",
       "            2076 |  # node_Sigmoid_6164\n",
       "                    %\"val_6278\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_80\")\n",
       "            2077 |  # node_silu_86\n",
       "                    %\"silu_86\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_80\", %\"val_6278\")\n",
       "            2078 |  # node_conv2d_121\n",
       "                    %\"conv2d_121\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_86\", %\"vae.decoder.up_blocks.2.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2079 |  # node_Reshape_6169\n",
       "                    %\"val_6283\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_121\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2080 |  # node_InstanceNormalization_6176\n",
       "                    %\"val_6290\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6283\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2081 |  # node_Reshape_6178\n",
       "                    %\"val_6292\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6290\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2082 |  # node_Mul_6185\n",
       "                    %\"val_6299\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6292\", %\"val_6298\"{...})\n",
       "            2083 |  # node_group_norm_81\n",
       "                    %\"group_norm_81\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6299\", %\"val_6300\"{...})\n",
       "            2084 |  # node_Sigmoid_6187\n",
       "                    %\"val_6301\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_81\")\n",
       "            2085 |  # node_silu_87\n",
       "                    %\"silu_87\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_81\", %\"val_6301\")\n",
       "            2086 |  # node_conv2d_122\n",
       "                    %\"conv2d_122\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_87\", %\"vae.decoder.up_blocks.2.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2087 |  # node_add_128\n",
       "                    %\"add_128\"<FLOAT,[8,256,128,128]>  ::Add(%\"add_127\", %\"conv2d_122\")\n",
       "            2088 |  # node_Reshape_6192\n",
       "                    %\"val_6306\"<FLOAT,[8,32,131072]>  ::Reshape(%\"add_128\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2089 |  # node_InstanceNormalization_6199\n",
       "                    %\"val_6313\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6306\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2090 |  # node_Reshape_6201\n",
       "                    %\"val_6315\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6313\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2091 |  # node_Mul_6208\n",
       "                    %\"val_6322\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6315\", %\"val_6321\"{...})\n",
       "            2092 |  # node_group_norm_82\n",
       "                    %\"group_norm_82\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6322\", %\"val_6323\"{...})\n",
       "            2093 |  # node_Sigmoid_6210\n",
       "                    %\"val_6324\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_82\")\n",
       "            2094 |  # node_silu_88\n",
       "                    %\"silu_88\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_82\", %\"val_6324\")\n",
       "            2095 |  # node_conv2d_123\n",
       "                    %\"conv2d_123\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_88\", %\"vae.decoder.up_blocks.2.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2096 |  # node_Reshape_6215\n",
       "                    %\"val_6329\"<FLOAT,[8,32,131072]>  ::Reshape(%\"conv2d_123\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2097 |  # node_InstanceNormalization_6222\n",
       "                    %\"val_6336\"<FLOAT,[8,32,131072]>  ::InstanceNormalization(%\"val_6329\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2098 |  # node_Reshape_6224\n",
       "                    %\"val_6338\"<FLOAT,[8,256,128,128]>  ::Reshape(%\"val_6336\", %\"val_6245\"{[8, 256, 128, 128]}) {allowzero=0}\n",
       "            2099 |  # node_Mul_6231\n",
       "                    %\"val_6345\"<FLOAT,[8,256,128,128]>  ::Mul(%\"val_6338\", %\"val_6344\"{...})\n",
       "            2100 |  # node_group_norm_83\n",
       "                    %\"group_norm_83\"<FLOAT,[8,256,128,128]>  ::Add(%\"val_6345\", %\"val_6346\"{...})\n",
       "            2101 |  # node_Sigmoid_6233\n",
       "                    %\"val_6347\"<FLOAT,[8,256,128,128]>  ::Sigmoid(%\"group_norm_83\")\n",
       "            2102 |  # node_silu_89\n",
       "                    %\"silu_89\"<FLOAT,[8,256,128,128]>  ::Mul(%\"group_norm_83\", %\"val_6347\")\n",
       "            2103 |  # node_conv2d_124\n",
       "                    %\"conv2d_124\"<FLOAT,[8,256,128,128]>  ::Conv(%\"silu_89\", %\"vae.decoder.up_blocks.2.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.2.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2104 |  # node_add_129\n",
       "                    %\"add_129\"<FLOAT,[8,256,128,128]>  ::Add(%\"add_128\", %\"conv2d_124\")\n",
       "            2105 |  # node_upsample_nearest2d_5\n",
       "                    %\"upsample_nearest2d_5\"<FLOAT,[8,256,256,256]>  ::Resize(%\"add_129\", None, %\"val_3902\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='asymmetric', cubic_coeff_a=-0.75, mode='nearest'}\n",
       "            2106 |  # node_conv2d_125\n",
       "                    %\"conv2d_125\"<FLOAT,[8,256,256,256]>  ::Conv(%\"upsample_nearest2d_5\", %\"vae.decoder.up_blocks.2.upsamplers.0.conv.weight\"{...}, %\"vae.decoder.up_blocks.2.upsamplers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2107 |  # node_Reshape_6239\n",
       "                    %\"val_6353\"<FLOAT,[8,32,524288]>  ::Reshape(%\"conv2d_125\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2108 |  # node_InstanceNormalization_6246\n",
       "                    %\"val_6360\"<FLOAT,[8,32,524288]>  ::InstanceNormalization(%\"val_6353\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2109 |  # node_Reshape_6248\n",
       "                    %\"val_6362\"<FLOAT,[8,256,256,256]>  ::Reshape(%\"val_6360\", %\"val_6361\"{[8, 256, 256, 256]}) {allowzero=0}\n",
       "            2110 |  # node_Mul_6255\n",
       "                    %\"val_6369\"<FLOAT,[8,256,256,256]>  ::Mul(%\"val_6362\", %\"val_6368\"{...})\n",
       "            2111 |  # node_group_norm_84\n",
       "                    %\"group_norm_84\"<FLOAT,[8,256,256,256]>  ::Add(%\"val_6369\", %\"val_6370\"{...})\n",
       "            2112 |  # node_Sigmoid_6257\n",
       "                    %\"val_6371\"<FLOAT,[8,256,256,256]>  ::Sigmoid(%\"group_norm_84\")\n",
       "            2113 |  # node_silu_90\n",
       "                    %\"silu_90\"<FLOAT,[8,256,256,256]>  ::Mul(%\"group_norm_84\", %\"val_6371\")\n",
       "            2114 |  # node_conv2d_126\n",
       "                    %\"conv2d_126\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_90\", %\"vae.decoder.up_blocks.3.resnets.0.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2115 |  # node_Reshape_6262\n",
       "                    %\"val_6376\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_126\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2116 |  # node_InstanceNormalization_6269\n",
       "                    %\"val_6383\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6376\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2117 |  # node_Reshape_6271\n",
       "                    %\"val_6385\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6383\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2118 |  # node_Mul_6278\n",
       "                    %\"val_6392\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6385\", %\"val_6391\"{...})\n",
       "            2119 |  # node_group_norm_85\n",
       "                    %\"group_norm_85\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6392\", %\"val_6393\"{...})\n",
       "            2120 |  # node_Sigmoid_6280\n",
       "                    %\"val_6394\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_85\")\n",
       "            2121 |  # node_silu_91\n",
       "                    %\"silu_91\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_85\", %\"val_6394\")\n",
       "            2122 |  # node_conv2d_127\n",
       "                    %\"conv2d_127\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_91\", %\"vae.decoder.up_blocks.3.resnets.0.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2123 |  # node_conv2d_128\n",
       "                    %\"conv2d_128\"<FLOAT,[8,128,256,256]>  ::Conv(%\"conv2d_125\", %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2124 |  # node_add_130\n",
       "                    %\"add_130\"<FLOAT,[8,128,256,256]>  ::Add(%\"conv2d_128\", %\"conv2d_127\")\n",
       "            2125 |  # node_Reshape_6285\n",
       "                    %\"val_6399\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_130\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2126 |  # node_InstanceNormalization_6292\n",
       "                    %\"val_6406\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6399\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2127 |  # node_Reshape_6294\n",
       "                    %\"val_6408\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6406\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2128 |  # node_Mul_6301\n",
       "                    %\"val_6415\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6408\", %\"val_6414\"{...})\n",
       "            2129 |  # node_group_norm_86\n",
       "                    %\"group_norm_86\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6415\", %\"val_6416\"{...})\n",
       "            2130 |  # node_Sigmoid_6303\n",
       "                    %\"val_6417\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_86\")\n",
       "            2131 |  # node_silu_92\n",
       "                    %\"silu_92\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_86\", %\"val_6417\")\n",
       "            2132 |  # node_conv2d_129\n",
       "                    %\"conv2d_129\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_92\", %\"vae.decoder.up_blocks.3.resnets.1.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2133 |  # node_Reshape_6308\n",
       "                    %\"val_6422\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_129\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2134 |  # node_InstanceNormalization_6315\n",
       "                    %\"val_6429\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6422\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2135 |  # node_Reshape_6317\n",
       "                    %\"val_6431\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6429\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2136 |  # node_Mul_6324\n",
       "                    %\"val_6438\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6431\", %\"val_6437\"{...})\n",
       "            2137 |  # node_group_norm_87\n",
       "                    %\"group_norm_87\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6438\", %\"val_6439\"{...})\n",
       "            2138 |  # node_Sigmoid_6326\n",
       "                    %\"val_6440\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_87\")\n",
       "            2139 |  # node_silu_93\n",
       "                    %\"silu_93\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_87\", %\"val_6440\")\n",
       "            2140 |  # node_conv2d_130\n",
       "                    %\"conv2d_130\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_93\", %\"vae.decoder.up_blocks.3.resnets.1.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2141 |  # node_add_131\n",
       "                    %\"add_131\"<FLOAT,[8,128,256,256]>  ::Add(%\"add_130\", %\"conv2d_130\")\n",
       "            2142 |  # node_Reshape_6331\n",
       "                    %\"val_6445\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_131\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2143 |  # node_InstanceNormalization_6338\n",
       "                    %\"val_6452\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6445\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2144 |  # node_Reshape_6340\n",
       "                    %\"val_6454\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6452\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2145 |  # node_Mul_6347\n",
       "                    %\"val_6461\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6454\", %\"val_6460\"{...})\n",
       "            2146 |  # node_group_norm_88\n",
       "                    %\"group_norm_88\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6461\", %\"val_6462\"{...})\n",
       "            2147 |  # node_Sigmoid_6349\n",
       "                    %\"val_6463\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_88\")\n",
       "            2148 |  # node_silu_94\n",
       "                    %\"silu_94\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_88\", %\"val_6463\")\n",
       "            2149 |  # node_conv2d_131\n",
       "                    %\"conv2d_131\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_94\", %\"vae.decoder.up_blocks.3.resnets.2.conv1.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2150 |  # node_Reshape_6354\n",
       "                    %\"val_6468\"<FLOAT,[8,32,262144]>  ::Reshape(%\"conv2d_131\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2151 |  # node_InstanceNormalization_6361\n",
       "                    %\"val_6475\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6468\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2152 |  # node_Reshape_6363\n",
       "                    %\"val_6477\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6475\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2153 |  # node_Mul_6370\n",
       "                    %\"val_6484\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6477\", %\"val_6483\"{...})\n",
       "            2154 |  # node_group_norm_89\n",
       "                    %\"group_norm_89\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6484\", %\"val_6485\"{...})\n",
       "            2155 |  # node_Sigmoid_6372\n",
       "                    %\"val_6486\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_89\")\n",
       "            2156 |  # node_silu_95\n",
       "                    %\"silu_95\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_89\", %\"val_6486\")\n",
       "            2157 |  # node_conv2d_132\n",
       "                    %\"conv2d_132\"<FLOAT,[8,128,256,256]>  ::Conv(%\"silu_95\", %\"vae.decoder.up_blocks.3.resnets.2.conv2.weight\"{...}, %\"vae.decoder.up_blocks.3.resnets.2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2158 |  # node_add_132\n",
       "                    %\"add_132\"<FLOAT,[8,128,256,256]>  ::Add(%\"add_131\", %\"conv2d_132\")\n",
       "            2159 |  # node_Reshape_6377\n",
       "                    %\"val_6491\"<FLOAT,[8,32,262144]>  ::Reshape(%\"add_132\", %\"val_2203\"{[0, 32, -1]}) {allowzero=0}\n",
       "            2160 |  # node_InstanceNormalization_6384\n",
       "                    %\"val_6498\"<FLOAT,[8,32,262144]>  ::InstanceNormalization(%\"val_6491\", %\"val_2207\"{...}, %\"val_2210\"{...}) {epsilon=1e-06}\n",
       "            2161 |  # node_Reshape_6386\n",
       "                    %\"val_6500\"<FLOAT,[8,128,256,256]>  ::Reshape(%\"val_6498\", %\"val_6384\"{[8, 128, 256, 256]}) {allowzero=0}\n",
       "            2162 |  # node_Mul_6393\n",
       "                    %\"val_6507\"<FLOAT,[8,128,256,256]>  ::Mul(%\"val_6500\", %\"val_6506\"{...})\n",
       "            2163 |  # node_group_norm_90\n",
       "                    %\"group_norm_90\"<FLOAT,[8,128,256,256]>  ::Add(%\"val_6507\", %\"val_6508\"{...})\n",
       "            2164 |  # node_Sigmoid_6395\n",
       "                    %\"val_6509\"<FLOAT,[8,128,256,256]>  ::Sigmoid(%\"group_norm_90\")\n",
       "            2165 |  # node_silu_96\n",
       "                    %\"silu_96\"<FLOAT,[8,128,256,256]>  ::Mul(%\"group_norm_90\", %\"val_6509\")\n",
       "            2166 |  # node_conv2d_133\n",
       "                    %\"conv2d_133\"<FLOAT,[8,3,256,256]>  ::Conv(%\"silu_96\", %\"vae.decoder.conv_out.weight\"{...}, %\"vae.decoder.conv_out.bias\"{[0.015753211453557014, -0.02031349577009678, -0.046358201652765274]}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            2167 |  # node_div_70\n",
       "                    %\"div_70\"<FLOAT,[8,3,256,256]>  ::Div(%\"conv2d_133\", %\"scalar_tensor_default_7\"{2.0})\n",
       "            2168 |  # node_add_133\n",
       "                    %\"add_133\"<FLOAT,[8,3,256,256]>  ::Add(%\"div_70\", %\"val_6510\"{0.5})\n",
       "            2169 |  # node_Clip_7830\n",
       "                    %\"clamp\"<FLOAT,[8,3,256,256]>  ::Clip(%\"add_133\", %\"add_133_min\"{0.0}, %\"add_133_max\"{1.0})\n",
       "            2170 |  # node_permute_33\n",
       "                    %\"permute_33\"<FLOAT,[8,256,256,3]>  ::Transpose(%\"clamp\") {perm=(0, 2, 3, 1)}\n",
       "            2171 |  # node_multiply\n",
       "                    %\"multiply\"<FLOAT,[8,256,256,3]>  ::Mul(%\"permute_33\", %\"clone_126\"{255.0})\n",
       "            2172 |  # n6\n",
       "                    %\"rounded\"<FLOAT,[8,256,256,3]>  ::Round(%\"multiply\")\n",
       "            2173 |  # node__to_copy_5\n",
       "                    %\"_to_copy_5\"<UINT8,[8,256,256,3]>  ::Cast(%\"rounded\") {to=2}\n",
       "            2174 |  # node_flip\n",
       "                    %\"alias\"<UINT8,[8,256,256,3]>  ::Slice(%\"_to_copy_5\", %\"val_6518\"{[-1]}, %\"val_6519\"{[-9223372036854775808]}, %\"val_289\"{[1]}, %\"val_6518\"{[-1]})\n",
       "            return %\"alias\"<UINT8,[8,256,256,3]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_whisper_encoder_embed_positions_weight: \"f32[1500, 384]\", p_whisper_encoder_conv1_weight: \"f32[384, 80, 3]\", p_whisper_encoder_conv1_bias: \"f32[384]\", p_whisper_encoder_conv2_weight: \"f32[384, 384, 3]\", p_whisper_encoder_conv2_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_0_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_0_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_0_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_0_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_0_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_0_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_0_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_1_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_1_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_1_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_1_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_1_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_1_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_1_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_2_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_2_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_2_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_2_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_2_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_2_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_2_fc2_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_q_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_q_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_k_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_v_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_v_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_self_attn_out_proj_weight: \"f32[384, 384]\", p_whisper_encoder_layers_3_self_attn_out_proj_bias: \"f32[384]\", p_whisper_encoder_layers_3_final_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layers_3_final_layer_norm_bias: \"f32[384]\", p_whisper_encoder_layers_3_fc1_weight: \"f32[1536, 384]\", p_whisper_encoder_layers_3_fc1_bias: \"f32[1536]\", p_whisper_encoder_layers_3_fc2_weight: \"f32[384, 1536]\", p_whisper_encoder_layers_3_fc2_bias: \"f32[384]\", p_whisper_encoder_layer_norm_weight: \"f32[384]\", p_whisper_encoder_layer_norm_bias: \"f32[384]\", p_unet_time_embedding_linear_1_weight: \"f32[1280, 320]\", p_unet_time_embedding_linear_1_bias: \"f32[1280]\", p_unet_time_embedding_linear_2_weight: \"f32[1280, 1280]\", p_unet_time_embedding_linear_2_bias: \"f32[1280]\", p_unet_conv_in_weight: \"f32[320, 8, 3, 3]\", p_unet_conv_in_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_conv1_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_0_conv1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_resnets_0_time_emb_proj_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_0_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_0_conv2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_conv1_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_1_conv1_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_resnets_1_time_emb_proj_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_resnets_1_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_resnets_1_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_resnets_1_conv2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_norm_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_norm_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_0_proj_in_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_0_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_0_proj_out_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_norm_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_norm_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_1_proj_in_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_down_blocks_0_attentions_1_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_down_blocks_0_attentions_1_proj_out_bias: \"f32[320]\", p_unet_down_blocks_0_downsamplers_0_conv_weight: \"f32[320, 320, 3, 3]\", p_unet_down_blocks_0_downsamplers_0_conv_bias: \"f32[320]\", p_unet_down_blocks_1_resnets_0_norm1_weight: \"f32[320]\", p_unet_down_blocks_1_resnets_0_norm1_bias: \"f32[320]\", p_unet_down_blocks_1_resnets_0_conv1_weight: \"f32[640, 320, 3, 3]\", p_unet_down_blocks_1_resnets_0_conv1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_down_blocks_1_resnets_0_time_emb_proj_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_0_conv2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_0_conv_shortcut_weight: \"f32[640, 320, 1, 1]\", p_unet_down_blocks_1_resnets_0_conv_shortcut_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_conv1_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_1_conv1_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_down_blocks_1_resnets_1_time_emb_proj_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_resnets_1_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_resnets_1_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_resnets_1_conv2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_norm_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_norm_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_0_proj_in_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_0_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_0_proj_out_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_norm_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_norm_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_1_proj_in_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_down_blocks_1_attentions_1_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_down_blocks_1_attentions_1_proj_out_bias: \"f32[640]\", p_unet_down_blocks_1_downsamplers_0_conv_weight: \"f32[640, 640, 3, 3]\", p_unet_down_blocks_1_downsamplers_0_conv_bias: \"f32[640]\", p_unet_down_blocks_2_resnets_0_norm1_weight: \"f32[640]\", p_unet_down_blocks_2_resnets_0_norm1_bias: \"f32[640]\", p_unet_down_blocks_2_resnets_0_conv1_weight: \"f32[1280, 640, 3, 3]\", p_unet_down_blocks_2_resnets_0_conv1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_0_conv2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_0_conv_shortcut_weight: \"f32[1280, 640, 1, 1]\", p_unet_down_blocks_2_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_1_conv1_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_resnets_1_conv2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_norm_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_norm_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_norm_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_norm_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_1_proj_in_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_down_blocks_2_attentions_1_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_down_blocks_2_attentions_1_proj_out_bias: \"f32[1280]\", p_unet_down_blocks_2_downsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_2_downsamplers_0_conv_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm1_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_0_conv1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_3_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm2_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_norm2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_0_conv2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm1_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_1_conv1_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_down_blocks_3_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm2_weight: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_norm2_bias: \"f32[1280]\", p_unet_down_blocks_3_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_down_blocks_3_resnets_1_conv2_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_norm1_weight: \"f32[1280]\", p_unet_mid_block_resnets_0_norm1_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_0_conv1_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_mid_block_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_norm2_weight: \"f32[1280]\", p_unet_mid_block_resnets_0_norm2_bias: \"f32[1280]\", p_unet_mid_block_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_0_conv2_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_norm1_weight: \"f32[1280]\", p_unet_mid_block_resnets_1_norm1_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_conv1_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_1_conv1_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_mid_block_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_norm2_weight: \"f32[1280]\", p_unet_mid_block_resnets_1_norm2_bias: \"f32[1280]\", p_unet_mid_block_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_mid_block_resnets_1_conv2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_norm_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_norm_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_mid_block_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_mid_block_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_mid_block_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_0_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_0_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_0_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_0_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_0_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_1_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_1_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_1_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_1_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_1_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_1_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm1_weight: \"f32[2560]\", p_unet_up_blocks_0_resnets_2_norm1_bias: \"f32[2560]\", p_unet_up_blocks_0_resnets_2_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_0_resnets_2_conv1_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_0_resnets_2_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm2_weight: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_norm2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_resnets_2_conv2_bias: \"f32[1280]\", p_unet_up_blocks_0_resnets_2_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_0_resnets_2_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_0_upsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_0_upsamplers_0_conv_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm1_weight: \"f32[2560]\", p_unet_up_blocks_1_resnets_0_norm1_bias: \"f32[2560]\", p_unet_up_blocks_1_resnets_0_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_1_resnets_0_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_0_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_0_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_0_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_1_resnets_0_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm1_weight: \"f32[2560]\", p_unet_up_blocks_1_resnets_1_norm1_bias: \"f32[2560]\", p_unet_up_blocks_1_resnets_1_conv1_weight: \"f32[1280, 2560, 3, 3]\", p_unet_up_blocks_1_resnets_1_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_1_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_1_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_1_conv_shortcut_weight: \"f32[1280, 2560, 1, 1]\", p_unet_up_blocks_1_resnets_1_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm1_weight: \"f32[1920]\", p_unet_up_blocks_1_resnets_2_norm1_bias: \"f32[1920]\", p_unet_up_blocks_1_resnets_2_conv1_weight: \"f32[1280, 1920, 3, 3]\", p_unet_up_blocks_1_resnets_2_conv1_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_time_emb_proj_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_resnets_2_time_emb_proj_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_conv2_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_resnets_2_conv2_bias: \"f32[1280]\", p_unet_up_blocks_1_resnets_2_conv_shortcut_weight: \"f32[1280, 1920, 1, 1]\", p_unet_up_blocks_1_resnets_2_conv_shortcut_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_0_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_0_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_0_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_1_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_1_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_1_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_norm_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_norm_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_proj_in_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_2_proj_in_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[1280, 384]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[1280, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[10240, 1280]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[10240]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[1280, 5120]\", p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[1280]\", p_unet_up_blocks_1_attentions_2_proj_out_weight: \"f32[1280, 1280, 1, 1]\", p_unet_up_blocks_1_attentions_2_proj_out_bias: \"f32[1280]\", p_unet_up_blocks_1_upsamplers_0_conv_weight: \"f32[1280, 1280, 3, 3]\", p_unet_up_blocks_1_upsamplers_0_conv_bias: \"f32[1280]\", p_unet_up_blocks_2_resnets_0_norm1_weight: \"f32[1920]\", p_unet_up_blocks_2_resnets_0_norm1_bias: \"f32[1920]\", p_unet_up_blocks_2_resnets_0_conv1_weight: \"f32[640, 1920, 3, 3]\", p_unet_up_blocks_2_resnets_0_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_0_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_0_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_0_conv_shortcut_weight: \"f32[640, 1920, 1, 1]\", p_unet_up_blocks_2_resnets_0_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm1_weight: \"f32[1280]\", p_unet_up_blocks_2_resnets_1_norm1_bias: \"f32[1280]\", p_unet_up_blocks_2_resnets_1_conv1_weight: \"f32[640, 1280, 3, 3]\", p_unet_up_blocks_2_resnets_1_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_1_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_1_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_1_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_1_conv_shortcut_weight: \"f32[640, 1280, 1, 1]\", p_unet_up_blocks_2_resnets_1_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm1_weight: \"f32[960]\", p_unet_up_blocks_2_resnets_2_norm1_bias: \"f32[960]\", p_unet_up_blocks_2_resnets_2_conv1_weight: \"f32[640, 960, 3, 3]\", p_unet_up_blocks_2_resnets_2_conv1_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_time_emb_proj_weight: \"f32[640, 1280]\", p_unet_up_blocks_2_resnets_2_time_emb_proj_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_resnets_2_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_conv2_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_resnets_2_conv2_bias: \"f32[640]\", p_unet_up_blocks_2_resnets_2_conv_shortcut_weight: \"f32[640, 960, 1, 1]\", p_unet_up_blocks_2_resnets_2_conv_shortcut_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_0_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_0_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_0_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_1_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_1_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_1_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_norm_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_norm_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_proj_in_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_2_proj_in_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[640, 384]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[640, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[5120, 640]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[5120]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[640, 2560]\", p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[640]\", p_unet_up_blocks_2_attentions_2_proj_out_weight: \"f32[640, 640, 1, 1]\", p_unet_up_blocks_2_attentions_2_proj_out_bias: \"f32[640]\", p_unet_up_blocks_2_upsamplers_0_conv_weight: \"f32[640, 640, 3, 3]\", p_unet_up_blocks_2_upsamplers_0_conv_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_0_norm1_weight: \"f32[960]\", p_unet_up_blocks_3_resnets_0_norm1_bias: \"f32[960]\", p_unet_up_blocks_3_resnets_0_conv1_weight: \"f32[320, 960, 3, 3]\", p_unet_up_blocks_3_resnets_0_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_0_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_0_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_0_conv_shortcut_weight: \"f32[320, 960, 1, 1]\", p_unet_up_blocks_3_resnets_0_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm1_weight: \"f32[640]\", p_unet_up_blocks_3_resnets_1_norm1_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_1_conv1_weight: \"f32[320, 640, 3, 3]\", p_unet_up_blocks_3_resnets_1_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_1_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_1_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_1_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_1_conv_shortcut_weight: \"f32[320, 640, 1, 1]\", p_unet_up_blocks_3_resnets_1_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm1_weight: \"f32[640]\", p_unet_up_blocks_3_resnets_2_norm1_bias: \"f32[640]\", p_unet_up_blocks_3_resnets_2_conv1_weight: \"f32[320, 640, 3, 3]\", p_unet_up_blocks_3_resnets_2_conv1_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_time_emb_proj_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_resnets_2_time_emb_proj_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_resnets_2_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_conv2_weight: \"f32[320, 320, 3, 3]\", p_unet_up_blocks_3_resnets_2_conv2_bias: \"f32[320]\", p_unet_up_blocks_3_resnets_2_conv_shortcut_weight: \"f32[320, 640, 1, 1]\", p_unet_up_blocks_3_resnets_2_conv_shortcut_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_0_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_0_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_0_proj_out_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_1_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_1_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_1_proj_out_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_norm_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_norm_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_proj_in_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_2_proj_in_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight: \"f32[320, 384]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: \"f32[320, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: \"f32[2560, 320]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: \"f32[2560]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight: \"f32[320, 1280]\", p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias: \"f32[320]\", p_unet_up_blocks_3_attentions_2_proj_out_weight: \"f32[320, 320, 1, 1]\", p_unet_up_blocks_3_attentions_2_proj_out_bias: \"f32[320]\", p_unet_conv_norm_out_weight: \"f32[320]\", p_unet_conv_norm_out_bias: \"f32[320]\", p_unet_conv_out_weight: \"f32[4, 320, 3, 3]\", p_unet_conv_out_bias: \"f32[4]\", p_vae_post_quant_conv_weight: \"f32[4, 4, 1, 1]\", p_vae_post_quant_conv_bias: \"f32[4]\", p_vae_decoder_conv_in_weight: \"f32[512, 4, 3, 3]\", p_vae_decoder_conv_in_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_mid_block_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_mid_block_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_group_norm_weight: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_group_norm_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_q_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_q_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_k_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_k_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_v_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_v_bias: \"f32[512]\", p_vae_decoder_mid_block_attentions_0_to_out_0_weight: \"f32[512, 512]\", p_vae_decoder_mid_block_attentions_0_to_out_0_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_2_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_resnets_2_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_resnets_2_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_0_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_0_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_1_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_1_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_conv1_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_2_conv1_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm2_weight: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_norm2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_resnets_2_conv2_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_resnets_2_conv2_bias: \"f32[512]\", p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight: \"f32[512, 512, 3, 3]\", p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_norm1_weight: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_norm1_bias: \"f32[512]\", p_vae_decoder_up_blocks_2_resnets_0_conv1_weight: \"f32[256, 512, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_0_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_0_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: \"f32[256, 512, 1, 1]\", p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_conv1_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_1_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_1_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_1_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_conv1_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_2_conv1_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm2_weight: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_norm2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_resnets_2_conv2_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_resnets_2_conv2_bias: \"f32[256]\", p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight: \"f32[256, 256, 3, 3]\", p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_norm1_weight: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_norm1_bias: \"f32[256]\", p_vae_decoder_up_blocks_3_resnets_0_conv1_weight: \"f32[128, 256, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_0_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_0_conv2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: \"f32[128, 256, 1, 1]\", p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm1_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_conv1_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_1_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_1_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_1_conv2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm1_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_conv1_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_2_conv1_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm2_weight: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_norm2_bias: \"f32[128]\", p_vae_decoder_up_blocks_3_resnets_2_conv2_weight: \"f32[128, 128, 3, 3]\", p_vae_decoder_up_blocks_3_resnets_2_conv2_bias: \"f32[128]\", p_vae_decoder_conv_norm_out_weight: \"f32[128]\", p_vae_decoder_conv_norm_out_bias: \"f32[128]\", p_vae_decoder_conv_out_weight: \"f32[3, 128, 3, 3]\", p_vae_decoder_conv_out_bias: \"f32[3]\", c_timesteps: \"i64[1]\", c_lifted_tensor_0: \"f32[]\", b_pe_pe: \"f32[1, 5000, 384]\", whisper_input_features_0: \"f32[1, 80, 3000]\", latent_inputs: \"f32[8, 8, 32, 32]\", frame_idx, librosa_length):\n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:68 in get_whisper_chunk, code: input_feature = input_feature.to(device).to(weight_dtype)\n",
       "                    _to_copy: \"f32[1, 80, 3000]\" = torch.ops.aten._to_copy.default(whisper_input_features_0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  whisper_input_features_0 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1175 in forward, code: inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
       "                    conv1d: \"f32[1, 384, 3000]\" = torch.ops.aten.conv1d.default(_to_copy, p_whisper_encoder_conv1_weight, p_whisper_encoder_conv1_bias, [1], [1]);  _to_copy = p_whisper_encoder_conv1_weight = p_whisper_encoder_conv1_bias = None\n",
       "                    gelu: \"f32[1, 384, 3000]\" = torch.ops.aten.gelu.default(conv1d);  conv1d = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1176 in forward, code: inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
       "                    conv1d_1: \"f32[1, 384, 1500]\" = torch.ops.aten.conv1d.default(gelu, p_whisper_encoder_conv2_weight, p_whisper_encoder_conv2_bias, [2], [1]);  gelu = p_whisper_encoder_conv2_weight = p_whisper_encoder_conv2_bias = None\n",
       "                    gelu_1: \"f32[1, 384, 1500]\" = torch.ops.aten.gelu.default(conv1d_1);  conv1d_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1178 in forward, code: inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
       "                    permute: \"f32[1, 1500, 384]\" = torch.ops.aten.permute.default(gelu_1, [0, 2, 1]);  gelu_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1181 in forward, code: hidden_states = inputs_embeds + embed_pos\n",
       "                    add: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(permute, p_whisper_encoder_embed_positions_weight);  permute = p_whisper_encoder_embed_positions_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1182 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(add);  add = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(clone, [384], p_whisper_encoder_layers_0_self_attn_layer_norm_weight, p_whisper_encoder_layers_0_self_attn_layer_norm_bias);  p_whisper_encoder_layers_0_self_attn_layer_norm_weight = p_whisper_encoder_layers_0_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_q_proj_weight, p_whisper_encoder_layers_0_self_attn_q_proj_bias);  p_whisper_encoder_layers_0_self_attn_q_proj_weight = p_whisper_encoder_layers_0_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_1: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_k_proj_weight);  p_whisper_encoder_layers_0_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_1, [1, -1, 6, 64]);  linear_1 = None\n",
       "                    transpose: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "                    clone_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose, memory_format = torch.contiguous_format);  transpose = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_2: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm, p_whisper_encoder_layers_0_self_attn_v_proj_weight, p_whisper_encoder_layers_0_self_attn_v_proj_bias);  layer_norm = p_whisper_encoder_layers_0_self_attn_v_proj_weight = p_whisper_encoder_layers_0_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_1: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_2, [1, -1, 6, 64]);  linear_2 = None\n",
       "                    transpose_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
       "                    clone_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_2: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear, [1, 1500, 6, 64]);  linear = None\n",
       "                    transpose_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
       "                    clone_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_3, clone_1, clone_2);  clone_3 = clone_1 = clone_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_3: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_3: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_3, [1, 1500, 384]);  transpose_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_3: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_3, p_whisper_encoder_layers_0_self_attn_out_proj_weight, p_whisper_encoder_layers_0_self_attn_out_proj_bias);  view_3 = p_whisper_encoder_layers_0_self_attn_out_proj_weight = p_whisper_encoder_layers_0_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_4: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(clone, clone_4);  clone_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_1: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_1, [384], p_whisper_encoder_layers_0_final_layer_norm_weight, p_whisper_encoder_layers_0_final_layer_norm_bias);  p_whisper_encoder_layers_0_final_layer_norm_weight = p_whisper_encoder_layers_0_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_1, p_whisper_encoder_layers_0_fc1_weight, p_whisper_encoder_layers_0_fc1_bias);  layer_norm_1 = p_whisper_encoder_layers_0_fc1_weight = p_whisper_encoder_layers_0_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_2: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_2);  gelu_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_5: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_5, p_whisper_encoder_layers_0_fc2_weight, p_whisper_encoder_layers_0_fc2_bias);  clone_5 = p_whisper_encoder_layers_0_fc2_weight = p_whisper_encoder_layers_0_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_6: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_2: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_1, clone_6);  add_1 = clone_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_2: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_2, [384], p_whisper_encoder_layers_1_self_attn_layer_norm_weight, p_whisper_encoder_layers_1_self_attn_layer_norm_bias);  p_whisper_encoder_layers_1_self_attn_layer_norm_weight = p_whisper_encoder_layers_1_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_6: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_q_proj_weight, p_whisper_encoder_layers_1_self_attn_q_proj_bias);  p_whisper_encoder_layers_1_self_attn_q_proj_weight = p_whisper_encoder_layers_1_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_7: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_k_proj_weight);  p_whisper_encoder_layers_1_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_4: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_7, [1, -1, 6, 64]);  linear_7 = None\n",
       "                    transpose_4: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
       "                    clone_7: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_8: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_whisper_encoder_layers_1_self_attn_v_proj_weight, p_whisper_encoder_layers_1_self_attn_v_proj_bias);  layer_norm_2 = p_whisper_encoder_layers_1_self_attn_v_proj_weight = p_whisper_encoder_layers_1_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_5: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_8, [1, -1, 6, 64]);  linear_8 = None\n",
       "                    transpose_5: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
       "                    clone_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_6: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_6, [1, 1500, 6, 64]);  linear_6 = None\n",
       "                    transpose_6: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
       "                    clone_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_1: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_9, clone_7, clone_8);  clone_9 = clone_7 = clone_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_7: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_7: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_7, [1, 1500, 384]);  transpose_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_9: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_7, p_whisper_encoder_layers_1_self_attn_out_proj_weight, p_whisper_encoder_layers_1_self_attn_out_proj_bias);  view_7 = p_whisper_encoder_layers_1_self_attn_out_proj_weight = p_whisper_encoder_layers_1_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_10: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_3: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_2, clone_10);  clone_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_3: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_3, [384], p_whisper_encoder_layers_1_final_layer_norm_weight, p_whisper_encoder_layers_1_final_layer_norm_bias);  p_whisper_encoder_layers_1_final_layer_norm_weight = p_whisper_encoder_layers_1_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_10: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_3, p_whisper_encoder_layers_1_fc1_weight, p_whisper_encoder_layers_1_fc1_bias);  layer_norm_3 = p_whisper_encoder_layers_1_fc1_weight = p_whisper_encoder_layers_1_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_3: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_11: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_3);  gelu_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_11: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_11, p_whisper_encoder_layers_1_fc2_weight, p_whisper_encoder_layers_1_fc2_bias);  clone_11 = p_whisper_encoder_layers_1_fc2_weight = p_whisper_encoder_layers_1_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_12: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_4: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_3, clone_12);  add_3 = clone_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_4: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_4, [384], p_whisper_encoder_layers_2_self_attn_layer_norm_weight, p_whisper_encoder_layers_2_self_attn_layer_norm_bias);  p_whisper_encoder_layers_2_self_attn_layer_norm_weight = p_whisper_encoder_layers_2_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_12: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_q_proj_weight, p_whisper_encoder_layers_2_self_attn_q_proj_bias);  p_whisper_encoder_layers_2_self_attn_q_proj_weight = p_whisper_encoder_layers_2_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_13: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_k_proj_weight);  p_whisper_encoder_layers_2_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_8: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_13, [1, -1, 6, 64]);  linear_13 = None\n",
       "                    transpose_8: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
       "                    clone_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_8, memory_format = torch.contiguous_format);  transpose_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_14: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_4, p_whisper_encoder_layers_2_self_attn_v_proj_weight, p_whisper_encoder_layers_2_self_attn_v_proj_bias);  layer_norm_4 = p_whisper_encoder_layers_2_self_attn_v_proj_weight = p_whisper_encoder_layers_2_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_9: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_14, [1, -1, 6, 64]);  linear_14 = None\n",
       "                    transpose_9: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
       "                    clone_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_10: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_12, [1, 1500, 6, 64]);  linear_12 = None\n",
       "                    transpose_10: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
       "                    clone_15: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_2: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_15, clone_13, clone_14);  clone_15 = clone_13 = clone_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_11: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_11: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_11, [1, 1500, 384]);  transpose_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_15: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_11, p_whisper_encoder_layers_2_self_attn_out_proj_weight, p_whisper_encoder_layers_2_self_attn_out_proj_bias);  view_11 = p_whisper_encoder_layers_2_self_attn_out_proj_weight = p_whisper_encoder_layers_2_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_16: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_5: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_4, clone_16);  clone_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_5: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_5, [384], p_whisper_encoder_layers_2_final_layer_norm_weight, p_whisper_encoder_layers_2_final_layer_norm_bias);  p_whisper_encoder_layers_2_final_layer_norm_weight = p_whisper_encoder_layers_2_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_16: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_5, p_whisper_encoder_layers_2_fc1_weight, p_whisper_encoder_layers_2_fc1_bias);  layer_norm_5 = p_whisper_encoder_layers_2_fc1_weight = p_whisper_encoder_layers_2_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_4: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_17: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_4);  gelu_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_17: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_17, p_whisper_encoder_layers_2_fc2_weight, p_whisper_encoder_layers_2_fc2_bias);  clone_17 = p_whisper_encoder_layers_2_fc2_weight = p_whisper_encoder_layers_2_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_18: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_6: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_5, clone_18);  add_5 = clone_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:770 in forward, code: hidden_states = self.self_attn_layer_norm(hidden_states)\n",
       "                    layer_norm_6: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], p_whisper_encoder_layers_3_self_attn_layer_norm_weight, p_whisper_encoder_layers_3_self_attn_layer_norm_bias);  p_whisper_encoder_layers_3_self_attn_layer_norm_weight = p_whisper_encoder_layers_3_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:655 in forward, code: query_states = self.q_proj(hidden_states)\n",
       "                    linear_18: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_q_proj_weight, p_whisper_encoder_layers_3_self_attn_q_proj_bias);  p_whisper_encoder_layers_3_self_attn_q_proj_weight = p_whisper_encoder_layers_3_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:680 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    linear_19: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_k_proj_weight);  p_whisper_encoder_layers_3_self_attn_k_proj_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_12: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_19, [1, -1, 6, 64]);  linear_19 = None\n",
       "                    transpose_12: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_12, 1, 2);  view_12 = None\n",
       "                    clone_19: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:681 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    linear_20: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(layer_norm_6, p_whisper_encoder_layers_3_self_attn_v_proj_weight, p_whisper_encoder_layers_3_self_attn_v_proj_bias);  layer_norm_6 = p_whisper_encoder_layers_3_self_attn_v_proj_weight = p_whisper_encoder_layers_3_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_13: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_20, [1, -1, 6, 64]);  linear_20 = None\n",
       "                    transpose_13: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
       "                    clone_20: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_13, memory_format = torch.contiguous_format);  transpose_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:277 in _shape, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
       "                    view_14: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.view.default(linear_18, [1, 1500, 6, 64]);  linear_18 = None\n",
       "                    transpose_14: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.transpose.int(view_14, 1, 2);  view_14 = None\n",
       "                    clone_21: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:697 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_3: \"f32[1, 6, 1500, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_21, clone_19, clone_20);  clone_21 = clone_19 = clone_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:713 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_15: \"f32[1, 1500, 6, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:717 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    view_15: \"f32[1, 1500, 384]\" = torch.ops.aten.view.default(transpose_15, [1, 1500, 384]);  transpose_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:719 in forward, code: attn_output = self.out_proj(attn_output)\n",
       "                    linear_21: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(view_15, p_whisper_encoder_layers_3_self_attn_out_proj_weight, p_whisper_encoder_layers_3_self_attn_out_proj_bias);  view_15 = p_whisper_encoder_layers_3_self_attn_out_proj_weight = p_whisper_encoder_layers_3_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:777 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_22: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:778 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_7: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_6, clone_22);  clone_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:781 in forward, code: hidden_states = self.final_layer_norm(hidden_states)\n",
       "                    layer_norm_7: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_7, [384], p_whisper_encoder_layers_3_final_layer_norm_weight, p_whisper_encoder_layers_3_final_layer_norm_bias);  p_whisper_encoder_layers_3_final_layer_norm_weight = p_whisper_encoder_layers_3_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:782 in forward, code: hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
       "                    linear_22: \"f32[1, 1500, 1536]\" = torch.ops.aten.linear.default(layer_norm_7, p_whisper_encoder_layers_3_fc1_weight, p_whisper_encoder_layers_3_fc1_bias);  layer_norm_7 = p_whisper_encoder_layers_3_fc1_weight = p_whisper_encoder_layers_3_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
       "                    gelu_5: \"f32[1, 1500, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:783 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_23: \"f32[1, 1500, 1536]\" = torch.ops.aten.clone.default(gelu_5);  gelu_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:784 in forward, code: hidden_states = self.fc2(hidden_states)\n",
       "                    linear_23: \"f32[1, 1500, 384]\" = torch.ops.aten.linear.default(clone_23, p_whisper_encoder_layers_3_fc2_weight, p_whisper_encoder_layers_3_fc2_bias);  clone_23 = p_whisper_encoder_layers_3_fc2_weight = p_whisper_encoder_layers_3_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:785 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_24: \"f32[1, 1500, 384]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:786 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_8: \"f32[1, 1500, 384]\" = torch.ops.aten.add.Tensor(add_7, clone_24);  add_7 = clone_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1227 in forward, code: hidden_states = self.layer_norm(hidden_states)\n",
       "                    layer_norm_8: \"f32[1, 1500, 384]\" = torch.ops.aten.layer_norm.default(add_8, [384], p_whisper_encoder_layer_norm_weight, p_whisper_encoder_layer_norm_bias);  add_8 = p_whisper_encoder_layer_norm_weight = p_whisper_encoder_layer_norm_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:70 in get_whisper_chunk, code: audio_feats = torch.stack(audio_feats, dim=2)\n",
       "                    stack: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.stack.default([clone, add_2, add_4, add_6, layer_norm_8], 2);  clone = add_2 = add_4 = add_6 = layer_norm_8 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:73 in get_whisper_chunk, code: whisper_feature = torch.cat(whisper_feature, dim=1)\n",
       "                    cat: \"f32[1, 1500, 5, 384]\" = torch.ops.aten.cat.default([stack], 1);  stack = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:81 in get_whisper_chunk, code: whisper_feature = whisper_feature[:,:actual_length,...]\n",
       "                    slice_1: \"f32[1, 332, 5, 384]\" = torch.ops.aten.slice.Tensor(cat, 1, 0, 332);  cat = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:87 in get_whisper_chunk, code: torch.zeros_like(whisper_feature[:, :padding_nums * self.audio_padding_length_left]),\n",
       "                    slice_2: \"f32[1, 4, 5, 384]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 4)\n",
       "                    zeros_like: \"f32[1, 4, 5, 384]\" = torch.ops.aten.zeros_like.default(slice_2, pin_memory = False);  slice_2 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:90 in get_whisper_chunk, code: torch.zeros_like(whisper_feature[:, :padding_nums * 3 * self.audio_padding_length_right])\n",
       "                    slice_3: \"f32[1, 12, 5, 384]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 12)\n",
       "                    zeros_like_1: \"f32[1, 12, 5, 384]\" = torch.ops.aten.zeros_like.default(slice_3, pin_memory = False);  slice_3 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:86 in get_whisper_chunk, code: whisper_feature = torch.cat([\n",
       "                    cat_1: \"f32[1, 348, 5, 384]\" = torch.ops.aten.cat.default([zeros_like, slice_1, zeros_like_1], 1);  zeros_like = slice_1 = zeros_like_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:96 in get_whisper_chunk, code: audio_clip = whisper_feature[:, audio_index: audio_index + audio_feature_length_per_frame]\n",
       "                    slice_4: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 0, 10)\n",
       "                    slice_5: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 2, 12)\n",
       "                    slice_6: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 4, 14)\n",
       "                    slice_7: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 6, 16)\n",
       "                    slice_8: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 8, 18)\n",
       "                    slice_9: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 10, 20)\n",
       "                    slice_10: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 12, 22)\n",
       "                    slice_11: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 14, 24)\n",
       "                    slice_12: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 16, 26)\n",
       "                    slice_13: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 18, 28)\n",
       "                    slice_14: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 20, 30)\n",
       "                    slice_15: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 22, 32)\n",
       "                    slice_16: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 24, 34)\n",
       "                    slice_17: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 26, 36)\n",
       "                    slice_18: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 28, 38)\n",
       "                    slice_19: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 30, 40)\n",
       "                    slice_20: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 32, 42)\n",
       "                    slice_21: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 34, 44)\n",
       "                    slice_22: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 36, 46)\n",
       "                    slice_23: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 38, 48)\n",
       "                    slice_24: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 40, 50)\n",
       "                    slice_25: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 42, 52)\n",
       "                    slice_26: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 44, 54)\n",
       "                    slice_27: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 46, 56)\n",
       "                    slice_28: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 48, 58)\n",
       "                    slice_29: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 50, 60)\n",
       "                    slice_30: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 52, 62)\n",
       "                    slice_31: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 54, 64)\n",
       "                    slice_32: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 56, 66)\n",
       "                    slice_33: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 58, 68)\n",
       "                    slice_34: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 60, 70)\n",
       "                    slice_35: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 62, 72)\n",
       "                    slice_36: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 64, 74)\n",
       "                    slice_37: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 66, 76)\n",
       "                    slice_38: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 68, 78)\n",
       "                    slice_39: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 70, 80)\n",
       "                    slice_40: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 72, 82)\n",
       "                    slice_41: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 74, 84)\n",
       "                    slice_42: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 76, 86)\n",
       "                    slice_43: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 78, 88)\n",
       "                    slice_44: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 80, 90)\n",
       "                    slice_45: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 82, 92)\n",
       "                    slice_46: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 84, 94)\n",
       "                    slice_47: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 86, 96)\n",
       "                    slice_48: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 88, 98)\n",
       "                    slice_49: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 90, 100)\n",
       "                    slice_50: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 92, 102)\n",
       "                    slice_51: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 94, 104)\n",
       "                    slice_52: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 96, 106)\n",
       "                    slice_53: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 98, 108)\n",
       "                    slice_54: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 100, 110)\n",
       "                    slice_55: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 102, 112)\n",
       "                    slice_56: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 104, 114)\n",
       "                    slice_57: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 106, 116)\n",
       "                    slice_58: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 108, 118)\n",
       "                    slice_59: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 110, 120)\n",
       "                    slice_60: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 112, 122)\n",
       "                    slice_61: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 114, 124)\n",
       "                    slice_62: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 116, 126)\n",
       "                    slice_63: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 118, 128)\n",
       "                    slice_64: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 120, 130)\n",
       "                    slice_65: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 122, 132)\n",
       "                    slice_66: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 124, 134)\n",
       "                    slice_67: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 126, 136)\n",
       "                    slice_68: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 128, 138)\n",
       "                    slice_69: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 130, 140)\n",
       "                    slice_70: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 132, 142)\n",
       "                    slice_71: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 134, 144)\n",
       "                    slice_72: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 136, 146)\n",
       "                    slice_73: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 138, 148)\n",
       "                    slice_74: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 140, 150)\n",
       "                    slice_75: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 142, 152)\n",
       "                    slice_76: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 144, 154)\n",
       "                    slice_77: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 146, 156)\n",
       "                    slice_78: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 148, 158)\n",
       "                    slice_79: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 150, 160)\n",
       "                    slice_80: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 152, 162)\n",
       "                    slice_81: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 154, 164)\n",
       "                    slice_82: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 156, 166)\n",
       "                    slice_83: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 158, 168)\n",
       "                    slice_84: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 160, 170)\n",
       "                    slice_85: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 162, 172)\n",
       "                    slice_86: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 164, 174)\n",
       "                    slice_87: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 166, 176)\n",
       "                    slice_88: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 168, 178)\n",
       "                    slice_89: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 170, 180)\n",
       "                    slice_90: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 172, 182)\n",
       "                    slice_91: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 174, 184)\n",
       "                    slice_92: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 176, 186)\n",
       "                    slice_93: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 178, 188)\n",
       "                    slice_94: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 180, 190)\n",
       "                    slice_95: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 182, 192)\n",
       "                    slice_96: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 184, 194)\n",
       "                    slice_97: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 186, 196)\n",
       "                    slice_98: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 188, 198)\n",
       "                    slice_99: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 190, 200)\n",
       "                    slice_100: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 192, 202)\n",
       "                    slice_101: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 194, 204)\n",
       "                    slice_102: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 196, 206)\n",
       "                    slice_103: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 198, 208)\n",
       "                    slice_104: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 200, 210)\n",
       "                    slice_105: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 202, 212)\n",
       "                    slice_106: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 204, 214)\n",
       "                    slice_107: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 206, 216)\n",
       "                    slice_108: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 208, 218)\n",
       "                    slice_109: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 210, 220)\n",
       "                    slice_110: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 212, 222)\n",
       "                    slice_111: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 214, 224)\n",
       "                    slice_112: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 216, 226)\n",
       "                    slice_113: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 218, 228)\n",
       "                    slice_114: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 220, 230)\n",
       "                    slice_115: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 222, 232)\n",
       "                    slice_116: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 224, 234)\n",
       "                    slice_117: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 226, 236)\n",
       "                    slice_118: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 228, 238)\n",
       "                    slice_119: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 230, 240)\n",
       "                    slice_120: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 232, 242)\n",
       "                    slice_121: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 234, 244)\n",
       "                    slice_122: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 236, 246)\n",
       "                    slice_123: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 238, 248)\n",
       "                    slice_124: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 240, 250)\n",
       "                    slice_125: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 242, 252)\n",
       "                    slice_126: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 244, 254)\n",
       "                    slice_127: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 246, 256)\n",
       "                    slice_128: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 248, 258)\n",
       "                    slice_129: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 250, 260)\n",
       "                    slice_130: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 252, 262)\n",
       "                    slice_131: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 254, 264)\n",
       "                    slice_132: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 256, 266)\n",
       "                    slice_133: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 258, 268)\n",
       "                    slice_134: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 260, 270)\n",
       "                    slice_135: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 262, 272)\n",
       "                    slice_136: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 264, 274)\n",
       "                    slice_137: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 266, 276)\n",
       "                    slice_138: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 268, 278)\n",
       "                    slice_139: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 270, 280)\n",
       "                    slice_140: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 272, 282)\n",
       "                    slice_141: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 274, 284)\n",
       "                    slice_142: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 276, 286)\n",
       "                    slice_143: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 278, 288)\n",
       "                    slice_144: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 280, 290)\n",
       "                    slice_145: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 282, 292)\n",
       "                    slice_146: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 284, 294)\n",
       "                    slice_147: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 286, 296)\n",
       "                    slice_148: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 288, 298)\n",
       "                    slice_149: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 290, 300)\n",
       "                    slice_150: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 292, 302)\n",
       "                    slice_151: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 294, 304)\n",
       "                    slice_152: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 296, 306)\n",
       "                    slice_153: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 298, 308)\n",
       "                    slice_154: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 300, 310)\n",
       "                    slice_155: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 302, 312)\n",
       "                    slice_156: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 304, 314)\n",
       "                    slice_157: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 306, 316)\n",
       "                    slice_158: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 308, 318)\n",
       "                    slice_159: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 310, 320)\n",
       "                    slice_160: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 312, 322)\n",
       "                    slice_161: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 314, 324)\n",
       "                    slice_162: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 316, 326)\n",
       "                    slice_163: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 318, 328)\n",
       "                    slice_164: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 320, 330)\n",
       "                    slice_165: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 322, 332)\n",
       "                    slice_166: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 324, 334)\n",
       "                    slice_167: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 326, 336)\n",
       "                    slice_168: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 328, 338)\n",
       "                    slice_169: \"f32[1, 10, 5, 384]\" = torch.ops.aten.slice.Tensor(cat_1, 1, 330, 340);  cat_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:99 in get_whisper_chunk, code: audio_prompts = torch.cat(audio_prompts, dim=0)  # T, 10, 5, 384\n",
       "                    cat_2: \"f32[166, 10, 5, 384]\" = torch.ops.aten.cat.default([slice_4, slice_5, slice_6, slice_7, slice_8, slice_9, slice_10, slice_11, slice_12, slice_13, slice_14, slice_15, slice_16, slice_17, slice_18, slice_19, slice_20, slice_21, slice_22, slice_23, slice_24, slice_25, slice_26, slice_27, slice_28, slice_29, slice_30, slice_31, slice_32, slice_33, slice_34, slice_35, slice_36, slice_37, slice_38, slice_39, slice_40, slice_41, slice_42, slice_43, slice_44, slice_45, slice_46, slice_47, slice_48, slice_49, slice_50, slice_51, slice_52, slice_53, slice_54, slice_55, slice_56, slice_57, slice_58, slice_59, slice_60, slice_61, slice_62, slice_63, slice_64, slice_65, slice_66, slice_67, slice_68, slice_69, slice_70, slice_71, slice_72, slice_73, slice_74, slice_75, slice_76, slice_77, slice_78, slice_79, slice_80, slice_81, slice_82, slice_83, slice_84, slice_85, slice_86, slice_87, slice_88, slice_89, slice_90, slice_91, slice_92, slice_93, slice_94, slice_95, slice_96, slice_97, slice_98, slice_99, slice_100, slice_101, slice_102, slice_103, slice_104, slice_105, slice_106, slice_107, slice_108, slice_109, slice_110, slice_111, slice_112, slice_113, slice_114, slice_115, slice_116, slice_117, slice_118, slice_119, slice_120, slice_121, slice_122, slice_123, slice_124, slice_125, slice_126, slice_127, slice_128, slice_129, slice_130, slice_131, slice_132, slice_133, slice_134, slice_135, slice_136, slice_137, slice_138, slice_139, slice_140, slice_141, slice_142, slice_143, slice_144, slice_145, slice_146, slice_147, slice_148, slice_149, slice_150, slice_151, slice_152, slice_153, slice_154, slice_155, slice_156, slice_157, slice_158, slice_159, slice_160, slice_161, slice_162, slice_163, slice_164, slice_165, slice_166, slice_167, slice_168, slice_169]);  slice_4 = slice_5 = slice_6 = slice_7 = slice_8 = slice_9 = slice_10 = slice_11 = slice_12 = slice_13 = slice_14 = slice_15 = slice_16 = slice_17 = slice_18 = slice_19 = slice_20 = slice_21 = slice_22 = slice_23 = slice_24 = slice_25 = slice_26 = slice_27 = slice_28 = slice_29 = slice_30 = slice_31 = slice_32 = slice_33 = slice_34 = slice_35 = slice_36 = slice_37 = slice_38 = slice_39 = slice_40 = slice_41 = slice_42 = slice_43 = slice_44 = slice_45 = slice_46 = slice_47 = slice_48 = slice_49 = slice_50 = slice_51 = slice_52 = slice_53 = slice_54 = slice_55 = slice_56 = slice_57 = slice_58 = slice_59 = slice_60 = slice_61 = slice_62 = slice_63 = slice_64 = slice_65 = slice_66 = slice_67 = slice_68 = slice_69 = slice_70 = slice_71 = slice_72 = slice_73 = slice_74 = slice_75 = slice_76 = slice_77 = slice_78 = slice_79 = slice_80 = slice_81 = slice_82 = slice_83 = slice_84 = slice_85 = slice_86 = slice_87 = slice_88 = slice_89 = slice_90 = slice_91 = slice_92 = slice_93 = slice_94 = slice_95 = slice_96 = slice_97 = slice_98 = slice_99 = slice_100 = slice_101 = slice_102 = slice_103 = slice_104 = slice_105 = slice_106 = slice_107 = slice_108 = slice_109 = slice_110 = slice_111 = slice_112 = slice_113 = slice_114 = slice_115 = slice_116 = slice_117 = slice_118 = slice_119 = slice_120 = slice_121 = slice_122 = slice_123 = slice_124 = slice_125 = slice_126 = slice_127 = slice_128 = slice_129 = slice_130 = slice_131 = slice_132 = slice_133 = slice_134 = slice_135 = slice_136 = slice_137 = slice_138 = slice_139 = slice_140 = slice_141 = slice_142 = slice_143 = slice_144 = slice_145 = slice_146 = slice_147 = slice_148 = slice_149 = slice_150 = slice_151 = slice_152 = slice_153 = slice_154 = slice_155 = slice_156 = slice_157 = slice_158 = slice_159 = slice_160 = slice_161 = slice_162 = slice_163 = slice_164 = slice_165 = slice_166 = slice_167 = slice_168 = slice_169 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:100 in get_whisper_chunk, code: audio_prompts = rearrange(audio_prompts, 'b c h w -> b (c h) w')\n",
       "                    view_16: \"f32[166, 50, 384]\" = torch.ops.aten.view.default(cat_2, [166, 50, 384]);  cat_2 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:119 in forward, code: context_whisper_chunk = whisper_chunks[frame_idx: frame_idx+batch_size]  # shape [batch_size, 50, 384]\n",
       "                    slice_170: \"f32[8, 50, 384]\" = torch.ops.aten.slice.Tensor(view_16, 0, 0, 8);  view_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/musetalk/models/unet.py:25 in forward, code: pe = self.pe[:, :seq_len, :]\n",
       "                    slice_171: \"f32[1, 50, 384]\" = torch.ops.aten.slice.Tensor(b_pe_pe, 1, 0, 50);  b_pe_pe = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/musetalk/models/unet.py:26 in forward, code: x = x + pe.to(x.device)\n",
       "                    _to_copy_1: \"f32[1, 50, 384]\" = torch.ops.aten._to_copy.default(slice_171, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  slice_171 = None\n",
       "                    add_9: \"f32[8, 50, 384]\" = torch.ops.aten.add.Tensor(slice_170, _to_copy_1);  slice_170 = _to_copy_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:122 in forward, code: latent_batch = latent_inputs.to(device=self.unet_device, dtype=self.unet.dtype)\n",
       "                    _to_copy_2: \"f32[8, 8, 32, 32]\" = torch.ops.aten._to_copy.default(latent_inputs, dtype = torch.float32, device = device(type='cuda', index=0));  latent_inputs = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:926 in get_time_embed, code: timesteps = timesteps.expand(sample.shape[0])\n",
       "                    expand: \"i64[8]\" = torch.ops.aten.expand.default(c_timesteps, [8]);  c_timesteps = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:57 in get_timestep_embedding, code: exponent = -math.log(max_period) * torch.arange(\n",
       "                    arange: \"f32[160]\" = torch.ops.aten.arange.start(0, 160, dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)\n",
       "                    mul: \"f32[160]\" = torch.ops.aten.mul.Tensor(arange, -9.210340371976184);  arange = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:60 in get_timestep_embedding, code: exponent = exponent / (half_dim - downscale_freq_shift)\n",
       "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(160, dtype = torch.float32)\n",
       "                    div: \"f32[160]\" = torch.ops.aten.div.Tensor(mul, scalar_tensor_default);  mul = scalar_tensor_default = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:62 in get_timestep_embedding, code: emb = torch.exp(exponent)\n",
       "                    exp: \"f32[160]\" = torch.ops.aten.exp.default(div);  div = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:63 in get_timestep_embedding, code: emb = timesteps[:, None].float() * emb[None, :]\n",
       "                    unsqueeze: \"i64[8, 1]\" = torch.ops.aten.unsqueeze.default(expand, 1);  expand = None\n",
       "                    _to_copy_3: \"f32[8, 1]\" = torch.ops.aten._to_copy.default(unsqueeze, dtype = torch.float32);  unsqueeze = None\n",
       "                    unsqueeze_1: \"f32[1, 160]\" = torch.ops.aten.unsqueeze.default(exp, 0);  exp = None\n",
       "                    mul_1: \"f32[8, 160]\" = torch.ops.aten.mul.Tensor(_to_copy_3, unsqueeze_1);  _to_copy_3 = unsqueeze_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:66 in get_timestep_embedding, code: emb = scale * emb\n",
       "                    scalar_tensor_default_1: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    mul_2: \"f32[8, 160]\" = torch.ops.aten.mul.Tensor(mul_1, scalar_tensor_default_1);  mul_1 = scalar_tensor_default_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:69 in get_timestep_embedding, code: emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
       "                    sin: \"f32[8, 160]\" = torch.ops.aten.sin.default(mul_2)\n",
       "                    cos: \"f32[8, 160]\" = torch.ops.aten.cos.default(mul_2);  mul_2 = None\n",
       "                    cat_3: \"f32[8, 320]\" = torch.ops.aten.cat.default([sin, cos], -1);  sin = cos = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:73 in get_timestep_embedding, code: emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n",
       "                    slice_172: \"f32[8, 160]\" = torch.ops.aten.slice.Tensor(cat_3, 1, 160, 9223372036854775807)\n",
       "                    slice_173: \"f32[8, 160]\" = torch.ops.aten.slice.Tensor(cat_3, 1, 0, 160);  cat_3 = None\n",
       "                    cat_4: \"f32[8, 320]\" = torch.ops.aten.cat.default([slice_172, slice_173], -1);  slice_172 = slice_173 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:663 in forward, code: sample = self.linear_1(sample)\n",
       "                    linear_24: \"f32[8, 1280]\" = torch.ops.aten.linear.default(cat_4, p_unet_time_embedding_linear_1_weight, p_unet_time_embedding_linear_1_bias);  cat_4 = p_unet_time_embedding_linear_1_weight = p_unet_time_embedding_linear_1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:666 in forward, code: sample = self.act(sample)\n",
       "                    silu: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_24);  linear_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/embeddings.py:668 in forward, code: sample = self.linear_2(sample)\n",
       "                    linear_25: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu, p_unet_time_embedding_linear_2_weight, p_unet_time_embedding_linear_2_bias);  silu = p_unet_time_embedding_linear_2_weight = p_unet_time_embedding_linear_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1169 in forward, code: sample = self.conv_in(sample)\n",
       "                    conv2d: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(_to_copy_2, p_unet_conv_in_weight, p_unet_conv_in_bias, [1, 1], [1, 1]);  _to_copy_2 = p_unet_conv_in_weight = p_unet_conv_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d, 32, p_unet_down_blocks_0_resnets_0_norm1_weight, p_unet_down_blocks_0_resnets_0_norm1_bias);  p_unet_down_blocks_0_resnets_0_norm1_weight = p_unet_down_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm);  group_norm = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_1, p_unet_down_blocks_0_resnets_0_conv1_weight, p_unet_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_1 = p_unet_down_blocks_0_resnets_0_conv1_weight = p_unet_down_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_2: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_26: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_2, p_unet_down_blocks_0_resnets_0_time_emb_proj_weight, p_unet_down_blocks_0_resnets_0_time_emb_proj_bias);  silu_2 = p_unet_down_blocks_0_resnets_0_time_emb_proj_weight = p_unet_down_blocks_0_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_2: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_26, 2);  linear_26 = None\n",
       "                    unsqueeze_3: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_2, 3);  unsqueeze_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_10: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_1, unsqueeze_3);  conv2d_1 = unsqueeze_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_10, 32, p_unet_down_blocks_0_resnets_0_norm2_weight, p_unet_down_blocks_0_resnets_0_norm2_bias);  add_10 = p_unet_down_blocks_0_resnets_0_norm2_weight = p_unet_down_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_1);  group_norm_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_25: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_3);  silu_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_25, p_unet_down_blocks_0_resnets_0_conv2_weight, p_unet_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_25 = p_unet_down_blocks_0_resnets_0_conv2_weight = p_unet_down_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_11: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d, conv2d_2);  conv2d_2 = None\n",
       "                    div_1: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_11, 1.0);  add_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_1, 32, p_unet_down_blocks_0_attentions_0_norm_weight, p_unet_down_blocks_0_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_0_attentions_0_norm_weight = p_unet_down_blocks_0_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_2, p_unet_down_blocks_0_attentions_0_proj_in_weight, p_unet_down_blocks_0_attentions_0_proj_in_bias);  group_norm_2 = p_unet_down_blocks_0_attentions_0_proj_in_weight = p_unet_down_blocks_0_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_1: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_3, [0, 2, 3, 1]);  conv2d_3 = None\n",
       "                    view_17: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_1, [8, 1024, 320]);  permute_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_9: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_17, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_27: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_28: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_29: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_9 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_18: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_27, [8, -1, 8, 40]);  linear_27 = None\n",
       "                    transpose_16: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_19: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_28, [8, -1, 8, 40]);  linear_28 = None\n",
       "                    transpose_17: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_19, 1, 2);  view_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_20: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_29, [8, -1, 8, 40]);  linear_29 = None\n",
       "                    transpose_18: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_4: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_16, transpose_17, transpose_18);  transpose_16 = transpose_17 = transpose_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_19: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
       "                    view_21: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_19, [8, -1, 320]);  transpose_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_30: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_21, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_21 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_26: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_30);  linear_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_2: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_26, 1.0);  clone_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_12: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_2, view_17);  div_2 = view_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_10: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_12, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_31: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_10, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_10 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_32: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_33: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_22: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_31, [8, -1, 8, 40]);  linear_31 = None\n",
       "                    transpose_20: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_23: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_32, [8, -1, 8, 40]);  linear_32 = None\n",
       "                    transpose_21: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_23, 1, 2);  view_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_24: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_33, [8, -1, 8, 40]);  linear_33 = None\n",
       "                    transpose_22: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_24, 1, 2);  view_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_5: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_20, transpose_21, transpose_22);  transpose_20 = transpose_21 = transpose_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_23: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
       "                    view_25: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_23, [8, -1, 320]);  transpose_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_34: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_25, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_25 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_27: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_3: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_27, 1.0);  clone_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_13: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_3, add_12);  div_3 = add_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_11: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_13, [320], p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_35: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_11, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_11 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split = torch.ops.aten.split.Tensor(linear_35, 1280, -1);  linear_35 = None\n",
       "                    getitem: \"f32[8, 1024, 1280]\" = split[0]\n",
       "                    getitem_1: \"f32[8, 1024, 1280]\" = split[1];  split = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_6: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_1);  getitem_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_3: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem, gelu_6);  getitem = gelu_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_28: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_3);  mul_3 = None\n",
       "                    linear_36: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_28, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_28 = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_14: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_36, add_13);  linear_36 = add_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_26: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_14, [8, 32, 32, 320]);  add_14 = None\n",
       "                    permute_2: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_26, [0, 3, 1, 2]);  view_26 = None\n",
       "                    clone_29: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_29, p_unet_down_blocks_0_attentions_0_proj_out_weight, p_unet_down_blocks_0_attentions_0_proj_out_bias);  clone_29 = p_unet_down_blocks_0_attentions_0_proj_out_weight = p_unet_down_blocks_0_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_15: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_4, div_1);  conv2d_4 = div_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_3: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_15, 32, p_unet_down_blocks_0_resnets_1_norm1_weight, p_unet_down_blocks_0_resnets_1_norm1_bias);  p_unet_down_blocks_0_resnets_1_norm1_weight = p_unet_down_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_3);  group_norm_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_5: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_4, p_unet_down_blocks_0_resnets_1_conv1_weight, p_unet_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_4 = p_unet_down_blocks_0_resnets_1_conv1_weight = p_unet_down_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_5: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_37: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_5, p_unet_down_blocks_0_resnets_1_time_emb_proj_weight, p_unet_down_blocks_0_resnets_1_time_emb_proj_bias);  silu_5 = p_unet_down_blocks_0_resnets_1_time_emb_proj_weight = p_unet_down_blocks_0_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_4: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_37, 2);  linear_37 = None\n",
       "                    unsqueeze_5: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_16: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_5, unsqueeze_5);  conv2d_5 = unsqueeze_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_16, 32, p_unet_down_blocks_0_resnets_1_norm2_weight, p_unet_down_blocks_0_resnets_1_norm2_bias);  add_16 = p_unet_down_blocks_0_resnets_1_norm2_weight = p_unet_down_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_6: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_4);  group_norm_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_30: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_6);  silu_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_6: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_30, p_unet_down_blocks_0_resnets_1_conv2_weight, p_unet_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_30 = p_unet_down_blocks_0_resnets_1_conv2_weight = p_unet_down_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_17: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(add_15, conv2d_6);  conv2d_6 = None\n",
       "                    div_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_17, 1.0);  add_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_5: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_4, 32, p_unet_down_blocks_0_attentions_1_norm_weight, p_unet_down_blocks_0_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_0_attentions_1_norm_weight = p_unet_down_blocks_0_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_7: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_5, p_unet_down_blocks_0_attentions_1_proj_in_weight, p_unet_down_blocks_0_attentions_1_proj_in_bias);  group_norm_5 = p_unet_down_blocks_0_attentions_1_proj_in_weight = p_unet_down_blocks_0_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_3: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_7, [0, 2, 3, 1]);  conv2d_7 = None\n",
       "                    view_27: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_3, [8, 1024, 320]);  permute_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_12: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_27, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_38: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_39: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_40: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_12, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_12 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_28: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_38, [8, -1, 8, 40]);  linear_38 = None\n",
       "                    transpose_24: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_28, 1, 2);  view_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_29: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_39, [8, -1, 8, 40]);  linear_39 = None\n",
       "                    transpose_25: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_30: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_40, [8, -1, 8, 40]);  linear_40 = None\n",
       "                    transpose_26: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_6: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_24, transpose_25, transpose_26);  transpose_24 = transpose_25 = transpose_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_27: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
       "                    view_31: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_27, [8, -1, 320]);  transpose_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_41: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_31, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_31 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_31: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_5: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_31, 1.0);  clone_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_18: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_5, view_27);  div_5 = view_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_13: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_18, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_42: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_13, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_13 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_43: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_44: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_32: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_42, [8, -1, 8, 40]);  linear_42 = None\n",
       "                    transpose_28: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_33: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_43, [8, -1, 8, 40]);  linear_43 = None\n",
       "                    transpose_29: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_33, 1, 2);  view_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_34: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_44, [8, -1, 8, 40]);  linear_44 = None\n",
       "                    transpose_30: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_34, 1, 2);  view_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_7: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_28, transpose_29, transpose_30);  transpose_28 = transpose_29 = transpose_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_31: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
       "                    view_35: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_31, [8, -1, 320]);  transpose_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_45: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_35, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_35 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_32: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_6: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_32, 1.0);  clone_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_19: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_6, add_18);  div_6 = add_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_14: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_19, [320], p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_46: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_14, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_14 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_1 = torch.ops.aten.split.Tensor(linear_46, 1280, -1);  linear_46 = None\n",
       "                    getitem_2: \"f32[8, 1024, 1280]\" = split_1[0]\n",
       "                    getitem_3: \"f32[8, 1024, 1280]\" = split_1[1];  split_1 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_7: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_4: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_2, gelu_7);  getitem_2 = gelu_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_33: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_4);  mul_4 = None\n",
       "                    linear_47: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_33, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_33 = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_20: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_47, add_19);  linear_47 = add_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_36: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_20, [8, 32, 32, 320]);  add_20 = None\n",
       "                    permute_4: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_36, [0, 3, 1, 2]);  view_36 = None\n",
       "                    clone_34: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_8: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_34, p_unet_down_blocks_0_attentions_1_proj_out_weight, p_unet_down_blocks_0_attentions_1_proj_out_bias);  clone_34 = p_unet_down_blocks_0_attentions_1_proj_out_weight = p_unet_down_blocks_0_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_21: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_8, div_4);  conv2d_8 = div_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_9: \"f32[8, 320, 16, 16]\" = torch.ops.aten.conv2d.default(add_21, p_unet_down_blocks_0_downsamplers_0_conv_weight, p_unet_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_0_downsamplers_0_conv_weight = p_unet_down_blocks_0_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_6: \"f32[8, 320, 16, 16]\" = torch.ops.aten.group_norm.default(conv2d_9, 32, p_unet_down_blocks_1_resnets_0_norm1_weight, p_unet_down_blocks_1_resnets_0_norm1_bias);  p_unet_down_blocks_1_resnets_0_norm1_weight = p_unet_down_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_7: \"f32[8, 320, 16, 16]\" = torch.ops.aten.silu.default(group_norm_6);  group_norm_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_7, p_unet_down_blocks_1_resnets_0_conv1_weight, p_unet_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_7 = p_unet_down_blocks_1_resnets_0_conv1_weight = p_unet_down_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_8: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_48: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_8, p_unet_down_blocks_1_resnets_0_time_emb_proj_weight, p_unet_down_blocks_1_resnets_0_time_emb_proj_bias);  silu_8 = p_unet_down_blocks_1_resnets_0_time_emb_proj_weight = p_unet_down_blocks_1_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_6: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_48, 2);  linear_48 = None\n",
       "                    unsqueeze_7: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 3);  unsqueeze_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_22: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_10, unsqueeze_7);  conv2d_10 = unsqueeze_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_7: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_22, 32, p_unet_down_blocks_1_resnets_0_norm2_weight, p_unet_down_blocks_1_resnets_0_norm2_bias);  add_22 = p_unet_down_blocks_1_resnets_0_norm2_weight = p_unet_down_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_9: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_7);  group_norm_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_35: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_9);  silu_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_11: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_35, p_unet_down_blocks_1_resnets_0_conv2_weight, p_unet_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_35 = p_unet_down_blocks_1_resnets_0_conv2_weight = p_unet_down_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_12: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(conv2d_9, p_unet_down_blocks_1_resnets_0_conv_shortcut_weight, p_unet_down_blocks_1_resnets_0_conv_shortcut_bias);  p_unet_down_blocks_1_resnets_0_conv_shortcut_weight = p_unet_down_blocks_1_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_23: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_12, conv2d_11);  conv2d_12 = conv2d_11 = None\n",
       "                    div_7: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_23, 1.0);  add_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_8: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_7, 32, p_unet_down_blocks_1_attentions_0_norm_weight, p_unet_down_blocks_1_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_1_attentions_0_norm_weight = p_unet_down_blocks_1_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_13: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_8, p_unet_down_blocks_1_attentions_0_proj_in_weight, p_unet_down_blocks_1_attentions_0_proj_in_bias);  group_norm_8 = p_unet_down_blocks_1_attentions_0_proj_in_weight = p_unet_down_blocks_1_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_5: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_13, [0, 2, 3, 1]);  conv2d_13 = None\n",
       "                    view_37: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_5, [8, 256, 640]);  permute_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_15: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_37, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_49: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_50: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_51: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_15, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_15 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_38: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_49, [8, -1, 8, 80]);  linear_49 = None\n",
       "                    transpose_32: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_38, 1, 2);  view_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_39: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_50, [8, -1, 8, 80]);  linear_50 = None\n",
       "                    transpose_33: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_39, 1, 2);  view_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_40: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_51, [8, -1, 8, 80]);  linear_51 = None\n",
       "                    transpose_34: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_40, 1, 2);  view_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_8: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_32, transpose_33, transpose_34);  transpose_32 = transpose_33 = transpose_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_35: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
       "                    view_41: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_35, [8, -1, 640]);  transpose_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_52: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_41, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_41 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_36: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_8: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_36, 1.0);  clone_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_24: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_8, view_37);  div_8 = view_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_16: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_24, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_53: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_16, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_16 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_54: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_55: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_42: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_53, [8, -1, 8, 80]);  linear_53 = None\n",
       "                    transpose_36: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_42, 1, 2);  view_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_43: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_54, [8, -1, 8, 80]);  linear_54 = None\n",
       "                    transpose_37: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_43, 1, 2);  view_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_44: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_55, [8, -1, 8, 80]);  linear_55 = None\n",
       "                    transpose_38: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_44, 1, 2);  view_44 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_9: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_36, transpose_37, transpose_38);  transpose_36 = transpose_37 = transpose_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_39: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
       "                    view_45: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_39, [8, -1, 640]);  transpose_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_56: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_45, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_45 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_37: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_56);  linear_56 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_9: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_37, 1.0);  clone_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_25: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_9, add_24);  div_9 = add_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_17: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_25, [640], p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_57: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_17, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_17 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_2 = torch.ops.aten.split.Tensor(linear_57, 2560, -1);  linear_57 = None\n",
       "                    getitem_4: \"f32[8, 256, 2560]\" = split_2[0]\n",
       "                    getitem_5: \"f32[8, 256, 2560]\" = split_2[1];  split_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_8: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_5);  getitem_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_5: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_4, gelu_8);  getitem_4 = gelu_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_38: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_5);  mul_5 = None\n",
       "                    linear_58: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_38, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_38 = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_26: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_58, add_25);  linear_58 = add_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_46: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_26, [8, 16, 16, 640]);  add_26 = None\n",
       "                    permute_6: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_46, [0, 3, 1, 2]);  view_46 = None\n",
       "                    clone_39: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_14: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_39, p_unet_down_blocks_1_attentions_0_proj_out_weight, p_unet_down_blocks_1_attentions_0_proj_out_bias);  clone_39 = p_unet_down_blocks_1_attentions_0_proj_out_weight = p_unet_down_blocks_1_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_27: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_14, div_7);  conv2d_14 = div_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_9: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_27, 32, p_unet_down_blocks_1_resnets_1_norm1_weight, p_unet_down_blocks_1_resnets_1_norm1_bias);  p_unet_down_blocks_1_resnets_1_norm1_weight = p_unet_down_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_9);  group_norm_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_15: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_10, p_unet_down_blocks_1_resnets_1_conv1_weight, p_unet_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_10 = p_unet_down_blocks_1_resnets_1_conv1_weight = p_unet_down_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_11: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_59: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_11, p_unet_down_blocks_1_resnets_1_time_emb_proj_weight, p_unet_down_blocks_1_resnets_1_time_emb_proj_bias);  silu_11 = p_unet_down_blocks_1_resnets_1_time_emb_proj_weight = p_unet_down_blocks_1_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_8: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_59, 2);  linear_59 = None\n",
       "                    unsqueeze_9: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_8, 3);  unsqueeze_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_28: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_15, unsqueeze_9);  conv2d_15 = unsqueeze_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_28, 32, p_unet_down_blocks_1_resnets_1_norm2_weight, p_unet_down_blocks_1_resnets_1_norm2_bias);  add_28 = p_unet_down_blocks_1_resnets_1_norm2_weight = p_unet_down_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_12: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_10);  group_norm_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_40: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_12);  silu_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_16: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_40, p_unet_down_blocks_1_resnets_1_conv2_weight, p_unet_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_40 = p_unet_down_blocks_1_resnets_1_conv2_weight = p_unet_down_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_29: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(add_27, conv2d_16);  conv2d_16 = None\n",
       "                    div_10: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_29, 1.0);  add_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_11: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_10, 32, p_unet_down_blocks_1_attentions_1_norm_weight, p_unet_down_blocks_1_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_1_attentions_1_norm_weight = p_unet_down_blocks_1_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_17: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_11, p_unet_down_blocks_1_attentions_1_proj_in_weight, p_unet_down_blocks_1_attentions_1_proj_in_bias);  group_norm_11 = p_unet_down_blocks_1_attentions_1_proj_in_weight = p_unet_down_blocks_1_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_7: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_17, [0, 2, 3, 1]);  conv2d_17 = None\n",
       "                    view_47: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_7, [8, 256, 640]);  permute_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_18: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_47, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_60: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_61: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_62: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_18, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_18 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_48: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_60, [8, -1, 8, 80]);  linear_60 = None\n",
       "                    transpose_40: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_48, 1, 2);  view_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_49: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_61, [8, -1, 8, 80]);  linear_61 = None\n",
       "                    transpose_41: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_49, 1, 2);  view_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_50: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_62, [8, -1, 8, 80]);  linear_62 = None\n",
       "                    transpose_42: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_50, 1, 2);  view_50 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_10: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_40, transpose_41, transpose_42);  transpose_40 = transpose_41 = transpose_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_43: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
       "                    view_51: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_43, [8, -1, 640]);  transpose_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_63: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_51, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_51 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_41: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_11: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_41, 1.0);  clone_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_30: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_11, view_47);  div_11 = view_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_19: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_30, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_64: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_19, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_19 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_65: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_66: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_52: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_64, [8, -1, 8, 80]);  linear_64 = None\n",
       "                    transpose_44: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_52, 1, 2);  view_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_53: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_65, [8, -1, 8, 80]);  linear_65 = None\n",
       "                    transpose_45: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_53, 1, 2);  view_53 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_54: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_66, [8, -1, 8, 80]);  linear_66 = None\n",
       "                    transpose_46: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_54, 1, 2);  view_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_11: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_44, transpose_45, transpose_46);  transpose_44 = transpose_45 = transpose_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_47: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
       "                    view_55: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_47, [8, -1, 640]);  transpose_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_67: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_55, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_55 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_42: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_67);  linear_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_12: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_42, 1.0);  clone_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_31: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_12, add_30);  div_12 = add_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_20: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_31, [640], p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_68: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_20, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_20 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_3 = torch.ops.aten.split.Tensor(linear_68, 2560, -1);  linear_68 = None\n",
       "                    getitem_6: \"f32[8, 256, 2560]\" = split_3[0]\n",
       "                    getitem_7: \"f32[8, 256, 2560]\" = split_3[1];  split_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_9: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_7);  getitem_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_6: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_6, gelu_9);  getitem_6 = gelu_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_43: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_6);  mul_6 = None\n",
       "                    linear_69: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_43, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_43 = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_32: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_69, add_31);  linear_69 = add_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_56: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_32, [8, 16, 16, 640]);  add_32 = None\n",
       "                    permute_8: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_56, [0, 3, 1, 2]);  view_56 = None\n",
       "                    clone_44: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_18: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_44, p_unet_down_blocks_1_attentions_1_proj_out_weight, p_unet_down_blocks_1_attentions_1_proj_out_bias);  clone_44 = p_unet_down_blocks_1_attentions_1_proj_out_weight = p_unet_down_blocks_1_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_33: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_18, div_10);  conv2d_18 = div_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_19: \"f32[8, 640, 8, 8]\" = torch.ops.aten.conv2d.default(add_33, p_unet_down_blocks_1_downsamplers_0_conv_weight, p_unet_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_1_downsamplers_0_conv_weight = p_unet_down_blocks_1_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_12: \"f32[8, 640, 8, 8]\" = torch.ops.aten.group_norm.default(conv2d_19, 32, p_unet_down_blocks_2_resnets_0_norm1_weight, p_unet_down_blocks_2_resnets_0_norm1_bias);  p_unet_down_blocks_2_resnets_0_norm1_weight = p_unet_down_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_13: \"f32[8, 640, 8, 8]\" = torch.ops.aten.silu.default(group_norm_12);  group_norm_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_20: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_13, p_unet_down_blocks_2_resnets_0_conv1_weight, p_unet_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_13 = p_unet_down_blocks_2_resnets_0_conv1_weight = p_unet_down_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_14: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_70: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_14, p_unet_down_blocks_2_resnets_0_time_emb_proj_weight, p_unet_down_blocks_2_resnets_0_time_emb_proj_bias);  silu_14 = p_unet_down_blocks_2_resnets_0_time_emb_proj_weight = p_unet_down_blocks_2_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_10: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_70, 2);  linear_70 = None\n",
       "                    unsqueeze_11: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_10, 3);  unsqueeze_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_20, unsqueeze_11);  conv2d_20 = unsqueeze_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_13: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_34, 32, p_unet_down_blocks_2_resnets_0_norm2_weight, p_unet_down_blocks_2_resnets_0_norm2_bias);  add_34 = p_unet_down_blocks_2_resnets_0_norm2_weight = p_unet_down_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_15: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_13);  group_norm_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_15);  silu_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_21: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_45, p_unet_down_blocks_2_resnets_0_conv2_weight, p_unet_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_45 = p_unet_down_blocks_2_resnets_0_conv2_weight = p_unet_down_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_22: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(conv2d_19, p_unet_down_blocks_2_resnets_0_conv_shortcut_weight, p_unet_down_blocks_2_resnets_0_conv_shortcut_bias);  p_unet_down_blocks_2_resnets_0_conv_shortcut_weight = p_unet_down_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_35: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_22, conv2d_21);  conv2d_22 = conv2d_21 = None\n",
       "                    div_13: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_35, 1.0);  add_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_14: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_13, 32, p_unet_down_blocks_2_attentions_0_norm_weight, p_unet_down_blocks_2_attentions_0_norm_bias, 1e-06);  p_unet_down_blocks_2_attentions_0_norm_weight = p_unet_down_blocks_2_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_23: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_14, p_unet_down_blocks_2_attentions_0_proj_in_weight, p_unet_down_blocks_2_attentions_0_proj_in_bias);  group_norm_14 = p_unet_down_blocks_2_attentions_0_proj_in_weight = p_unet_down_blocks_2_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_9: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_23, [0, 2, 3, 1]);  conv2d_23 = None\n",
       "                    view_57: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_9, [8, 64, 1280]);  permute_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_21: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_57, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_71: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_72: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_73: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_21, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_21 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_58: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_71, [8, -1, 8, 160]);  linear_71 = None\n",
       "                    transpose_48: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_58, 1, 2);  view_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_59: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_72, [8, -1, 8, 160]);  linear_72 = None\n",
       "                    transpose_49: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_59, 1, 2);  view_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_60: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_73, [8, -1, 8, 160]);  linear_73 = None\n",
       "                    transpose_50: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_60, 1, 2);  view_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_12: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_48, transpose_49, transpose_50);  transpose_48 = transpose_49 = transpose_50 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_51: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_12, 1, 2);  scaled_dot_product_attention_12 = None\n",
       "                    view_61: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_51, [8, -1, 1280]);  transpose_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_74: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_61, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_61 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_46: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_74);  linear_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_14: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_46, 1.0);  clone_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_36: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_14, view_57);  div_14 = view_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_22: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_36, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_75: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_22, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_22 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_76: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_77: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_62: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_75, [8, -1, 8, 160]);  linear_75 = None\n",
       "                    transpose_52: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_62, 1, 2);  view_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_63: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_76, [8, -1, 8, 160]);  linear_76 = None\n",
       "                    transpose_53: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_63, 1, 2);  view_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_64: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_77, [8, -1, 8, 160]);  linear_77 = None\n",
       "                    transpose_54: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_64, 1, 2);  view_64 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_13: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_52, transpose_53, transpose_54);  transpose_52 = transpose_53 = transpose_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_55: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_13, 1, 2);  scaled_dot_product_attention_13 = None\n",
       "                    view_65: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_55, [8, -1, 1280]);  transpose_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_78: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_65, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_65 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_47: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_78);  linear_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_15: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_47, 1.0);  clone_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_37: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_15, add_36);  div_15 = add_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_23: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_37, [1280], p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_79: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_23, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_23 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_4 = torch.ops.aten.split.Tensor(linear_79, 5120, -1);  linear_79 = None\n",
       "                    getitem_8: \"f32[8, 64, 5120]\" = split_4[0]\n",
       "                    getitem_9: \"f32[8, 64, 5120]\" = split_4[1];  split_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_10: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_9);  getitem_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_7: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_8, gelu_10);  getitem_8 = gelu_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_48: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_7);  mul_7 = None\n",
       "                    linear_80: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_48, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_48 = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_38: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_80, add_37);  linear_80 = add_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_66: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_38, [8, 8, 8, 1280]);  add_38 = None\n",
       "                    permute_10: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_66, [0, 3, 1, 2]);  view_66 = None\n",
       "                    clone_49: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_24: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_49, p_unet_down_blocks_2_attentions_0_proj_out_weight, p_unet_down_blocks_2_attentions_0_proj_out_bias);  clone_49 = p_unet_down_blocks_2_attentions_0_proj_out_weight = p_unet_down_blocks_2_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_39: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_24, div_13);  conv2d_24 = div_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_15: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_39, 32, p_unet_down_blocks_2_resnets_1_norm1_weight, p_unet_down_blocks_2_resnets_1_norm1_bias);  p_unet_down_blocks_2_resnets_1_norm1_weight = p_unet_down_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_15);  group_norm_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_25: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_16, p_unet_down_blocks_2_resnets_1_conv1_weight, p_unet_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_16 = p_unet_down_blocks_2_resnets_1_conv1_weight = p_unet_down_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_17: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_81: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_17, p_unet_down_blocks_2_resnets_1_time_emb_proj_weight, p_unet_down_blocks_2_resnets_1_time_emb_proj_bias);  silu_17 = p_unet_down_blocks_2_resnets_1_time_emb_proj_weight = p_unet_down_blocks_2_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_12: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_81, 2);  linear_81 = None\n",
       "                    unsqueeze_13: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 3);  unsqueeze_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_40: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_25, unsqueeze_13);  conv2d_25 = unsqueeze_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_40, 32, p_unet_down_blocks_2_resnets_1_norm2_weight, p_unet_down_blocks_2_resnets_1_norm2_bias);  add_40 = p_unet_down_blocks_2_resnets_1_norm2_weight = p_unet_down_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_18: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_16);  group_norm_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_50: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_18);  silu_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_26: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_50, p_unet_down_blocks_2_resnets_1_conv2_weight, p_unet_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_50 = p_unet_down_blocks_2_resnets_1_conv2_weight = p_unet_down_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_41: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(add_39, conv2d_26);  conv2d_26 = None\n",
       "                    div_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_41, 1.0);  add_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_17: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_16, 32, p_unet_down_blocks_2_attentions_1_norm_weight, p_unet_down_blocks_2_attentions_1_norm_bias, 1e-06);  p_unet_down_blocks_2_attentions_1_norm_weight = p_unet_down_blocks_2_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_27: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_17, p_unet_down_blocks_2_attentions_1_proj_in_weight, p_unet_down_blocks_2_attentions_1_proj_in_bias);  group_norm_17 = p_unet_down_blocks_2_attentions_1_proj_in_weight = p_unet_down_blocks_2_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_11: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_27, [0, 2, 3, 1]);  conv2d_27 = None\n",
       "                    view_67: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_11, [8, 64, 1280]);  permute_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_24: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_67, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_82: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_83: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_84: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_24, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_24 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_68: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_82, [8, -1, 8, 160]);  linear_82 = None\n",
       "                    transpose_56: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_69: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_83, [8, -1, 8, 160]);  linear_83 = None\n",
       "                    transpose_57: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_69, 1, 2);  view_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_70: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_84, [8, -1, 8, 160]);  linear_84 = None\n",
       "                    transpose_58: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_14: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_56, transpose_57, transpose_58);  transpose_56 = transpose_57 = transpose_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_59: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_14, 1, 2);  scaled_dot_product_attention_14 = None\n",
       "                    view_71: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_59, [8, -1, 1280]);  transpose_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_85: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_71, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_71 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_51: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_85);  linear_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_17: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_51, 1.0);  clone_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_42: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_17, view_67);  div_17 = view_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_25: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_42, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_86: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_25, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_25 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_87: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_88: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_72: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_86, [8, -1, 8, 160]);  linear_86 = None\n",
       "                    transpose_60: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_73: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_87, [8, -1, 8, 160]);  linear_87 = None\n",
       "                    transpose_61: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_73, 1, 2);  view_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_74: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_88, [8, -1, 8, 160]);  linear_88 = None\n",
       "                    transpose_62: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_15: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_60, transpose_61, transpose_62);  transpose_60 = transpose_61 = transpose_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_63: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_15, 1, 2);  scaled_dot_product_attention_15 = None\n",
       "                    view_75: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_63, [8, -1, 1280]);  transpose_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_89: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_75, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_75 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_52: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_89);  linear_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_18: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_52, 1.0);  clone_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_43: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_18, add_42);  div_18 = add_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_26: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_43, [1280], p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_90: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_26, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_26 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_5 = torch.ops.aten.split.Tensor(linear_90, 5120, -1);  linear_90 = None\n",
       "                    getitem_10: \"f32[8, 64, 5120]\" = split_5[0]\n",
       "                    getitem_11: \"f32[8, 64, 5120]\" = split_5[1];  split_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_11: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_11);  getitem_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_8: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_10, gelu_11);  getitem_10 = gelu_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_53: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_8);  mul_8 = None\n",
       "                    linear_91: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_53, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_53 = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_44: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_91, add_43);  linear_91 = add_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_76: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_44, [8, 8, 8, 1280]);  add_44 = None\n",
       "                    permute_12: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_76, [0, 3, 1, 2]);  view_76 = None\n",
       "                    clone_54: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_28: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_54, p_unet_down_blocks_2_attentions_1_proj_out_weight, p_unet_down_blocks_2_attentions_1_proj_out_bias);  clone_54 = p_unet_down_blocks_2_attentions_1_proj_out_weight = p_unet_down_blocks_2_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_28, div_16);  conv2d_28 = div_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:147 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_29: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(add_45, p_unet_down_blocks_2_downsamplers_0_conv_weight, p_unet_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1]);  p_unet_down_blocks_2_downsamplers_0_conv_weight = p_unet_down_blocks_2_downsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_18: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(conv2d_29, 32, p_unet_down_blocks_3_resnets_0_norm1_weight, p_unet_down_blocks_3_resnets_0_norm1_bias);  p_unet_down_blocks_3_resnets_0_norm1_weight = p_unet_down_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_18);  group_norm_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_19, p_unet_down_blocks_3_resnets_0_conv1_weight, p_unet_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_19 = p_unet_down_blocks_3_resnets_0_conv1_weight = p_unet_down_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_20: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_92: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_20, p_unet_down_blocks_3_resnets_0_time_emb_proj_weight, p_unet_down_blocks_3_resnets_0_time_emb_proj_bias);  silu_20 = p_unet_down_blocks_3_resnets_0_time_emb_proj_weight = p_unet_down_blocks_3_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_14: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_92, 2);  linear_92 = None\n",
       "                    unsqueeze_15: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_14, 3);  unsqueeze_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_46: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_30, unsqueeze_15);  conv2d_30 = unsqueeze_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_46, 32, p_unet_down_blocks_3_resnets_0_norm2_weight, p_unet_down_blocks_3_resnets_0_norm2_bias);  add_46 = p_unet_down_blocks_3_resnets_0_norm2_weight = p_unet_down_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_19);  group_norm_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_55: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_21);  silu_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_31: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_55, p_unet_down_blocks_3_resnets_0_conv2_weight, p_unet_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_55 = p_unet_down_blocks_3_resnets_0_conv2_weight = p_unet_down_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_47: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_29, conv2d_31);  conv2d_31 = None\n",
       "                    div_19: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_47, 1.0);  add_47 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_20: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_19, 32, p_unet_down_blocks_3_resnets_1_norm1_weight, p_unet_down_blocks_3_resnets_1_norm1_bias);  p_unet_down_blocks_3_resnets_1_norm1_weight = p_unet_down_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_22: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_20);  group_norm_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_32: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_22, p_unet_down_blocks_3_resnets_1_conv1_weight, p_unet_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_22 = p_unet_down_blocks_3_resnets_1_conv1_weight = p_unet_down_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_23: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_93: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_23, p_unet_down_blocks_3_resnets_1_time_emb_proj_weight, p_unet_down_blocks_3_resnets_1_time_emb_proj_bias);  silu_23 = p_unet_down_blocks_3_resnets_1_time_emb_proj_weight = p_unet_down_blocks_3_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_16: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_93, 2);  linear_93 = None\n",
       "                    unsqueeze_17: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_16, 3);  unsqueeze_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_48: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_32, unsqueeze_17);  conv2d_32 = unsqueeze_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_48, 32, p_unet_down_blocks_3_resnets_1_norm2_weight, p_unet_down_blocks_3_resnets_1_norm2_bias);  add_48 = p_unet_down_blocks_3_resnets_1_norm2_weight = p_unet_down_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_21);  group_norm_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_56: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_24);  silu_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_33: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_56, p_unet_down_blocks_3_resnets_1_conv2_weight, p_unet_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_56 = p_unet_down_blocks_3_resnets_1_conv2_weight = p_unet_down_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_49: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(div_19, conv2d_33);  conv2d_33 = None\n",
       "                    div_20: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_49, 1.0);  add_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_22: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_20, 32, p_unet_mid_block_resnets_0_norm1_weight, p_unet_mid_block_resnets_0_norm1_bias);  p_unet_mid_block_resnets_0_norm1_weight = p_unet_mid_block_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_22);  group_norm_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_34: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_25, p_unet_mid_block_resnets_0_conv1_weight, p_unet_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_25 = p_unet_mid_block_resnets_0_conv1_weight = p_unet_mid_block_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_26: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_94: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_26, p_unet_mid_block_resnets_0_time_emb_proj_weight, p_unet_mid_block_resnets_0_time_emb_proj_bias);  silu_26 = p_unet_mid_block_resnets_0_time_emb_proj_weight = p_unet_mid_block_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_18: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_94, 2);  linear_94 = None\n",
       "                    unsqueeze_19: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_18, 3);  unsqueeze_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_50: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_34, unsqueeze_19);  conv2d_34 = unsqueeze_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_23: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_50, 32, p_unet_mid_block_resnets_0_norm2_weight, p_unet_mid_block_resnets_0_norm2_bias);  add_50 = p_unet_mid_block_resnets_0_norm2_weight = p_unet_mid_block_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_27: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_23);  group_norm_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_57: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_27);  silu_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_35: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_57, p_unet_mid_block_resnets_0_conv2_weight, p_unet_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_57 = p_unet_mid_block_resnets_0_conv2_weight = p_unet_mid_block_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_51: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(div_20, conv2d_35);  conv2d_35 = None\n",
       "                    scalar_tensor_default_2: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_21: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_51, scalar_tensor_default_2);  add_51 = scalar_tensor_default_2 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(div_21, 32, p_unet_mid_block_attentions_0_norm_weight, p_unet_mid_block_attentions_0_norm_bias, 1e-06);  p_unet_mid_block_attentions_0_norm_weight = p_unet_mid_block_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_36: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(group_norm_24, p_unet_mid_block_attentions_0_proj_in_weight, p_unet_mid_block_attentions_0_proj_in_bias);  group_norm_24 = p_unet_mid_block_attentions_0_proj_in_weight = p_unet_mid_block_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_13: \"f32[8, 4, 4, 1280]\" = torch.ops.aten.permute.default(conv2d_36, [0, 2, 3, 1]);  conv2d_36 = None\n",
       "                    view_77: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(permute_13, [8, 16, 1280]);  permute_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_27: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(view_77, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_95: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_96: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_97: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_27, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_27 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_78: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_95, [8, -1, 8, 160]);  linear_95 = None\n",
       "                    transpose_64: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_79: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_96, [8, -1, 8, 160]);  linear_96 = None\n",
       "                    transpose_65: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_80: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_97, [8, -1, 8, 160]);  linear_97 = None\n",
       "                    transpose_66: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_16: \"f32[8, 8, 16, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_64, transpose_65, transpose_66);  transpose_64 = transpose_65 = transpose_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_67: \"f32[8, 16, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_16, 1, 2);  scaled_dot_product_attention_16 = None\n",
       "                    view_81: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(transpose_67, [8, -1, 1280]);  transpose_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_98: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(view_81, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_81 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_58: \"f32[8, 16, 1280]\" = torch.ops.aten.clone.default(linear_98);  linear_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_22: \"f32[8, 16, 1280]\" = torch.ops.aten.div.Tensor(clone_58, 1.0);  clone_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_52: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(div_22, view_77);  div_22 = view_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_28: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(add_52, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_99: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(layer_norm_28, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_28 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_100: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_101: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_82: \"f32[8, 16, 8, 160]\" = torch.ops.aten.view.default(linear_99, [8, -1, 8, 160]);  linear_99 = None\n",
       "                    transpose_68: \"f32[8, 8, 16, 160]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_83: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_100, [8, -1, 8, 160]);  linear_100 = None\n",
       "                    transpose_69: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_84: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_101, [8, -1, 8, 160]);  linear_101 = None\n",
       "                    transpose_70: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_17: \"f32[8, 8, 16, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_68, transpose_69, transpose_70);  transpose_68 = transpose_69 = transpose_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_71: \"f32[8, 16, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_17, 1, 2);  scaled_dot_product_attention_17 = None\n",
       "                    view_85: \"f32[8, 16, 1280]\" = torch.ops.aten.view.default(transpose_71, [8, -1, 1280]);  transpose_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_102: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(view_85, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_85 = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_59: \"f32[8, 16, 1280]\" = torch.ops.aten.clone.default(linear_102);  linear_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_23: \"f32[8, 16, 1280]\" = torch.ops.aten.div.Tensor(clone_59, 1.0);  clone_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_53: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(div_23, add_52);  div_23 = add_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_29: \"f32[8, 16, 1280]\" = torch.ops.aten.layer_norm.default(add_53, [1280], p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_103: \"f32[8, 16, 10240]\" = torch.ops.aten.linear.default(layer_norm_29, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_29 = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_6 = torch.ops.aten.split.Tensor(linear_103, 5120, -1);  linear_103 = None\n",
       "                    getitem_12: \"f32[8, 16, 5120]\" = split_6[0]\n",
       "                    getitem_13: \"f32[8, 16, 5120]\" = split_6[1];  split_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_12: \"f32[8, 16, 5120]\" = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_9: \"f32[8, 16, 5120]\" = torch.ops.aten.mul.Tensor(getitem_12, gelu_12);  getitem_12 = gelu_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_60: \"f32[8, 16, 5120]\" = torch.ops.aten.clone.default(mul_9);  mul_9 = None\n",
       "                    linear_104: \"f32[8, 16, 1280]\" = torch.ops.aten.linear.default(clone_60, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_60 = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_54: \"f32[8, 16, 1280]\" = torch.ops.aten.add.Tensor(linear_104, add_53);  linear_104 = add_53 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_86: \"f32[8, 4, 4, 1280]\" = torch.ops.aten.view.default(add_54, [8, 4, 4, 1280]);  add_54 = None\n",
       "                    permute_14: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.permute.default(view_86, [0, 3, 1, 2]);  view_86 = None\n",
       "                    clone_61: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_37: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_61, p_unet_mid_block_attentions_0_proj_out_weight, p_unet_mid_block_attentions_0_proj_out_bias);  clone_61 = p_unet_mid_block_attentions_0_proj_out_weight = p_unet_mid_block_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_55: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_37, div_21);  conv2d_37 = div_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_55, 32, p_unet_mid_block_resnets_1_norm1_weight, p_unet_mid_block_resnets_1_norm1_bias);  p_unet_mid_block_resnets_1_norm1_weight = p_unet_mid_block_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_28: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_25);  group_norm_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_38: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_28, p_unet_mid_block_resnets_1_conv1_weight, p_unet_mid_block_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_28 = p_unet_mid_block_resnets_1_conv1_weight = p_unet_mid_block_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_29: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_105: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_29, p_unet_mid_block_resnets_1_time_emb_proj_weight, p_unet_mid_block_resnets_1_time_emb_proj_bias);  silu_29 = p_unet_mid_block_resnets_1_time_emb_proj_weight = p_unet_mid_block_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_20: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_105, 2);  linear_105 = None\n",
       "                    unsqueeze_21: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_20, 3);  unsqueeze_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_56: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_38, unsqueeze_21);  conv2d_38 = unsqueeze_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_26: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_56, 32, p_unet_mid_block_resnets_1_norm2_weight, p_unet_mid_block_resnets_1_norm2_bias);  add_56 = p_unet_mid_block_resnets_1_norm2_weight = p_unet_mid_block_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_26);  group_norm_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_62: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_30);  silu_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_39: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_62, p_unet_mid_block_resnets_1_conv2_weight, p_unet_mid_block_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_62 = p_unet_mid_block_resnets_1_conv2_weight = p_unet_mid_block_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_57: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(add_55, conv2d_39);  add_55 = conv2d_39 = None\n",
       "                    scalar_tensor_default_3: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_24: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_57, scalar_tensor_default_3);  add_57 = scalar_tensor_default_3 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_5: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_24, div_20], 1);  div_24 = div_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_27: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_5, 32, p_unet_up_blocks_0_resnets_0_norm1_weight, p_unet_up_blocks_0_resnets_0_norm1_bias);  p_unet_up_blocks_0_resnets_0_norm1_weight = p_unet_up_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_31: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_27);  group_norm_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_40: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_31, p_unet_up_blocks_0_resnets_0_conv1_weight, p_unet_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_31 = p_unet_up_blocks_0_resnets_0_conv1_weight = p_unet_up_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_32: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_106: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_32, p_unet_up_blocks_0_resnets_0_time_emb_proj_weight, p_unet_up_blocks_0_resnets_0_time_emb_proj_bias);  silu_32 = p_unet_up_blocks_0_resnets_0_time_emb_proj_weight = p_unet_up_blocks_0_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_22: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_106, 2);  linear_106 = None\n",
       "                    unsqueeze_23: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_22, 3);  unsqueeze_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_58: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_40, unsqueeze_23);  conv2d_40 = unsqueeze_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_28: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_58, 32, p_unet_up_blocks_0_resnets_0_norm2_weight, p_unet_up_blocks_0_resnets_0_norm2_bias);  add_58 = p_unet_up_blocks_0_resnets_0_norm2_weight = p_unet_up_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_33: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_28);  group_norm_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_63: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_33);  silu_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_41: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_63, p_unet_up_blocks_0_resnets_0_conv2_weight, p_unet_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_63 = p_unet_up_blocks_0_resnets_0_conv2_weight = p_unet_up_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_42: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_5, p_unet_up_blocks_0_resnets_0_conv_shortcut_weight, p_unet_up_blocks_0_resnets_0_conv_shortcut_bias);  cat_5 = p_unet_up_blocks_0_resnets_0_conv_shortcut_weight = p_unet_up_blocks_0_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_59: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_42, conv2d_41);  conv2d_42 = conv2d_41 = None\n",
       "                    div_25: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_59, 1.0);  add_59 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_6: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_25, div_19], 1);  div_25 = div_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_29: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_6, 32, p_unet_up_blocks_0_resnets_1_norm1_weight, p_unet_up_blocks_0_resnets_1_norm1_bias);  p_unet_up_blocks_0_resnets_1_norm1_weight = p_unet_up_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_34: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_29);  group_norm_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_43: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_34, p_unet_up_blocks_0_resnets_1_conv1_weight, p_unet_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_34 = p_unet_up_blocks_0_resnets_1_conv1_weight = p_unet_up_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_35: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_107: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_35, p_unet_up_blocks_0_resnets_1_time_emb_proj_weight, p_unet_up_blocks_0_resnets_1_time_emb_proj_bias);  silu_35 = p_unet_up_blocks_0_resnets_1_time_emb_proj_weight = p_unet_up_blocks_0_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_24: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_107, 2);  linear_107 = None\n",
       "                    unsqueeze_25: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_24, 3);  unsqueeze_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_60: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_43, unsqueeze_25);  conv2d_43 = unsqueeze_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_30: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_60, 32, p_unet_up_blocks_0_resnets_1_norm2_weight, p_unet_up_blocks_0_resnets_1_norm2_bias);  add_60 = p_unet_up_blocks_0_resnets_1_norm2_weight = p_unet_up_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_36: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_30);  group_norm_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_64: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_36);  silu_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_44: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_64, p_unet_up_blocks_0_resnets_1_conv2_weight, p_unet_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_64 = p_unet_up_blocks_0_resnets_1_conv2_weight = p_unet_up_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_45: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_6, p_unet_up_blocks_0_resnets_1_conv_shortcut_weight, p_unet_up_blocks_0_resnets_1_conv_shortcut_bias);  cat_6 = p_unet_up_blocks_0_resnets_1_conv_shortcut_weight = p_unet_up_blocks_0_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_61: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_45, conv2d_44);  conv2d_45 = conv2d_44 = None\n",
       "                    div_26: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_61, 1.0);  add_61 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2654 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_7: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.cat.default([div_26, conv2d_29], 1);  div_26 = conv2d_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_31: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.group_norm.default(cat_7, 32, p_unet_up_blocks_0_resnets_2_norm1_weight, p_unet_up_blocks_0_resnets_2_norm1_bias);  p_unet_up_blocks_0_resnets_2_norm1_weight = p_unet_up_blocks_0_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_37: \"f32[8, 2560, 4, 4]\" = torch.ops.aten.silu.default(group_norm_31);  group_norm_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_46: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(silu_37, p_unet_up_blocks_0_resnets_2_conv1_weight, p_unet_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_37 = p_unet_up_blocks_0_resnets_2_conv1_weight = p_unet_up_blocks_0_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_38: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_108: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_38, p_unet_up_blocks_0_resnets_2_time_emb_proj_weight, p_unet_up_blocks_0_resnets_2_time_emb_proj_bias);  silu_38 = p_unet_up_blocks_0_resnets_2_time_emb_proj_weight = p_unet_up_blocks_0_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_26: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_108, 2);  linear_108 = None\n",
       "                    unsqueeze_27: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_26, 3);  unsqueeze_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_62: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_46, unsqueeze_27);  conv2d_46 = unsqueeze_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_32: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.group_norm.default(add_62, 32, p_unet_up_blocks_0_resnets_2_norm2_weight, p_unet_up_blocks_0_resnets_2_norm2_bias);  add_62 = p_unet_up_blocks_0_resnets_2_norm2_weight = p_unet_up_blocks_0_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_39: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.silu.default(group_norm_32);  group_norm_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_65: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.clone.default(silu_39);  silu_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_47: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(clone_65, p_unet_up_blocks_0_resnets_2_conv2_weight, p_unet_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_65 = p_unet_up_blocks_0_resnets_2_conv2_weight = p_unet_up_blocks_0_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_48: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.conv2d.default(cat_7, p_unet_up_blocks_0_resnets_2_conv_shortcut_weight, p_unet_up_blocks_0_resnets_2_conv_shortcut_bias);  cat_7 = p_unet_up_blocks_0_resnets_2_conv_shortcut_weight = p_unet_up_blocks_0_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_63: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.add.Tensor(conv2d_48, conv2d_47);  conv2d_48 = conv2d_47 = None\n",
       "                    div_27: \"f32[8, 1280, 4, 4]\" = torch.ops.aten.div.Tensor(add_63, 1.0);  add_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.upsample_nearest2d.vec(div_27, None, [2.0, 2.0]);  div_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_49: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(upsample_nearest2d, p_unet_up_blocks_0_upsamplers_0_conv_weight, p_unet_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d = p_unet_up_blocks_0_upsamplers_0_conv_weight = p_unet_up_blocks_0_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_8: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.cat.default([conv2d_49, add_45], 1);  conv2d_49 = add_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_33: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.group_norm.default(cat_8, 32, p_unet_up_blocks_1_resnets_0_norm1_weight, p_unet_up_blocks_1_resnets_0_norm1_bias);  p_unet_up_blocks_1_resnets_0_norm1_weight = p_unet_up_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_40: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.silu.default(group_norm_33);  group_norm_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_50: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_40, p_unet_up_blocks_1_resnets_0_conv1_weight, p_unet_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_40 = p_unet_up_blocks_1_resnets_0_conv1_weight = p_unet_up_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_41: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_109: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_41, p_unet_up_blocks_1_resnets_0_time_emb_proj_weight, p_unet_up_blocks_1_resnets_0_time_emb_proj_bias);  silu_41 = p_unet_up_blocks_1_resnets_0_time_emb_proj_weight = p_unet_up_blocks_1_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_28: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_109, 2);  linear_109 = None\n",
       "                    unsqueeze_29: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_28, 3);  unsqueeze_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_64: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_50, unsqueeze_29);  conv2d_50 = unsqueeze_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_64, 32, p_unet_up_blocks_1_resnets_0_norm2_weight, p_unet_up_blocks_1_resnets_0_norm2_bias);  add_64 = p_unet_up_blocks_1_resnets_0_norm2_weight = p_unet_up_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_42: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_34);  group_norm_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_66: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_42);  silu_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_51: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_66, p_unet_up_blocks_1_resnets_0_conv2_weight, p_unet_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_66 = p_unet_up_blocks_1_resnets_0_conv2_weight = p_unet_up_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_52: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_8, p_unet_up_blocks_1_resnets_0_conv_shortcut_weight, p_unet_up_blocks_1_resnets_0_conv_shortcut_bias);  cat_8 = p_unet_up_blocks_1_resnets_0_conv_shortcut_weight = p_unet_up_blocks_1_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_65: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_52, conv2d_51);  conv2d_52 = conv2d_51 = None\n",
       "                    div_28: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_65, 1.0);  add_65 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_35: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_28, 32, p_unet_up_blocks_1_attentions_0_norm_weight, p_unet_up_blocks_1_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_0_norm_weight = p_unet_up_blocks_1_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_53: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_35, p_unet_up_blocks_1_attentions_0_proj_in_weight, p_unet_up_blocks_1_attentions_0_proj_in_bias);  group_norm_35 = p_unet_up_blocks_1_attentions_0_proj_in_weight = p_unet_up_blocks_1_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_15: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_53, [0, 2, 3, 1]);  conv2d_53 = None\n",
       "                    view_87: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_15, [8, 64, 1280]);  permute_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_30: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_87, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_110: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_111: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_112: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_30, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_30 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_88: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_110, [8, -1, 8, 160]);  linear_110 = None\n",
       "                    transpose_72: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_89: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_111, [8, -1, 8, 160]);  linear_111 = None\n",
       "                    transpose_73: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_89, 1, 2);  view_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_90: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_112, [8, -1, 8, 160]);  linear_112 = None\n",
       "                    transpose_74: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_18: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_72, transpose_73, transpose_74);  transpose_72 = transpose_73 = transpose_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_75: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_18, 1, 2);  scaled_dot_product_attention_18 = None\n",
       "                    view_91: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_75, [8, -1, 1280]);  transpose_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_113: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_91, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_91 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_67: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_113);  linear_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_29: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_67, 1.0);  clone_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_66: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_29, view_87);  div_29 = view_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_31: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_66, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_114: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_31, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_31 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_115: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_116: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_92: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_114, [8, -1, 8, 160]);  linear_114 = None\n",
       "                    transpose_76: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_93: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_115, [8, -1, 8, 160]);  linear_115 = None\n",
       "                    transpose_77: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_93, 1, 2);  view_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_94: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_116, [8, -1, 8, 160]);  linear_116 = None\n",
       "                    transpose_78: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_19: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_76, transpose_77, transpose_78);  transpose_76 = transpose_77 = transpose_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_79: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_19, 1, 2);  scaled_dot_product_attention_19 = None\n",
       "                    view_95: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_79, [8, -1, 1280]);  transpose_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_117: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_95, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_95 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_68: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_117);  linear_117 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_30: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_68, 1.0);  clone_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_67: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_30, add_66);  div_30 = add_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_32: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_67, [1280], p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_118: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_32, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_32 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_7 = torch.ops.aten.split.Tensor(linear_118, 5120, -1);  linear_118 = None\n",
       "                    getitem_14: \"f32[8, 64, 5120]\" = split_7[0]\n",
       "                    getitem_15: \"f32[8, 64, 5120]\" = split_7[1];  split_7 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_13: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_10: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_14, gelu_13);  getitem_14 = gelu_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_69: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_10);  mul_10 = None\n",
       "                    linear_119: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_69, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_69 = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_68: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_119, add_67);  linear_119 = add_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_96: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_68, [8, 8, 8, 1280]);  add_68 = None\n",
       "                    permute_16: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_96, [0, 3, 1, 2]);  view_96 = None\n",
       "                    clone_70: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_54: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_70, p_unet_up_blocks_1_attentions_0_proj_out_weight, p_unet_up_blocks_1_attentions_0_proj_out_bias);  clone_70 = p_unet_up_blocks_1_attentions_0_proj_out_weight = p_unet_up_blocks_1_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_69: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_54, div_28);  conv2d_54 = div_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_9: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.cat.default([add_69, add_39], 1);  add_69 = add_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_36: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.group_norm.default(cat_9, 32, p_unet_up_blocks_1_resnets_1_norm1_weight, p_unet_up_blocks_1_resnets_1_norm1_bias);  p_unet_up_blocks_1_resnets_1_norm1_weight = p_unet_up_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_43: \"f32[8, 2560, 8, 8]\" = torch.ops.aten.silu.default(group_norm_36);  group_norm_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_55: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_43, p_unet_up_blocks_1_resnets_1_conv1_weight, p_unet_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_43 = p_unet_up_blocks_1_resnets_1_conv1_weight = p_unet_up_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_44: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_120: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_44, p_unet_up_blocks_1_resnets_1_time_emb_proj_weight, p_unet_up_blocks_1_resnets_1_time_emb_proj_bias);  silu_44 = p_unet_up_blocks_1_resnets_1_time_emb_proj_weight = p_unet_up_blocks_1_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_30: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_120, 2);  linear_120 = None\n",
       "                    unsqueeze_31: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_30, 3);  unsqueeze_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_70: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_55, unsqueeze_31);  conv2d_55 = unsqueeze_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_37: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_70, 32, p_unet_up_blocks_1_resnets_1_norm2_weight, p_unet_up_blocks_1_resnets_1_norm2_bias);  add_70 = p_unet_up_blocks_1_resnets_1_norm2_weight = p_unet_up_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_45: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_37);  group_norm_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_71: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_45);  silu_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_56: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_71, p_unet_up_blocks_1_resnets_1_conv2_weight, p_unet_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_71 = p_unet_up_blocks_1_resnets_1_conv2_weight = p_unet_up_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_57: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_9, p_unet_up_blocks_1_resnets_1_conv_shortcut_weight, p_unet_up_blocks_1_resnets_1_conv_shortcut_bias);  cat_9 = p_unet_up_blocks_1_resnets_1_conv_shortcut_weight = p_unet_up_blocks_1_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_71: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_57, conv2d_56);  conv2d_57 = conv2d_56 = None\n",
       "                    div_31: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_71, 1.0);  add_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_38: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_31, 32, p_unet_up_blocks_1_attentions_1_norm_weight, p_unet_up_blocks_1_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_1_norm_weight = p_unet_up_blocks_1_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_58: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_38, p_unet_up_blocks_1_attentions_1_proj_in_weight, p_unet_up_blocks_1_attentions_1_proj_in_bias);  group_norm_38 = p_unet_up_blocks_1_attentions_1_proj_in_weight = p_unet_up_blocks_1_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_17: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_58, [0, 2, 3, 1]);  conv2d_58 = None\n",
       "                    view_97: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_17, [8, 64, 1280]);  permute_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_33: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_97, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_121: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_122: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_123: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_33, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_33 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_98: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_121, [8, -1, 8, 160]);  linear_121 = None\n",
       "                    transpose_80: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_98, 1, 2);  view_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_99: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_122, [8, -1, 8, 160]);  linear_122 = None\n",
       "                    transpose_81: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_100: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_123, [8, -1, 8, 160]);  linear_123 = None\n",
       "                    transpose_82: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_20: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_80, transpose_81, transpose_82);  transpose_80 = transpose_81 = transpose_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_83: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_20, 1, 2);  scaled_dot_product_attention_20 = None\n",
       "                    view_101: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_83, [8, -1, 1280]);  transpose_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_124: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_101, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_101 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_72: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_124);  linear_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_32: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_72, 1.0);  clone_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_72: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_32, view_97);  div_32 = view_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_34: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_72, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_125: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_34, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_34 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_126: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_127: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_102: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_125, [8, -1, 8, 160]);  linear_125 = None\n",
       "                    transpose_84: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_102, 1, 2);  view_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_103: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_126, [8, -1, 8, 160]);  linear_126 = None\n",
       "                    transpose_85: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_104: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_127, [8, -1, 8, 160]);  linear_127 = None\n",
       "                    transpose_86: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_21: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_84, transpose_85, transpose_86);  transpose_84 = transpose_85 = transpose_86 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_87: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_21, 1, 2);  scaled_dot_product_attention_21 = None\n",
       "                    view_105: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_87, [8, -1, 1280]);  transpose_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_128: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_105, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_105 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_73: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_128);  linear_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_33: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_73, 1.0);  clone_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_73: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_33, add_72);  div_33 = add_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_35: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_73, [1280], p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_129: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_35, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_35 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_8 = torch.ops.aten.split.Tensor(linear_129, 5120, -1);  linear_129 = None\n",
       "                    getitem_16: \"f32[8, 64, 5120]\" = split_8[0]\n",
       "                    getitem_17: \"f32[8, 64, 5120]\" = split_8[1];  split_8 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_14: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_17);  getitem_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_11: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_16, gelu_14);  getitem_16 = gelu_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_74: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_11);  mul_11 = None\n",
       "                    linear_130: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_74, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_74 = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_74: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_130, add_73);  linear_130 = add_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_106: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_74, [8, 8, 8, 1280]);  add_74 = None\n",
       "                    permute_18: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_106, [0, 3, 1, 2]);  view_106 = None\n",
       "                    clone_75: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_18, memory_format = torch.contiguous_format);  permute_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_59: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_75, p_unet_up_blocks_1_attentions_1_proj_out_weight, p_unet_up_blocks_1_attentions_1_proj_out_bias);  clone_75 = p_unet_up_blocks_1_attentions_1_proj_out_weight = p_unet_up_blocks_1_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_75: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_59, div_31);  conv2d_59 = div_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_10: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.cat.default([add_75, conv2d_19], 1);  add_75 = conv2d_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_39: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.group_norm.default(cat_10, 32, p_unet_up_blocks_1_resnets_2_norm1_weight, p_unet_up_blocks_1_resnets_2_norm1_bias);  p_unet_up_blocks_1_resnets_2_norm1_weight = p_unet_up_blocks_1_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_46: \"f32[8, 1920, 8, 8]\" = torch.ops.aten.silu.default(group_norm_39);  group_norm_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_60: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(silu_46, p_unet_up_blocks_1_resnets_2_conv1_weight, p_unet_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_46 = p_unet_up_blocks_1_resnets_2_conv1_weight = p_unet_up_blocks_1_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_47: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_131: \"f32[8, 1280]\" = torch.ops.aten.linear.default(silu_47, p_unet_up_blocks_1_resnets_2_time_emb_proj_weight, p_unet_up_blocks_1_resnets_2_time_emb_proj_bias);  silu_47 = p_unet_up_blocks_1_resnets_2_time_emb_proj_weight = p_unet_up_blocks_1_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_32: \"f32[8, 1280, 1]\" = torch.ops.aten.unsqueeze.default(linear_131, 2);  linear_131 = None\n",
       "                    unsqueeze_33: \"f32[8, 1280, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_32, 3);  unsqueeze_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_76: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_60, unsqueeze_33);  conv2d_60 = unsqueeze_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_40: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(add_76, 32, p_unet_up_blocks_1_resnets_2_norm2_weight, p_unet_up_blocks_1_resnets_2_norm2_bias);  add_76 = p_unet_up_blocks_1_resnets_2_norm2_weight = p_unet_up_blocks_1_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_48: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.silu.default(group_norm_40);  group_norm_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_76: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(silu_48);  silu_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_61: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_76, p_unet_up_blocks_1_resnets_2_conv2_weight, p_unet_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_76 = p_unet_up_blocks_1_resnets_2_conv2_weight = p_unet_up_blocks_1_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_62: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(cat_10, p_unet_up_blocks_1_resnets_2_conv_shortcut_weight, p_unet_up_blocks_1_resnets_2_conv_shortcut_bias);  cat_10 = p_unet_up_blocks_1_resnets_2_conv_shortcut_weight = p_unet_up_blocks_1_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_77: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_62, conv2d_61);  conv2d_62 = conv2d_61 = None\n",
       "                    div_34: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.div.Tensor(add_77, 1.0);  add_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_41: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.group_norm.default(div_34, 32, p_unet_up_blocks_1_attentions_2_norm_weight, p_unet_up_blocks_1_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_1_attentions_2_norm_weight = p_unet_up_blocks_1_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_63: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(group_norm_41, p_unet_up_blocks_1_attentions_2_proj_in_weight, p_unet_up_blocks_1_attentions_2_proj_in_bias);  group_norm_41 = p_unet_up_blocks_1_attentions_2_proj_in_weight = p_unet_up_blocks_1_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_19: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.permute.default(conv2d_63, [0, 2, 3, 1]);  conv2d_63 = None\n",
       "                    view_107: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(permute_19, [8, 64, 1280]);  permute_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_36: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(view_107, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_132: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_133: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_134: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_36, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_36 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_108: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_132, [8, -1, 8, 160]);  linear_132 = None\n",
       "                    transpose_88: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_109: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_133, [8, -1, 8, 160]);  linear_133 = None\n",
       "                    transpose_89: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_110: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_134, [8, -1, 8, 160]);  linear_134 = None\n",
       "                    transpose_90: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_110, 1, 2);  view_110 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_22: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_88, transpose_89, transpose_90);  transpose_88 = transpose_89 = transpose_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_91: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_22, 1, 2);  scaled_dot_product_attention_22 = None\n",
       "                    view_111: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_91, [8, -1, 1280]);  transpose_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_135: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_111, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_111 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_77: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_135);  linear_135 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_35: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_77, 1.0);  clone_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_78: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_35, view_107);  div_35 = view_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_37: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_78, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_136: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(layer_norm_37, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_37 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_137: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_138: \"f32[8, 50, 1280]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_112: \"f32[8, 64, 8, 160]\" = torch.ops.aten.view.default(linear_136, [8, -1, 8, 160]);  linear_136 = None\n",
       "                    transpose_92: \"f32[8, 8, 64, 160]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_113: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_137, [8, -1, 8, 160]);  linear_137 = None\n",
       "                    transpose_93: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_114: \"f32[8, 50, 8, 160]\" = torch.ops.aten.view.default(linear_138, [8, -1, 8, 160]);  linear_138 = None\n",
       "                    transpose_94: \"f32[8, 8, 50, 160]\" = torch.ops.aten.transpose.int(view_114, 1, 2);  view_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_23: \"f32[8, 8, 64, 160]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_92, transpose_93, transpose_94);  transpose_92 = transpose_93 = transpose_94 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_95: \"f32[8, 64, 8, 160]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_23, 1, 2);  scaled_dot_product_attention_23 = None\n",
       "                    view_115: \"f32[8, 64, 1280]\" = torch.ops.aten.view.default(transpose_95, [8, -1, 1280]);  transpose_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_139: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(view_115, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_115 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_78: \"f32[8, 64, 1280]\" = torch.ops.aten.clone.default(linear_139);  linear_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_36: \"f32[8, 64, 1280]\" = torch.ops.aten.div.Tensor(clone_78, 1.0);  clone_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_79: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(div_36, add_78);  div_36 = add_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_38: \"f32[8, 64, 1280]\" = torch.ops.aten.layer_norm.default(add_79, [1280], p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_140: \"f32[8, 64, 10240]\" = torch.ops.aten.linear.default(layer_norm_38, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_38 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_9 = torch.ops.aten.split.Tensor(linear_140, 5120, -1);  linear_140 = None\n",
       "                    getitem_18: \"f32[8, 64, 5120]\" = split_9[0]\n",
       "                    getitem_19: \"f32[8, 64, 5120]\" = split_9[1];  split_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_15: \"f32[8, 64, 5120]\" = torch.ops.aten.gelu.default(getitem_19);  getitem_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_12: \"f32[8, 64, 5120]\" = torch.ops.aten.mul.Tensor(getitem_18, gelu_15);  getitem_18 = gelu_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_79: \"f32[8, 64, 5120]\" = torch.ops.aten.clone.default(mul_12);  mul_12 = None\n",
       "                    linear_141: \"f32[8, 64, 1280]\" = torch.ops.aten.linear.default(clone_79, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_79 = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_80: \"f32[8, 64, 1280]\" = torch.ops.aten.add.Tensor(linear_141, add_79);  linear_141 = add_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_116: \"f32[8, 8, 8, 1280]\" = torch.ops.aten.view.default(add_80, [8, 8, 8, 1280]);  add_80 = None\n",
       "                    permute_20: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.permute.default(view_116, [0, 3, 1, 2]);  view_116 = None\n",
       "                    clone_80: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_64: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.conv2d.default(clone_80, p_unet_up_blocks_1_attentions_2_proj_out_weight, p_unet_up_blocks_1_attentions_2_proj_out_bias);  clone_80 = p_unet_up_blocks_1_attentions_2_proj_out_weight = p_unet_up_blocks_1_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_81: \"f32[8, 1280, 8, 8]\" = torch.ops.aten.add.Tensor(conv2d_64, div_34);  conv2d_64 = div_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_1: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.upsample_nearest2d.vec(add_81, None, [2.0, 2.0]);  add_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_65: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_1, p_unet_up_blocks_1_upsamplers_0_conv_weight, p_unet_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_1 = p_unet_up_blocks_1_upsamplers_0_conv_weight = p_unet_up_blocks_1_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_11: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.cat.default([conv2d_65, add_33], 1);  conv2d_65 = add_33 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_42: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.group_norm.default(cat_11, 32, p_unet_up_blocks_2_resnets_0_norm1_weight, p_unet_up_blocks_2_resnets_0_norm1_bias);  p_unet_up_blocks_2_resnets_0_norm1_weight = p_unet_up_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_49: \"f32[8, 1920, 16, 16]\" = torch.ops.aten.silu.default(group_norm_42);  group_norm_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_66: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_49, p_unet_up_blocks_2_resnets_0_conv1_weight, p_unet_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_49 = p_unet_up_blocks_2_resnets_0_conv1_weight = p_unet_up_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_50: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_142: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_50, p_unet_up_blocks_2_resnets_0_time_emb_proj_weight, p_unet_up_blocks_2_resnets_0_time_emb_proj_bias);  silu_50 = p_unet_up_blocks_2_resnets_0_time_emb_proj_weight = p_unet_up_blocks_2_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_34: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_142, 2);  linear_142 = None\n",
       "                    unsqueeze_35: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_34, 3);  unsqueeze_34 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_82: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_66, unsqueeze_35);  conv2d_66 = unsqueeze_35 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_43: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_82, 32, p_unet_up_blocks_2_resnets_0_norm2_weight, p_unet_up_blocks_2_resnets_0_norm2_bias);  add_82 = p_unet_up_blocks_2_resnets_0_norm2_weight = p_unet_up_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_51: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_43);  group_norm_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_81: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_51);  silu_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_67: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_81, p_unet_up_blocks_2_resnets_0_conv2_weight, p_unet_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_81 = p_unet_up_blocks_2_resnets_0_conv2_weight = p_unet_up_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_68: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_11, p_unet_up_blocks_2_resnets_0_conv_shortcut_weight, p_unet_up_blocks_2_resnets_0_conv_shortcut_bias);  cat_11 = p_unet_up_blocks_2_resnets_0_conv_shortcut_weight = p_unet_up_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_83: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_68, conv2d_67);  conv2d_68 = conv2d_67 = None\n",
       "                    div_37: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_83, 1.0);  add_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_44: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_37, 32, p_unet_up_blocks_2_attentions_0_norm_weight, p_unet_up_blocks_2_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_0_norm_weight = p_unet_up_blocks_2_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_69: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_44, p_unet_up_blocks_2_attentions_0_proj_in_weight, p_unet_up_blocks_2_attentions_0_proj_in_bias);  group_norm_44 = p_unet_up_blocks_2_attentions_0_proj_in_weight = p_unet_up_blocks_2_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_21: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_69, [0, 2, 3, 1]);  conv2d_69 = None\n",
       "                    view_117: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_21, [8, 256, 640]);  permute_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_39: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_117, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_143: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_144: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_145: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_39, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_39 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_118: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_143, [8, -1, 8, 80]);  linear_143 = None\n",
       "                    transpose_96: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_118, 1, 2);  view_118 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_119: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_144, [8, -1, 8, 80]);  linear_144 = None\n",
       "                    transpose_97: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_120: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_145, [8, -1, 8, 80]);  linear_145 = None\n",
       "                    transpose_98: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_24: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_96, transpose_97, transpose_98);  transpose_96 = transpose_97 = transpose_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_99: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_24, 1, 2);  scaled_dot_product_attention_24 = None\n",
       "                    view_121: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_99, [8, -1, 640]);  transpose_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_146: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_121, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_121 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_82: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_146);  linear_146 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_38: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_82, 1.0);  clone_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_84: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_38, view_117);  div_38 = view_117 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_40: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_84, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_147: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_40, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_40 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_148: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_149: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_122: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_147, [8, -1, 8, 80]);  linear_147 = None\n",
       "                    transpose_100: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_122, 1, 2);  view_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_123: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_148, [8, -1, 8, 80]);  linear_148 = None\n",
       "                    transpose_101: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_124: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_149, [8, -1, 8, 80]);  linear_149 = None\n",
       "                    transpose_102: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_25: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_100, transpose_101, transpose_102);  transpose_100 = transpose_101 = transpose_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_103: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_25, 1, 2);  scaled_dot_product_attention_25 = None\n",
       "                    view_125: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_103, [8, -1, 640]);  transpose_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_150: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_125, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_125 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_83: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_150);  linear_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_39: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_83, 1.0);  clone_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_85: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_39, add_84);  div_39 = add_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_41: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_85, [640], p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_151: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_41, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_41 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_10 = torch.ops.aten.split.Tensor(linear_151, 2560, -1);  linear_151 = None\n",
       "                    getitem_20: \"f32[8, 256, 2560]\" = split_10[0]\n",
       "                    getitem_21: \"f32[8, 256, 2560]\" = split_10[1];  split_10 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_16: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_21);  getitem_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_13: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_20, gelu_16);  getitem_20 = gelu_16 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_84: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_13);  mul_13 = None\n",
       "                    linear_152: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_84, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_84 = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_86: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_152, add_85);  linear_152 = add_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_126: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_86, [8, 16, 16, 640]);  add_86 = None\n",
       "                    permute_22: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_126, [0, 3, 1, 2]);  view_126 = None\n",
       "                    clone_85: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_70: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_85, p_unet_up_blocks_2_attentions_0_proj_out_weight, p_unet_up_blocks_2_attentions_0_proj_out_bias);  clone_85 = p_unet_up_blocks_2_attentions_0_proj_out_weight = p_unet_up_blocks_2_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_87: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_70, div_37);  conv2d_70 = div_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_12: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.cat.default([add_87, add_27], 1);  add_87 = add_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_45: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.group_norm.default(cat_12, 32, p_unet_up_blocks_2_resnets_1_norm1_weight, p_unet_up_blocks_2_resnets_1_norm1_bias);  p_unet_up_blocks_2_resnets_1_norm1_weight = p_unet_up_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_52: \"f32[8, 1280, 16, 16]\" = torch.ops.aten.silu.default(group_norm_45);  group_norm_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_71: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_52, p_unet_up_blocks_2_resnets_1_conv1_weight, p_unet_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_52 = p_unet_up_blocks_2_resnets_1_conv1_weight = p_unet_up_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_53: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_153: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_53, p_unet_up_blocks_2_resnets_1_time_emb_proj_weight, p_unet_up_blocks_2_resnets_1_time_emb_proj_bias);  silu_53 = p_unet_up_blocks_2_resnets_1_time_emb_proj_weight = p_unet_up_blocks_2_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_36: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_153, 2);  linear_153 = None\n",
       "                    unsqueeze_37: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_36, 3);  unsqueeze_36 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_88: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_71, unsqueeze_37);  conv2d_71 = unsqueeze_37 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_46: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_88, 32, p_unet_up_blocks_2_resnets_1_norm2_weight, p_unet_up_blocks_2_resnets_1_norm2_bias);  add_88 = p_unet_up_blocks_2_resnets_1_norm2_weight = p_unet_up_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_54: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_46);  group_norm_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_86: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_54);  silu_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_72: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_86, p_unet_up_blocks_2_resnets_1_conv2_weight, p_unet_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_86 = p_unet_up_blocks_2_resnets_1_conv2_weight = p_unet_up_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_73: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_12, p_unet_up_blocks_2_resnets_1_conv_shortcut_weight, p_unet_up_blocks_2_resnets_1_conv_shortcut_bias);  cat_12 = p_unet_up_blocks_2_resnets_1_conv_shortcut_weight = p_unet_up_blocks_2_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_89: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_73, conv2d_72);  conv2d_73 = conv2d_72 = None\n",
       "                    div_40: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_89, 1.0);  add_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_47: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_40, 32, p_unet_up_blocks_2_attentions_1_norm_weight, p_unet_up_blocks_2_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_1_norm_weight = p_unet_up_blocks_2_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_74: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_47, p_unet_up_blocks_2_attentions_1_proj_in_weight, p_unet_up_blocks_2_attentions_1_proj_in_bias);  group_norm_47 = p_unet_up_blocks_2_attentions_1_proj_in_weight = p_unet_up_blocks_2_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_23: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_74, [0, 2, 3, 1]);  conv2d_74 = None\n",
       "                    view_127: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_23, [8, 256, 640]);  permute_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_42: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_127, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_154: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_155: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_156: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_42, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_42 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_128: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_154, [8, -1, 8, 80]);  linear_154 = None\n",
       "                    transpose_104: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_129: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_155, [8, -1, 8, 80]);  linear_155 = None\n",
       "                    transpose_105: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_130: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_156, [8, -1, 8, 80]);  linear_156 = None\n",
       "                    transpose_106: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_130, 1, 2);  view_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_26: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_104, transpose_105, transpose_106);  transpose_104 = transpose_105 = transpose_106 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_107: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_26, 1, 2);  scaled_dot_product_attention_26 = None\n",
       "                    view_131: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_107, [8, -1, 640]);  transpose_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_157: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_131, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_131 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_87: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_157);  linear_157 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_41: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_87, 1.0);  clone_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_90: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_41, view_127);  div_41 = view_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_43: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_90, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_158: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_43, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_43 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_159: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_160: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_132: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_158, [8, -1, 8, 80]);  linear_158 = None\n",
       "                    transpose_108: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_133: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_159, [8, -1, 8, 80]);  linear_159 = None\n",
       "                    transpose_109: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_134: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_160, [8, -1, 8, 80]);  linear_160 = None\n",
       "                    transpose_110: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_134, 1, 2);  view_134 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_27: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_108, transpose_109, transpose_110);  transpose_108 = transpose_109 = transpose_110 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_111: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_27, 1, 2);  scaled_dot_product_attention_27 = None\n",
       "                    view_135: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_111, [8, -1, 640]);  transpose_111 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_161: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_135, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_135 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_88: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_161);  linear_161 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_42: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_88, 1.0);  clone_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_91: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_42, add_90);  div_42 = add_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_44: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_91, [640], p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_162: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_44, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_44 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_11 = torch.ops.aten.split.Tensor(linear_162, 2560, -1);  linear_162 = None\n",
       "                    getitem_22: \"f32[8, 256, 2560]\" = split_11[0]\n",
       "                    getitem_23: \"f32[8, 256, 2560]\" = split_11[1];  split_11 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_17: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_23);  getitem_23 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_14: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_22, gelu_17);  getitem_22 = gelu_17 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_89: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_14);  mul_14 = None\n",
       "                    linear_163: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_89, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_89 = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_92: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_163, add_91);  linear_163 = add_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_136: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_92, [8, 16, 16, 640]);  add_92 = None\n",
       "                    permute_24: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_136, [0, 3, 1, 2]);  view_136 = None\n",
       "                    clone_90: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_24, memory_format = torch.contiguous_format);  permute_24 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_75: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_90, p_unet_up_blocks_2_attentions_1_proj_out_weight, p_unet_up_blocks_2_attentions_1_proj_out_bias);  clone_90 = p_unet_up_blocks_2_attentions_1_proj_out_weight = p_unet_up_blocks_2_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_93: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_75, div_40);  conv2d_75 = div_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_13: \"f32[8, 960, 16, 16]\" = torch.ops.aten.cat.default([add_93, conv2d_9], 1);  add_93 = conv2d_9 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_48: \"f32[8, 960, 16, 16]\" = torch.ops.aten.group_norm.default(cat_13, 32, p_unet_up_blocks_2_resnets_2_norm1_weight, p_unet_up_blocks_2_resnets_2_norm1_bias);  p_unet_up_blocks_2_resnets_2_norm1_weight = p_unet_up_blocks_2_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_55: \"f32[8, 960, 16, 16]\" = torch.ops.aten.silu.default(group_norm_48);  group_norm_48 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_76: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(silu_55, p_unet_up_blocks_2_resnets_2_conv1_weight, p_unet_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_55 = p_unet_up_blocks_2_resnets_2_conv1_weight = p_unet_up_blocks_2_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_56: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_164: \"f32[8, 640]\" = torch.ops.aten.linear.default(silu_56, p_unet_up_blocks_2_resnets_2_time_emb_proj_weight, p_unet_up_blocks_2_resnets_2_time_emb_proj_bias);  silu_56 = p_unet_up_blocks_2_resnets_2_time_emb_proj_weight = p_unet_up_blocks_2_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_38: \"f32[8, 640, 1]\" = torch.ops.aten.unsqueeze.default(linear_164, 2);  linear_164 = None\n",
       "                    unsqueeze_39: \"f32[8, 640, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_38, 3);  unsqueeze_38 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_94: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_76, unsqueeze_39);  conv2d_76 = unsqueeze_39 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_49: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(add_94, 32, p_unet_up_blocks_2_resnets_2_norm2_weight, p_unet_up_blocks_2_resnets_2_norm2_bias);  add_94 = p_unet_up_blocks_2_resnets_2_norm2_weight = p_unet_up_blocks_2_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_57: \"f32[8, 640, 16, 16]\" = torch.ops.aten.silu.default(group_norm_49);  group_norm_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_91: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(silu_57);  silu_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_77: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_91, p_unet_up_blocks_2_resnets_2_conv2_weight, p_unet_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_91 = p_unet_up_blocks_2_resnets_2_conv2_weight = p_unet_up_blocks_2_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_78: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(cat_13, p_unet_up_blocks_2_resnets_2_conv_shortcut_weight, p_unet_up_blocks_2_resnets_2_conv_shortcut_bias);  cat_13 = p_unet_up_blocks_2_resnets_2_conv_shortcut_weight = p_unet_up_blocks_2_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_95: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_78, conv2d_77);  conv2d_78 = conv2d_77 = None\n",
       "                    div_43: \"f32[8, 640, 16, 16]\" = torch.ops.aten.div.Tensor(add_95, 1.0);  add_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_50: \"f32[8, 640, 16, 16]\" = torch.ops.aten.group_norm.default(div_43, 32, p_unet_up_blocks_2_attentions_2_norm_weight, p_unet_up_blocks_2_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_2_attentions_2_norm_weight = p_unet_up_blocks_2_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_79: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(group_norm_50, p_unet_up_blocks_2_attentions_2_proj_in_weight, p_unet_up_blocks_2_attentions_2_proj_in_bias);  group_norm_50 = p_unet_up_blocks_2_attentions_2_proj_in_weight = p_unet_up_blocks_2_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_25: \"f32[8, 16, 16, 640]\" = torch.ops.aten.permute.default(conv2d_79, [0, 2, 3, 1]);  conv2d_79 = None\n",
       "                    view_137: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(permute_25, [8, 256, 640]);  permute_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_45: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(view_137, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_165: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_166: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_167: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_45, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_45 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_138: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_165, [8, -1, 8, 80]);  linear_165 = None\n",
       "                    transpose_112: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_138, 1, 2);  view_138 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_139: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_166, [8, -1, 8, 80]);  linear_166 = None\n",
       "                    transpose_113: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_140: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_167, [8, -1, 8, 80]);  linear_167 = None\n",
       "                    transpose_114: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_28: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_112, transpose_113, transpose_114);  transpose_112 = transpose_113 = transpose_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_115: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_28, 1, 2);  scaled_dot_product_attention_28 = None\n",
       "                    view_141: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_115, [8, -1, 640]);  transpose_115 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_168: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_141, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_141 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_92: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_168);  linear_168 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_44: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_92, 1.0);  clone_92 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_96: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_44, view_137);  div_44 = view_137 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_46: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_96, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_169: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(layer_norm_46, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_46 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_170: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_171: \"f32[8, 50, 640]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_142: \"f32[8, 256, 8, 80]\" = torch.ops.aten.view.default(linear_169, [8, -1, 8, 80]);  linear_169 = None\n",
       "                    transpose_116: \"f32[8, 8, 256, 80]\" = torch.ops.aten.transpose.int(view_142, 1, 2);  view_142 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_143: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_170, [8, -1, 8, 80]);  linear_170 = None\n",
       "                    transpose_117: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_144: \"f32[8, 50, 8, 80]\" = torch.ops.aten.view.default(linear_171, [8, -1, 8, 80]);  linear_171 = None\n",
       "                    transpose_118: \"f32[8, 8, 50, 80]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_29: \"f32[8, 8, 256, 80]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_116, transpose_117, transpose_118);  transpose_116 = transpose_117 = transpose_118 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_119: \"f32[8, 256, 8, 80]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_29, 1, 2);  scaled_dot_product_attention_29 = None\n",
       "                    view_145: \"f32[8, 256, 640]\" = torch.ops.aten.view.default(transpose_119, [8, -1, 640]);  transpose_119 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_172: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(view_145, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_145 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_93: \"f32[8, 256, 640]\" = torch.ops.aten.clone.default(linear_172);  linear_172 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_45: \"f32[8, 256, 640]\" = torch.ops.aten.div.Tensor(clone_93, 1.0);  clone_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_97: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(div_45, add_96);  div_45 = add_96 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_47: \"f32[8, 256, 640]\" = torch.ops.aten.layer_norm.default(add_97, [640], p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_173: \"f32[8, 256, 5120]\" = torch.ops.aten.linear.default(layer_norm_47, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_47 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_12 = torch.ops.aten.split.Tensor(linear_173, 2560, -1);  linear_173 = None\n",
       "                    getitem_24: \"f32[8, 256, 2560]\" = split_12[0]\n",
       "                    getitem_25: \"f32[8, 256, 2560]\" = split_12[1];  split_12 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_18: \"f32[8, 256, 2560]\" = torch.ops.aten.gelu.default(getitem_25);  getitem_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_15: \"f32[8, 256, 2560]\" = torch.ops.aten.mul.Tensor(getitem_24, gelu_18);  getitem_24 = gelu_18 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_94: \"f32[8, 256, 2560]\" = torch.ops.aten.clone.default(mul_15);  mul_15 = None\n",
       "                    linear_174: \"f32[8, 256, 640]\" = torch.ops.aten.linear.default(clone_94, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_94 = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_98: \"f32[8, 256, 640]\" = torch.ops.aten.add.Tensor(linear_174, add_97);  linear_174 = add_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_146: \"f32[8, 16, 16, 640]\" = torch.ops.aten.view.default(add_98, [8, 16, 16, 640]);  add_98 = None\n",
       "                    permute_26: \"f32[8, 640, 16, 16]\" = torch.ops.aten.permute.default(view_146, [0, 3, 1, 2]);  view_146 = None\n",
       "                    clone_95: \"f32[8, 640, 16, 16]\" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_80: \"f32[8, 640, 16, 16]\" = torch.ops.aten.conv2d.default(clone_95, p_unet_up_blocks_2_attentions_2_proj_out_weight, p_unet_up_blocks_2_attentions_2_proj_out_bias);  clone_95 = p_unet_up_blocks_2_attentions_2_proj_out_weight = p_unet_up_blocks_2_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_99: \"f32[8, 640, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_80, div_43);  conv2d_80 = div_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_2: \"f32[8, 640, 32, 32]\" = torch.ops.aten.upsample_nearest2d.vec(add_99, None, [2.0, 2.0]);  add_99 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_81: \"f32[8, 640, 32, 32]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_2, p_unet_up_blocks_2_upsamplers_0_conv_weight, p_unet_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_2 = p_unet_up_blocks_2_upsamplers_0_conv_weight = p_unet_up_blocks_2_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_14: \"f32[8, 960, 32, 32]\" = torch.ops.aten.cat.default([conv2d_81, add_21], 1);  conv2d_81 = add_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_51: \"f32[8, 960, 32, 32]\" = torch.ops.aten.group_norm.default(cat_14, 32, p_unet_up_blocks_3_resnets_0_norm1_weight, p_unet_up_blocks_3_resnets_0_norm1_bias);  p_unet_up_blocks_3_resnets_0_norm1_weight = p_unet_up_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_58: \"f32[8, 960, 32, 32]\" = torch.ops.aten.silu.default(group_norm_51);  group_norm_51 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_82: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_58, p_unet_up_blocks_3_resnets_0_conv1_weight, p_unet_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_58 = p_unet_up_blocks_3_resnets_0_conv1_weight = p_unet_up_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_59: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_175: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_59, p_unet_up_blocks_3_resnets_0_time_emb_proj_weight, p_unet_up_blocks_3_resnets_0_time_emb_proj_bias);  silu_59 = p_unet_up_blocks_3_resnets_0_time_emb_proj_weight = p_unet_up_blocks_3_resnets_0_time_emb_proj_bias = None\n",
       "                    unsqueeze_40: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_175, 2);  linear_175 = None\n",
       "                    unsqueeze_41: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_40, 3);  unsqueeze_40 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_100: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_82, unsqueeze_41);  conv2d_82 = unsqueeze_41 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_52: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_100, 32, p_unet_up_blocks_3_resnets_0_norm2_weight, p_unet_up_blocks_3_resnets_0_norm2_bias);  add_100 = p_unet_up_blocks_3_resnets_0_norm2_weight = p_unet_up_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_60: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_52);  group_norm_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_96: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_60);  silu_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_83: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_96, p_unet_up_blocks_3_resnets_0_conv2_weight, p_unet_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_96 = p_unet_up_blocks_3_resnets_0_conv2_weight = p_unet_up_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_84: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_14, p_unet_up_blocks_3_resnets_0_conv_shortcut_weight, p_unet_up_blocks_3_resnets_0_conv_shortcut_bias);  cat_14 = p_unet_up_blocks_3_resnets_0_conv_shortcut_weight = p_unet_up_blocks_3_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_101: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_84, conv2d_83);  conv2d_84 = conv2d_83 = None\n",
       "                    div_46: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_101, 1.0);  add_101 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_53: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_46, 32, p_unet_up_blocks_3_attentions_0_norm_weight, p_unet_up_blocks_3_attentions_0_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_0_norm_weight = p_unet_up_blocks_3_attentions_0_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_85: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_53, p_unet_up_blocks_3_attentions_0_proj_in_weight, p_unet_up_blocks_3_attentions_0_proj_in_bias);  group_norm_53 = p_unet_up_blocks_3_attentions_0_proj_in_weight = p_unet_up_blocks_3_attentions_0_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_27: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_85, [0, 2, 3, 1]);  conv2d_85 = None\n",
       "                    view_147: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_27, [8, 1024, 320]);  permute_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_48: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_147, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_176: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_177: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_178: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_48, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight);  layer_norm_48 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_148: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_176, [8, -1, 8, 40]);  linear_176 = None\n",
       "                    transpose_120: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_149: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_177, [8, -1, 8, 40]);  linear_177 = None\n",
       "                    transpose_121: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_150: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_178, [8, -1, 8, 40]);  linear_178 = None\n",
       "                    transpose_122: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_150, 1, 2);  view_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_30: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_120, transpose_121, transpose_122);  transpose_120 = transpose_121 = transpose_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_123: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_30, 1, 2);  scaled_dot_product_attention_30 = None\n",
       "                    view_151: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_123, [8, -1, 320]);  transpose_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_179: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_151, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias);  view_151 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_97: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_179);  linear_179 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_47: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_97, 1.0);  clone_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_102: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_47, view_147);  div_47 = view_147 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_49: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_102, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_180: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_49, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight);  layer_norm_49 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_181: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_182: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_152: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_180, [8, -1, 8, 40]);  linear_180 = None\n",
       "                    transpose_124: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_153: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_181, [8, -1, 8, 40]);  linear_181 = None\n",
       "                    transpose_125: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_154: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_182, [8, -1, 8, 40]);  linear_182 = None\n",
       "                    transpose_126: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_154, 1, 2);  view_154 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_31: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_124, transpose_125, transpose_126);  transpose_124 = transpose_125 = transpose_126 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_127: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_31, 1, 2);  scaled_dot_product_attention_31 = None\n",
       "                    view_155: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_127, [8, -1, 320]);  transpose_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_183: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_155, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias);  view_155 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_98: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_183);  linear_183 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_48: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_98, 1.0);  clone_98 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_103: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_48, add_102);  div_48 = add_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_50: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_103, [320], p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_184: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_50, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_50 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_13 = torch.ops.aten.split.Tensor(linear_184, 1280, -1);  linear_184 = None\n",
       "                    getitem_26: \"f32[8, 1024, 1280]\" = split_13[0]\n",
       "                    getitem_27: \"f32[8, 1024, 1280]\" = split_13[1];  split_13 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_19: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_27);  getitem_27 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_16: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_26, gelu_19);  getitem_26 = gelu_19 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_99: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_16);  mul_16 = None\n",
       "                    linear_185: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_99, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias);  clone_99 = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_104: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_185, add_103);  linear_185 = add_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_156: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_104, [8, 32, 32, 320]);  add_104 = None\n",
       "                    permute_28: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_156, [0, 3, 1, 2]);  view_156 = None\n",
       "                    clone_100: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_86: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_100, p_unet_up_blocks_3_attentions_0_proj_out_weight, p_unet_up_blocks_3_attentions_0_proj_out_bias);  clone_100 = p_unet_up_blocks_3_attentions_0_proj_out_weight = p_unet_up_blocks_3_attentions_0_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_105: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_86, div_46);  conv2d_86 = div_46 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_15: \"f32[8, 640, 32, 32]\" = torch.ops.aten.cat.default([add_105, add_15], 1);  add_105 = add_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_54: \"f32[8, 640, 32, 32]\" = torch.ops.aten.group_norm.default(cat_15, 32, p_unet_up_blocks_3_resnets_1_norm1_weight, p_unet_up_blocks_3_resnets_1_norm1_bias);  p_unet_up_blocks_3_resnets_1_norm1_weight = p_unet_up_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_61: \"f32[8, 640, 32, 32]\" = torch.ops.aten.silu.default(group_norm_54);  group_norm_54 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_87: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_61, p_unet_up_blocks_3_resnets_1_conv1_weight, p_unet_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_61 = p_unet_up_blocks_3_resnets_1_conv1_weight = p_unet_up_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_62: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25)\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_186: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_62, p_unet_up_blocks_3_resnets_1_time_emb_proj_weight, p_unet_up_blocks_3_resnets_1_time_emb_proj_bias);  silu_62 = p_unet_up_blocks_3_resnets_1_time_emb_proj_weight = p_unet_up_blocks_3_resnets_1_time_emb_proj_bias = None\n",
       "                    unsqueeze_42: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_186, 2);  linear_186 = None\n",
       "                    unsqueeze_43: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_42, 3);  unsqueeze_42 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_106: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_87, unsqueeze_43);  conv2d_87 = unsqueeze_43 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_55: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_106, 32, p_unet_up_blocks_3_resnets_1_norm2_weight, p_unet_up_blocks_3_resnets_1_norm2_bias);  add_106 = p_unet_up_blocks_3_resnets_1_norm2_weight = p_unet_up_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_63: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_55);  group_norm_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_101: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_63);  silu_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_88: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_101, p_unet_up_blocks_3_resnets_1_conv2_weight, p_unet_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_101 = p_unet_up_blocks_3_resnets_1_conv2_weight = p_unet_up_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_89: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_15, p_unet_up_blocks_3_resnets_1_conv_shortcut_weight, p_unet_up_blocks_3_resnets_1_conv_shortcut_bias);  cat_15 = p_unet_up_blocks_3_resnets_1_conv_shortcut_weight = p_unet_up_blocks_3_resnets_1_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_107: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_89, conv2d_88);  conv2d_89 = conv2d_88 = None\n",
       "                    div_49: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_107, 1.0);  add_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_56: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_49, 32, p_unet_up_blocks_3_attentions_1_norm_weight, p_unet_up_blocks_3_attentions_1_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_1_norm_weight = p_unet_up_blocks_3_attentions_1_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_90: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_56, p_unet_up_blocks_3_attentions_1_proj_in_weight, p_unet_up_blocks_3_attentions_1_proj_in_bias);  group_norm_56 = p_unet_up_blocks_3_attentions_1_proj_in_weight = p_unet_up_blocks_3_attentions_1_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_29: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_90, [0, 2, 3, 1]);  conv2d_90 = None\n",
       "                    view_157: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_29, [8, 1024, 320]);  permute_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_51: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_157, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_187: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_188: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_189: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_51, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight);  layer_norm_51 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_158: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_187, [8, -1, 8, 40]);  linear_187 = None\n",
       "                    transpose_128: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_158, 1, 2);  view_158 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_159: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_188, [8, -1, 8, 40]);  linear_188 = None\n",
       "                    transpose_129: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_160: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_189, [8, -1, 8, 40]);  linear_189 = None\n",
       "                    transpose_130: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_32: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_128, transpose_129, transpose_130);  transpose_128 = transpose_129 = transpose_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_131: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_32, 1, 2);  scaled_dot_product_attention_32 = None\n",
       "                    view_161: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_131, [8, -1, 320]);  transpose_131 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_190: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_161, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias);  view_161 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_102: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_190);  linear_190 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_50: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_102, 1.0);  clone_102 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_108: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_50, view_157);  div_50 = view_157 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_52: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_108, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_191: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_52, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight);  layer_norm_52 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_192: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_193: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_162: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_191, [8, -1, 8, 40]);  linear_191 = None\n",
       "                    transpose_132: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_162, 1, 2);  view_162 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_163: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_192, [8, -1, 8, 40]);  linear_192 = None\n",
       "                    transpose_133: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_163, 1, 2);  view_163 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_164: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_193, [8, -1, 8, 40]);  linear_193 = None\n",
       "                    transpose_134: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_164, 1, 2);  view_164 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_33: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_132, transpose_133, transpose_134);  transpose_132 = transpose_133 = transpose_134 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_135: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_33, 1, 2);  scaled_dot_product_attention_33 = None\n",
       "                    view_165: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_135, [8, -1, 320]);  transpose_135 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_194: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_165, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias);  view_165 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_103: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_194);  linear_194 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_51: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_103, 1.0);  clone_103 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_109: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_51, add_108);  div_51 = add_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_53: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_109, [320], p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_195: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_53, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_53 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_14 = torch.ops.aten.split.Tensor(linear_195, 1280, -1);  linear_195 = None\n",
       "                    getitem_28: \"f32[8, 1024, 1280]\" = split_14[0]\n",
       "                    getitem_29: \"f32[8, 1024, 1280]\" = split_14[1];  split_14 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_20: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_29);  getitem_29 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_17: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_28, gelu_20);  getitem_28 = gelu_20 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_104: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_17);  mul_17 = None\n",
       "                    linear_196: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_104, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias);  clone_104 = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_110: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_196, add_109);  linear_196 = add_109 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_166: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_110, [8, 32, 32, 320]);  add_110 = None\n",
       "                    permute_30: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_166, [0, 3, 1, 2]);  view_166 = None\n",
       "                    clone_105: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_30, memory_format = torch.contiguous_format);  permute_30 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_91: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_105, p_unet_up_blocks_3_attentions_1_proj_out_weight, p_unet_up_blocks_3_attentions_1_proj_out_bias);  clone_105 = p_unet_up_blocks_3_attentions_1_proj_out_weight = p_unet_up_blocks_3_attentions_1_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_111: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_91, div_49);  conv2d_91 = div_49 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2521 in forward, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
       "                    cat_16: \"f32[8, 640, 32, 32]\" = torch.ops.aten.cat.default([add_111, conv2d], 1);  add_111 = conv2d = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_57: \"f32[8, 640, 32, 32]\" = torch.ops.aten.group_norm.default(cat_16, 32, p_unet_up_blocks_3_resnets_2_norm1_weight, p_unet_up_blocks_3_resnets_2_norm1_bias);  p_unet_up_blocks_3_resnets_2_norm1_weight = p_unet_up_blocks_3_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_64: \"f32[8, 640, 32, 32]\" = torch.ops.aten.silu.default(group_norm_57);  group_norm_57 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_92: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(silu_64, p_unet_up_blocks_3_resnets_2_conv1_weight, p_unet_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_64 = p_unet_up_blocks_3_resnets_2_conv1_weight = p_unet_up_blocks_3_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:345 in forward, code: temb = self.nonlinearity(temb)\n",
       "                    silu_65: \"f32[8, 1280]\" = torch.ops.aten.silu.default(linear_25);  linear_25 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:346 in forward, code: temb = self.time_emb_proj(temb)[:, :, None, None]\n",
       "                    linear_197: \"f32[8, 320]\" = torch.ops.aten.linear.default(silu_65, p_unet_up_blocks_3_resnets_2_time_emb_proj_weight, p_unet_up_blocks_3_resnets_2_time_emb_proj_bias);  silu_65 = p_unet_up_blocks_3_resnets_2_time_emb_proj_weight = p_unet_up_blocks_3_resnets_2_time_emb_proj_bias = None\n",
       "                    unsqueeze_44: \"f32[8, 320, 1]\" = torch.ops.aten.unsqueeze.default(linear_197, 2);  linear_197 = None\n",
       "                    unsqueeze_45: \"f32[8, 320, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_44, 3);  unsqueeze_44 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:350 in forward, code: hidden_states = hidden_states + temb\n",
       "                    add_112: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_92, unsqueeze_45);  conv2d_92 = unsqueeze_45 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_58: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_112, 32, p_unet_up_blocks_3_resnets_2_norm2_weight, p_unet_up_blocks_3_resnets_2_norm2_bias);  add_112 = p_unet_up_blocks_3_resnets_2_norm2_weight = p_unet_up_blocks_3_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_66: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_58);  group_norm_58 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_106: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(silu_66);  silu_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_93: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_106, p_unet_up_blocks_3_resnets_2_conv2_weight, p_unet_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_106 = p_unet_up_blocks_3_resnets_2_conv2_weight = p_unet_up_blocks_3_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_94: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(cat_16, p_unet_up_blocks_3_resnets_2_conv_shortcut_weight, p_unet_up_blocks_3_resnets_2_conv_shortcut_bias);  cat_16 = p_unet_up_blocks_3_resnets_2_conv_shortcut_weight = p_unet_up_blocks_3_resnets_2_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_113: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_94, conv2d_93);  conv2d_94 = conv2d_93 = None\n",
       "                    div_52: \"f32[8, 320, 32, 32]\" = torch.ops.aten.div.Tensor(add_113, 1.0);  add_113 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:481 in _operate_on_continuous_inputs, code: hidden_states = self.norm(hidden_states)\n",
       "                    group_norm_59: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(div_52, 32, p_unet_up_blocks_3_attentions_2_norm_weight, p_unet_up_blocks_3_attentions_2_norm_bias, 1e-06);  p_unet_up_blocks_3_attentions_2_norm_weight = p_unet_up_blocks_3_attentions_2_norm_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:484 in _operate_on_continuous_inputs, code: hidden_states = self.proj_in(hidden_states)\n",
       "                    conv2d_95: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(group_norm_59, p_unet_up_blocks_3_attentions_2_proj_in_weight, p_unet_up_blocks_3_attentions_2_proj_in_bias);  group_norm_59 = p_unet_up_blocks_3_attentions_2_proj_in_weight = p_unet_up_blocks_3_attentions_2_proj_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:486 in _operate_on_continuous_inputs, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)\n",
       "                    permute_31: \"f32[8, 32, 32, 320]\" = torch.ops.aten.permute.default(conv2d_95, [0, 2, 3, 1]);  conv2d_95 = None\n",
       "                    view_167: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(permute_31, [8, 1024, 320]);  permute_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:447 in forward, code: norm_hidden_states = self.norm1(hidden_states)\n",
       "                    layer_norm_54: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(view_167, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_198: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_199: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_200: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_54, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight);  layer_norm_54 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_168: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_198, [8, -1, 8, 40]);  linear_198 = None\n",
       "                    transpose_136: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_168, 1, 2);  view_168 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_169: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_199, [8, -1, 8, 40]);  linear_199 = None\n",
       "                    transpose_137: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_169, 1, 2);  view_169 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_170: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_200, [8, -1, 8, 40]);  linear_200 = None\n",
       "                    transpose_138: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_170, 1, 2);  view_170 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_34: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_136, transpose_137, transpose_138);  transpose_136 = transpose_137 = transpose_138 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_139: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_34, 1, 2);  scaled_dot_product_attention_34 = None\n",
       "                    view_171: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_139, [8, -1, 320]);  transpose_139 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_201: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_171, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias);  view_171 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_107: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_201);  linear_201 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_53: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_107, 1.0);  clone_107 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:478 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_114: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_53, view_167);  div_53 = view_167 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:491 in forward, code: norm_hidden_states = self.norm2(hidden_states)\n",
       "                    layer_norm_55: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_114, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_202: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(layer_norm_55, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight);  layer_norm_55 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_203: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_204: \"f32[8, 50, 320]\" = torch.ops.aten.linear.default(add_9, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight);  add_9 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_172: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.view.default(linear_202, [8, -1, 8, 40]);  linear_202 = None\n",
       "                    transpose_140: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.transpose.int(view_172, 1, 2);  view_172 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_173: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_203, [8, -1, 8, 40]);  linear_203 = None\n",
       "                    transpose_141: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_173, 1, 2);  view_173 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_174: \"f32[8, 50, 8, 40]\" = torch.ops.aten.view.default(linear_204, [8, -1, 8, 40]);  linear_204 = None\n",
       "                    transpose_142: \"f32[8, 8, 50, 40]\" = torch.ops.aten.transpose.int(view_174, 1, 2);  view_174 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_35: \"f32[8, 8, 1024, 40]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_140, transpose_141, transpose_142);  transpose_140 = transpose_141 = transpose_142 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_143: \"f32[8, 1024, 8, 40]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_35, 1, 2);  scaled_dot_product_attention_35 = None\n",
       "                    view_175: \"f32[8, 1024, 320]\" = torch.ops.aten.view.default(transpose_143, [8, -1, 320]);  transpose_143 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_205: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(view_175, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias);  view_175 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_108: \"f32[8, 1024, 320]\" = torch.ops.aten.clone.default(linear_205);  linear_205 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    div_54: \"f32[8, 1024, 320]\" = torch.ops.aten.div.Tensor(clone_108, 1.0);  clone_108 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:510 in forward, code: hidden_states = attn_output + hidden_states\n",
       "                    add_115: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(div_54, add_114);  div_54 = add_114 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:517 in forward, code: norm_hidden_states = self.norm3(hidden_states)\n",
       "                    layer_norm_56: \"f32[8, 1024, 320]\" = torch.ops.aten.layer_norm.default(add_115, [320], p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias);  p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:117 in forward, code: hidden_states = self.proj(hidden_states)\n",
       "                    linear_206: \"f32[8, 1024, 2560]\" = torch.ops.aten.linear.default(layer_norm_56, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias);  layer_norm_56 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:122 in forward, code: hidden_states, gate = hidden_states.chunk(2, dim=-1)\n",
       "                    split_15 = torch.ops.aten.split.Tensor(linear_206, 1280, -1);  linear_206 = None\n",
       "                    getitem_30: \"f32[8, 1024, 1280]\" = split_15[0]\n",
       "                    getitem_31: \"f32[8, 1024, 1280]\" = split_15[1];  split_15 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:109 in gelu, code: return F.gelu(gate)\n",
       "                    gelu_21: \"f32[8, 1024, 1280]\" = torch.ops.aten.gelu.default(getitem_31);  getitem_31 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/activations.py:123 in forward, code: return hidden_states * self.gelu(gate)\n",
       "                    mul_18: \"f32[8, 1024, 1280]\" = torch.ops.aten.mul.Tensor(getitem_30, gelu_21);  getitem_30 = gelu_21 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:1166 in forward, code: hidden_states = module(hidden_states)\n",
       "                    clone_109: \"f32[8, 1024, 1280]\" = torch.ops.aten.clone.default(mul_18);  mul_18 = None\n",
       "                    linear_207: \"f32[8, 1024, 320]\" = torch.ops.aten.linear.default(clone_109, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias);  clone_109 = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight = p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention.py:537 in forward, code: hidden_states = ff_output + hidden_states\n",
       "                    add_116: \"f32[8, 1024, 320]\" = torch.ops.aten.add.Tensor(linear_207, add_115);  linear_207 = add_115 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:517 in _get_output_for_continuous_inputs, code: hidden_states.reshape(batch_size, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
       "                    view_176: \"f32[8, 32, 32, 320]\" = torch.ops.aten.view.default(add_116, [8, 32, 32, 320]);  add_116 = None\n",
       "                    permute_32: \"f32[8, 320, 32, 32]\" = torch.ops.aten.permute.default(view_176, [0, 3, 1, 2]);  view_176 = None\n",
       "                    clone_110: \"f32[8, 320, 32, 32]\" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:519 in _get_output_for_continuous_inputs, code: hidden_states = self.proj_out(hidden_states)\n",
       "                    conv2d_96: \"f32[8, 320, 32, 32]\" = torch.ops.aten.conv2d.default(clone_110, p_unet_up_blocks_3_attentions_2_proj_out_weight, p_unet_up_blocks_3_attentions_2_proj_out_bias);  clone_110 = p_unet_up_blocks_3_attentions_2_proj_out_weight = p_unet_up_blocks_3_attentions_2_proj_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:526 in _get_output_for_continuous_inputs, code: output = hidden_states + residual\n",
       "                    add_117: \"f32[8, 320, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_96, div_52);  conv2d_96 = div_52 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1301 in forward, code: sample = self.conv_norm_out(sample)\n",
       "                    group_norm_60: \"f32[8, 320, 32, 32]\" = torch.ops.aten.group_norm.default(add_117, 32, p_unet_conv_norm_out_weight, p_unet_conv_norm_out_bias);  add_117 = p_unet_conv_norm_out_weight = p_unet_conv_norm_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1302 in forward, code: sample = self.conv_act(sample)\n",
       "                    silu_67: \"f32[8, 320, 32, 32]\" = torch.ops.aten.silu.default(group_norm_60);  group_norm_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1303 in forward, code: sample = self.conv_out(sample)\n",
       "                    conv2d_97: \"f32[8, 4, 32, 32]\" = torch.ops.aten.conv2d.default(silu_67, p_unet_conv_out_weight, p_unet_conv_out_bias, [1, 1], [1, 1]);  silu_67 = p_unet_conv_out_weight = p_unet_conv_out_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:48 in decode_latents, code: latents = (1/  self.scaling_factor) * latents\n",
       "                    mul_19: \"f32[8, 4, 32, 32]\" = torch.ops.aten.mul.Tensor(conv2d_97, 5.489980785067252);  conv2d_97 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:290 in _decode, code: z = self.post_quant_conv(z)\n",
       "                    conv2d_98: \"f32[8, 4, 32, 32]\" = torch.ops.aten.conv2d.default(mul_19, p_vae_post_quant_conv_weight, p_vae_post_quant_conv_bias);  mul_19 = p_vae_post_quant_conv_weight = p_vae_post_quant_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:291 in forward, code: sample = self.conv_in(sample)\n",
       "                    conv2d_99: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(conv2d_98, p_vae_decoder_conv_in_weight, p_vae_decoder_conv_in_bias, [1, 1], [1, 1]);  conv2d_98 = p_vae_decoder_conv_in_weight = p_vae_decoder_conv_in_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_61: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_99, 32, p_vae_decoder_mid_block_resnets_0_norm1_weight, p_vae_decoder_mid_block_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_mid_block_resnets_0_norm1_weight = p_vae_decoder_mid_block_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_68: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_61);  group_norm_61 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_100: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_68, p_vae_decoder_mid_block_resnets_0_conv1_weight, p_vae_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_68 = p_vae_decoder_mid_block_resnets_0_conv1_weight = p_vae_decoder_mid_block_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_62: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_100, 32, p_vae_decoder_mid_block_resnets_0_norm2_weight, p_vae_decoder_mid_block_resnets_0_norm2_bias, 1e-06);  conv2d_100 = p_vae_decoder_mid_block_resnets_0_norm2_weight = p_vae_decoder_mid_block_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_69: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_62);  group_norm_62 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_111: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_69);  silu_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_101: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_111, p_vae_decoder_mid_block_resnets_0_conv2_weight, p_vae_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_111 = p_vae_decoder_mid_block_resnets_0_conv2_weight = p_vae_decoder_mid_block_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_118: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_99, conv2d_101);  conv2d_99 = conv2d_101 = None\n",
       "                    scalar_tensor_default_4: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_55: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_118, scalar_tensor_default_4);  add_118 = scalar_tensor_default_4 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2318 in __call__, code: hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
       "                    view_177: \"f32[8, 512, 1024]\" = torch.ops.aten.view.default(div_55, [8, 512, 1024])\n",
       "                    transpose_144: \"f32[8, 1024, 512]\" = torch.ops.aten.transpose.int(view_177, 1, 2);  view_177 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2331 in __call__, code: hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
       "                    transpose_145: \"f32[8, 512, 1024]\" = torch.ops.aten.transpose.int(transpose_144, 1, 2);  transpose_144 = None\n",
       "                    group_norm_63: \"f32[8, 512, 1024]\" = torch.ops.aten.group_norm.default(transpose_145, 32, p_vae_decoder_mid_block_attentions_0_group_norm_weight, p_vae_decoder_mid_block_attentions_0_group_norm_bias, 1e-06);  transpose_145 = p_vae_decoder_mid_block_attentions_0_group_norm_weight = p_vae_decoder_mid_block_attentions_0_group_norm_bias = None\n",
       "                    transpose_146: \"f32[8, 1024, 512]\" = torch.ops.aten.transpose.int(group_norm_63, 1, 2);  group_norm_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2333 in __call__, code: query = attn.to_q(hidden_states)\n",
       "                    linear_208: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_q_weight, p_vae_decoder_mid_block_attentions_0_to_q_bias);  p_vae_decoder_mid_block_attentions_0_to_q_weight = p_vae_decoder_mid_block_attentions_0_to_q_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2340 in __call__, code: key = attn.to_k(encoder_hidden_states)\n",
       "                    linear_209: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_k_weight, p_vae_decoder_mid_block_attentions_0_to_k_bias);  p_vae_decoder_mid_block_attentions_0_to_k_weight = p_vae_decoder_mid_block_attentions_0_to_k_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2341 in __call__, code: value = attn.to_v(encoder_hidden_states)\n",
       "                    linear_210: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(transpose_146, p_vae_decoder_mid_block_attentions_0_to_v_weight, p_vae_decoder_mid_block_attentions_0_to_v_bias);  transpose_146 = p_vae_decoder_mid_block_attentions_0_to_v_weight = p_vae_decoder_mid_block_attentions_0_to_v_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2346 in __call__, code: query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_178: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_208, [8, -1, 1, 512]);  linear_208 = None\n",
       "                    transpose_147: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_178, 1, 2);  view_178 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2348 in __call__, code: key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_179: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_209, [8, -1, 1, 512]);  linear_209 = None\n",
       "                    transpose_148: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_179, 1, 2);  view_179 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2349 in __call__, code: value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
       "                    view_180: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.view.default(linear_210, [8, -1, 1, 512]);  linear_210 = None\n",
       "                    transpose_149: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.transpose.int(view_180, 1, 2);  view_180 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358 in __call__, code: hidden_states = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_36: \"f32[8, 1, 1024, 512]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_147, transpose_148, transpose_149);  transpose_147 = transpose_148 = transpose_149 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2362 in __call__, code: hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
       "                    transpose_150: \"f32[8, 1024, 1, 512]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_36, 1, 2);  scaled_dot_product_attention_36 = None\n",
       "                    view_181: \"f32[8, 1024, 512]\" = torch.ops.aten.view.default(transpose_150, [8, -1, 512]);  transpose_150 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2366 in __call__, code: hidden_states = attn.to_out[0](hidden_states)\n",
       "                    linear_211: \"f32[8, 1024, 512]\" = torch.ops.aten.linear.default(view_181, p_vae_decoder_mid_block_attentions_0_to_out_0_weight, p_vae_decoder_mid_block_attentions_0_to_out_0_bias);  view_181 = p_vae_decoder_mid_block_attentions_0_to_out_0_weight = p_vae_decoder_mid_block_attentions_0_to_out_0_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2368 in __call__, code: hidden_states = attn.to_out[1](hidden_states)\n",
       "                    clone_112: \"f32[8, 1024, 512]\" = torch.ops.aten.clone.default(linear_211);  linear_211 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2371 in __call__, code: hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
       "                    transpose_151: \"f32[8, 512, 1024]\" = torch.ops.aten.transpose.int(clone_112, -1, -2);  clone_112 = None\n",
       "                    view_182: \"f32[8, 512, 32, 32]\" = torch.ops.aten.view.default(transpose_151, [8, 512, 32, 32]);  transpose_151 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2374 in __call__, code: hidden_states = hidden_states + residual\n",
       "                    add_119: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(view_182, div_55);  view_182 = div_55 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2376 in __call__, code: hidden_states = hidden_states / attn.rescale_output_factor\n",
       "                    scalar_tensor_default_5: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_56: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_119, scalar_tensor_default_5);  add_119 = scalar_tensor_default_5 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_64: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_56, 32, p_vae_decoder_mid_block_resnets_1_norm1_weight, p_vae_decoder_mid_block_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_mid_block_resnets_1_norm1_weight = p_vae_decoder_mid_block_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_70: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_64);  group_norm_64 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_102: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_70, p_vae_decoder_mid_block_resnets_1_conv1_weight, p_vae_decoder_mid_block_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_70 = p_vae_decoder_mid_block_resnets_1_conv1_weight = p_vae_decoder_mid_block_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:351 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_65: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_102, 32, p_vae_decoder_mid_block_resnets_1_norm2_weight, p_vae_decoder_mid_block_resnets_1_norm2_bias, 1e-06);  conv2d_102 = p_vae_decoder_mid_block_resnets_1_norm2_weight = p_vae_decoder_mid_block_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_71: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_65);  group_norm_65 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_113: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_71);  silu_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_103: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_113, p_vae_decoder_mid_block_resnets_1_conv2_weight, p_vae_decoder_mid_block_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_113 = p_vae_decoder_mid_block_resnets_1_conv2_weight = p_vae_decoder_mid_block_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_120: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_56, conv2d_103);  div_56 = conv2d_103 = None\n",
       "                    scalar_tensor_default_6: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    div_57: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_120, scalar_tensor_default_6);  add_120 = scalar_tensor_default_6 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_66: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_57, 32, p_vae_decoder_up_blocks_0_resnets_0_norm1_weight, p_vae_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_0_norm1_weight = p_vae_decoder_up_blocks_0_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_72: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_66);  group_norm_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_104: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_72, p_vae_decoder_up_blocks_0_resnets_0_conv1_weight, p_vae_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_72 = p_vae_decoder_up_blocks_0_resnets_0_conv1_weight = p_vae_decoder_up_blocks_0_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_67: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_104, 32, p_vae_decoder_up_blocks_0_resnets_0_norm2_weight, p_vae_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06);  conv2d_104 = p_vae_decoder_up_blocks_0_resnets_0_norm2_weight = p_vae_decoder_up_blocks_0_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_73: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_67);  group_norm_67 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_114: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_73);  silu_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_105: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_114, p_vae_decoder_up_blocks_0_resnets_0_conv2_weight, p_vae_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_114 = p_vae_decoder_up_blocks_0_resnets_0_conv2_weight = p_vae_decoder_up_blocks_0_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_121: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_57, conv2d_105);  div_57 = conv2d_105 = None\n",
       "                    div_58: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_121, 1.0);  add_121 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_68: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_58, 32, p_vae_decoder_up_blocks_0_resnets_1_norm1_weight, p_vae_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_1_norm1_weight = p_vae_decoder_up_blocks_0_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_74: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_68);  group_norm_68 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_106: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_74, p_vae_decoder_up_blocks_0_resnets_1_conv1_weight, p_vae_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_74 = p_vae_decoder_up_blocks_0_resnets_1_conv1_weight = p_vae_decoder_up_blocks_0_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_69: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_106, 32, p_vae_decoder_up_blocks_0_resnets_1_norm2_weight, p_vae_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06);  conv2d_106 = p_vae_decoder_up_blocks_0_resnets_1_norm2_weight = p_vae_decoder_up_blocks_0_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_75: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_69);  group_norm_69 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_115: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_75);  silu_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_107: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_115, p_vae_decoder_up_blocks_0_resnets_1_conv2_weight, p_vae_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_115 = p_vae_decoder_up_blocks_0_resnets_1_conv2_weight = p_vae_decoder_up_blocks_0_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_122: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_58, conv2d_107);  div_58 = conv2d_107 = None\n",
       "                    div_59: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_122, 1.0);  add_122 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_70: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(div_59, 32, p_vae_decoder_up_blocks_0_resnets_2_norm1_weight, p_vae_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_0_resnets_2_norm1_weight = p_vae_decoder_up_blocks_0_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_76: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_70);  group_norm_70 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_108: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(silu_76, p_vae_decoder_up_blocks_0_resnets_2_conv1_weight, p_vae_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_76 = p_vae_decoder_up_blocks_0_resnets_2_conv1_weight = p_vae_decoder_up_blocks_0_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_71: \"f32[8, 512, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_108, 32, p_vae_decoder_up_blocks_0_resnets_2_norm2_weight, p_vae_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06);  conv2d_108 = p_vae_decoder_up_blocks_0_resnets_2_norm2_weight = p_vae_decoder_up_blocks_0_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_77: \"f32[8, 512, 32, 32]\" = torch.ops.aten.silu.default(group_norm_71);  group_norm_71 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_116: \"f32[8, 512, 32, 32]\" = torch.ops.aten.clone.default(silu_77);  silu_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_109: \"f32[8, 512, 32, 32]\" = torch.ops.aten.conv2d.default(clone_116, p_vae_decoder_up_blocks_0_resnets_2_conv2_weight, p_vae_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_116 = p_vae_decoder_up_blocks_0_resnets_2_conv2_weight = p_vae_decoder_up_blocks_0_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_123: \"f32[8, 512, 32, 32]\" = torch.ops.aten.add.Tensor(div_59, conv2d_109);  div_59 = conv2d_109 = None\n",
       "                    div_60: \"f32[8, 512, 32, 32]\" = torch.ops.aten.div.Tensor(add_123, 1.0);  add_123 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_3: \"f32[8, 512, 64, 64]\" = torch.ops.aten.upsample_nearest2d.vec(div_60, None, [2.0, 2.0]);  div_60 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_110: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_3, p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_3 = p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_72: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_110, 32, p_vae_decoder_up_blocks_1_resnets_0_norm1_weight, p_vae_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_0_norm1_weight = p_vae_decoder_up_blocks_1_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_78: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_72);  group_norm_72 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_111: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_78, p_vae_decoder_up_blocks_1_resnets_0_conv1_weight, p_vae_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_78 = p_vae_decoder_up_blocks_1_resnets_0_conv1_weight = p_vae_decoder_up_blocks_1_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_73: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_111, 32, p_vae_decoder_up_blocks_1_resnets_0_norm2_weight, p_vae_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06);  conv2d_111 = p_vae_decoder_up_blocks_1_resnets_0_norm2_weight = p_vae_decoder_up_blocks_1_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_79: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_73);  group_norm_73 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_117: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_79);  silu_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_112: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_117, p_vae_decoder_up_blocks_1_resnets_0_conv2_weight, p_vae_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_117 = p_vae_decoder_up_blocks_1_resnets_0_conv2_weight = p_vae_decoder_up_blocks_1_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_124: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(conv2d_110, conv2d_112);  conv2d_110 = conv2d_112 = None\n",
       "                    div_61: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_124, 1.0);  add_124 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_74: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(div_61, 32, p_vae_decoder_up_blocks_1_resnets_1_norm1_weight, p_vae_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_1_norm1_weight = p_vae_decoder_up_blocks_1_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_80: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_74);  group_norm_74 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_113: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_80, p_vae_decoder_up_blocks_1_resnets_1_conv1_weight, p_vae_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_80 = p_vae_decoder_up_blocks_1_resnets_1_conv1_weight = p_vae_decoder_up_blocks_1_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_75: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_113, 32, p_vae_decoder_up_blocks_1_resnets_1_norm2_weight, p_vae_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06);  conv2d_113 = p_vae_decoder_up_blocks_1_resnets_1_norm2_weight = p_vae_decoder_up_blocks_1_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_81: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_75);  group_norm_75 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_118: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_81);  silu_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_114: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_118, p_vae_decoder_up_blocks_1_resnets_1_conv2_weight, p_vae_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_118 = p_vae_decoder_up_blocks_1_resnets_1_conv2_weight = p_vae_decoder_up_blocks_1_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_125: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(div_61, conv2d_114);  div_61 = conv2d_114 = None\n",
       "                    div_62: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_125, 1.0);  add_125 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_76: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(div_62, 32, p_vae_decoder_up_blocks_1_resnets_2_norm1_weight, p_vae_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_1_resnets_2_norm1_weight = p_vae_decoder_up_blocks_1_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_82: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_76);  group_norm_76 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_115: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(silu_82, p_vae_decoder_up_blocks_1_resnets_2_conv1_weight, p_vae_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_82 = p_vae_decoder_up_blocks_1_resnets_2_conv1_weight = p_vae_decoder_up_blocks_1_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_77: \"f32[8, 512, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_115, 32, p_vae_decoder_up_blocks_1_resnets_2_norm2_weight, p_vae_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06);  conv2d_115 = p_vae_decoder_up_blocks_1_resnets_2_norm2_weight = p_vae_decoder_up_blocks_1_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_83: \"f32[8, 512, 64, 64]\" = torch.ops.aten.silu.default(group_norm_77);  group_norm_77 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_119: \"f32[8, 512, 64, 64]\" = torch.ops.aten.clone.default(silu_83);  silu_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_116: \"f32[8, 512, 64, 64]\" = torch.ops.aten.conv2d.default(clone_119, p_vae_decoder_up_blocks_1_resnets_2_conv2_weight, p_vae_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_119 = p_vae_decoder_up_blocks_1_resnets_2_conv2_weight = p_vae_decoder_up_blocks_1_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_126: \"f32[8, 512, 64, 64]\" = torch.ops.aten.add.Tensor(div_62, conv2d_116);  div_62 = conv2d_116 = None\n",
       "                    div_63: \"f32[8, 512, 64, 64]\" = torch.ops.aten.div.Tensor(add_126, 1.0);  add_126 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_4: \"f32[8, 512, 128, 128]\" = torch.ops.aten.upsample_nearest2d.vec(div_63, None, [2.0, 2.0]);  div_63 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_117: \"f32[8, 512, 128, 128]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_4, p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_4 = p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_78: \"f32[8, 512, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_117, 32, p_vae_decoder_up_blocks_2_resnets_0_norm1_weight, p_vae_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_0_norm1_weight = p_vae_decoder_up_blocks_2_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_84: \"f32[8, 512, 128, 128]\" = torch.ops.aten.silu.default(group_norm_78);  group_norm_78 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_118: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_84, p_vae_decoder_up_blocks_2_resnets_0_conv1_weight, p_vae_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_84 = p_vae_decoder_up_blocks_2_resnets_0_conv1_weight = p_vae_decoder_up_blocks_2_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_79: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_118, 32, p_vae_decoder_up_blocks_2_resnets_0_norm2_weight, p_vae_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06);  conv2d_118 = p_vae_decoder_up_blocks_2_resnets_0_norm2_weight = p_vae_decoder_up_blocks_2_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_85: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_79);  group_norm_79 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_120: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_85);  silu_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_119: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_120, p_vae_decoder_up_blocks_2_resnets_0_conv2_weight, p_vae_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_120 = p_vae_decoder_up_blocks_2_resnets_0_conv2_weight = p_vae_decoder_up_blocks_2_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_120: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(conv2d_117, p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias);  conv2d_117 = p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight = p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_127: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(conv2d_120, conv2d_119);  conv2d_120 = conv2d_119 = None\n",
       "                    div_64: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_127, 1.0);  add_127 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_80: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(div_64, 32, p_vae_decoder_up_blocks_2_resnets_1_norm1_weight, p_vae_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_1_norm1_weight = p_vae_decoder_up_blocks_2_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_86: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_80);  group_norm_80 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_121: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_86, p_vae_decoder_up_blocks_2_resnets_1_conv1_weight, p_vae_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_86 = p_vae_decoder_up_blocks_2_resnets_1_conv1_weight = p_vae_decoder_up_blocks_2_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_81: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_121, 32, p_vae_decoder_up_blocks_2_resnets_1_norm2_weight, p_vae_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06);  conv2d_121 = p_vae_decoder_up_blocks_2_resnets_1_norm2_weight = p_vae_decoder_up_blocks_2_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_87: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_81);  group_norm_81 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_121: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_87);  silu_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_122: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_121, p_vae_decoder_up_blocks_2_resnets_1_conv2_weight, p_vae_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_121 = p_vae_decoder_up_blocks_2_resnets_1_conv2_weight = p_vae_decoder_up_blocks_2_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_128: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(div_64, conv2d_122);  div_64 = conv2d_122 = None\n",
       "                    div_65: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_128, 1.0);  add_128 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_82: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(div_65, 32, p_vae_decoder_up_blocks_2_resnets_2_norm1_weight, p_vae_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_2_resnets_2_norm1_weight = p_vae_decoder_up_blocks_2_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_88: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_82);  group_norm_82 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_123: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(silu_88, p_vae_decoder_up_blocks_2_resnets_2_conv1_weight, p_vae_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_88 = p_vae_decoder_up_blocks_2_resnets_2_conv1_weight = p_vae_decoder_up_blocks_2_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_83: \"f32[8, 256, 128, 128]\" = torch.ops.aten.group_norm.default(conv2d_123, 32, p_vae_decoder_up_blocks_2_resnets_2_norm2_weight, p_vae_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06);  conv2d_123 = p_vae_decoder_up_blocks_2_resnets_2_norm2_weight = p_vae_decoder_up_blocks_2_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_89: \"f32[8, 256, 128, 128]\" = torch.ops.aten.silu.default(group_norm_83);  group_norm_83 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_122: \"f32[8, 256, 128, 128]\" = torch.ops.aten.clone.default(silu_89);  silu_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_124: \"f32[8, 256, 128, 128]\" = torch.ops.aten.conv2d.default(clone_122, p_vae_decoder_up_blocks_2_resnets_2_conv2_weight, p_vae_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_122 = p_vae_decoder_up_blocks_2_resnets_2_conv2_weight = p_vae_decoder_up_blocks_2_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_129: \"f32[8, 256, 128, 128]\" = torch.ops.aten.add.Tensor(div_65, conv2d_124);  div_65 = conv2d_124 = None\n",
       "                    div_66: \"f32[8, 256, 128, 128]\" = torch.ops.aten.div.Tensor(add_129, 1.0);  add_129 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:169 in forward, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
       "                    upsample_nearest2d_5: \"f32[8, 256, 256, 256]\" = torch.ops.aten.upsample_nearest2d.vec(div_66, None, [2.0, 2.0]);  div_66 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:180 in forward, code: hidden_states = self.conv(hidden_states)\n",
       "                    conv2d_125: \"f32[8, 256, 256, 256]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_5, p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight, p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]);  upsample_nearest2d_5 = p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight = p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_84: \"f32[8, 256, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_125, 32, p_vae_decoder_up_blocks_3_resnets_0_norm1_weight, p_vae_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_0_norm1_weight = p_vae_decoder_up_blocks_3_resnets_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_90: \"f32[8, 256, 256, 256]\" = torch.ops.aten.silu.default(group_norm_84);  group_norm_84 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_126: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_90, p_vae_decoder_up_blocks_3_resnets_0_conv1_weight, p_vae_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]);  silu_90 = p_vae_decoder_up_blocks_3_resnets_0_conv1_weight = p_vae_decoder_up_blocks_3_resnets_0_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_85: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_126, 32, p_vae_decoder_up_blocks_3_resnets_0_norm2_weight, p_vae_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06);  conv2d_126 = p_vae_decoder_up_blocks_3_resnets_0_norm2_weight = p_vae_decoder_up_blocks_3_resnets_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_91: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_85);  group_norm_85 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_123: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_91);  silu_91 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_127: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_123, p_vae_decoder_up_blocks_3_resnets_0_conv2_weight, p_vae_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]);  clone_123 = p_vae_decoder_up_blocks_3_resnets_0_conv2_weight = p_vae_decoder_up_blocks_3_resnets_0_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369 in forward, code: input_tensor = self.conv_shortcut(input_tensor)\n",
       "                    conv2d_128: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(conv2d_125, p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias);  conv2d_125 = p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight = p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_130: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(conv2d_128, conv2d_127);  conv2d_128 = conv2d_127 = None\n",
       "                    div_67: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_130, 1.0);  add_130 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_86: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_67, 32, p_vae_decoder_up_blocks_3_resnets_1_norm1_weight, p_vae_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_1_norm1_weight = p_vae_decoder_up_blocks_3_resnets_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_92: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_86);  group_norm_86 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_129: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_92, p_vae_decoder_up_blocks_3_resnets_1_conv1_weight, p_vae_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]);  silu_92 = p_vae_decoder_up_blocks_3_resnets_1_conv1_weight = p_vae_decoder_up_blocks_3_resnets_1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_87: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_129, 32, p_vae_decoder_up_blocks_3_resnets_1_norm2_weight, p_vae_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06);  conv2d_129 = p_vae_decoder_up_blocks_3_resnets_1_norm2_weight = p_vae_decoder_up_blocks_3_resnets_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_93: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_87);  group_norm_87 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_124: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_93);  silu_93 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_130: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_124, p_vae_decoder_up_blocks_3_resnets_1_conv2_weight, p_vae_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]);  clone_124 = p_vae_decoder_up_blocks_3_resnets_1_conv2_weight = p_vae_decoder_up_blocks_3_resnets_1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_131: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(div_67, conv2d_130);  div_67 = conv2d_130 = None\n",
       "                    div_68: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_131, 1.0);  add_131 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:327 in forward, code: hidden_states = self.norm1(hidden_states)\n",
       "                    group_norm_88: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_68, 32, p_vae_decoder_up_blocks_3_resnets_2_norm1_weight, p_vae_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06);  p_vae_decoder_up_blocks_3_resnets_2_norm1_weight = p_vae_decoder_up_blocks_3_resnets_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:328 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_94: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_88);  group_norm_88 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:341 in forward, code: hidden_states = self.conv1(hidden_states)\n",
       "                    conv2d_131: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(silu_94, p_vae_decoder_up_blocks_3_resnets_2_conv1_weight, p_vae_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]);  silu_94 = p_vae_decoder_up_blocks_3_resnets_2_conv1_weight = p_vae_decoder_up_blocks_3_resnets_2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:361 in forward, code: hidden_states = self.norm2(hidden_states)\n",
       "                    group_norm_89: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(conv2d_131, 32, p_vae_decoder_up_blocks_3_resnets_2_norm2_weight, p_vae_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06);  conv2d_131 = p_vae_decoder_up_blocks_3_resnets_2_norm2_weight = p_vae_decoder_up_blocks_3_resnets_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:363 in forward, code: hidden_states = self.nonlinearity(hidden_states)\n",
       "                    silu_95: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_89);  group_norm_89 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:365 in forward, code: hidden_states = self.dropout(hidden_states)\n",
       "                    clone_125: \"f32[8, 128, 256, 256]\" = torch.ops.aten.clone.default(silu_95);  silu_95 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:366 in forward, code: hidden_states = self.conv2(hidden_states)\n",
       "                    conv2d_132: \"f32[8, 128, 256, 256]\" = torch.ops.aten.conv2d.default(clone_125, p_vae_decoder_up_blocks_3_resnets_2_conv2_weight, p_vae_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]);  clone_125 = p_vae_decoder_up_blocks_3_resnets_2_conv2_weight = p_vae_decoder_up_blocks_3_resnets_2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/resnet.py:371 in forward, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
       "                    add_132: \"f32[8, 128, 256, 256]\" = torch.ops.aten.add.Tensor(div_68, conv2d_132);  div_68 = conv2d_132 = None\n",
       "                    div_69: \"f32[8, 128, 256, 256]\" = torch.ops.aten.div.Tensor(add_132, 1.0);  add_132 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:341 in forward, code: sample = self.conv_norm_out(sample)\n",
       "                    group_norm_90: \"f32[8, 128, 256, 256]\" = torch.ops.aten.group_norm.default(div_69, 32, p_vae_decoder_conv_norm_out_weight, p_vae_decoder_conv_norm_out_bias, 1e-06);  div_69 = p_vae_decoder_conv_norm_out_weight = p_vae_decoder_conv_norm_out_bias = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:344 in forward, code: sample = self.conv_act(sample)\n",
       "                    silu_96: \"f32[8, 128, 256, 256]\" = torch.ops.aten.silu.default(group_norm_90);  group_norm_90 = None\n",
       "            \n",
       "                     # File: /home/lukadarsalia/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:345 in forward, code: sample = self.conv_out(sample)\n",
       "                    conv2d_133: \"f32[8, 3, 256, 256]\" = torch.ops.aten.conv2d.default(silu_96, p_vae_decoder_conv_out_weight, p_vae_decoder_conv_out_bias, [1, 1], [1, 1]);  silu_96 = p_vae_decoder_conv_out_weight = p_vae_decoder_conv_out_bias = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:50 in decode_latents, code: image = (image / 2 + 0.5).clamp(0, 1)\n",
       "                    scalar_tensor_default_7: \"f32[]\" = torch.ops.aten.scalar_tensor.default(2, dtype = torch.float32)\n",
       "                    div_70: \"f32[8, 3, 256, 256]\" = torch.ops.aten.div.Tensor(conv2d_133, scalar_tensor_default_7);  conv2d_133 = scalar_tensor_default_7 = None\n",
       "                    add_133: \"f32[8, 3, 256, 256]\" = torch.ops.aten.add.Tensor(div_70, 0.5);  div_70 = None\n",
       "                    scalar_tensor_default_8: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
       "                    scalar_tensor_default_9: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    clamp: \"f32[8, 3, 256, 256]\" = torch.ops.aten.clamp.Tensor(add_133, scalar_tensor_default_8, scalar_tensor_default_9);  add_133 = scalar_tensor_default_8 = scalar_tensor_default_9 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:51 in decode_latents, code: image = image.detach().cpu().permute(0, 2, 3, 1).float().numpy()\n",
       "                    detach: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(clamp);  clamp = None\n",
       "                    detach_1: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(detach);  detach = None\n",
       "                    detach_2: \"f32[8, 3, 256, 256]\" = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
       "                    _to_copy_4: \"f32[8, 3, 256, 256]\" = torch.ops.aten._to_copy.default(detach_2, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  detach_2 = None\n",
       "                    permute_33: \"f32[8, 256, 256, 3]\" = torch.ops.aten.permute.default(_to_copy_4, [0, 2, 3, 1]);  _to_copy_4 = None\n",
       "                    view_183: \"f32[8, 256, 256, 3]\" = torch.ops.aten.view.default(permute_33, [8, 256, 256, 3]);  permute_33 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:52 in decode_latents, code: image = (image * 255).round().astype(\"uint8\")\n",
       "                    clone_126: \"f32[]\" = torch.ops.aten.clone.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n",
       "                    multiply: \"f32[8, 256, 256, 3]\" = torch.ops.aten.multiply.Tensor(view_183, clone_126);  view_183 = clone_126 = None\n",
       "                    round_1: \"f32[8, 256, 256, 3]\" = torch.ops.aten.round.decimals(multiply, decimals = 0);  multiply = None\n",
       "                    _to_copy_5: \"u8[8, 256, 256, 3]\" = torch.ops.aten._to_copy.default(round_1, dtype = torch.uint8);  round_1 = None\n",
       "            \n",
       "                     # File: /tmp/ipykernel_74925/3616881455.py:53 in decode_latents, code: image = image[...,::-1] # RGB to BGR\n",
       "                    flip: \"u8[8, 256, 256, 3]\" = torch.ops.aten.flip.default(_to_copy_5, [1]);  _to_copy_5 = None\n",
       "                    alias: \"u8[8, 256, 256, 3]\" = torch.ops.aten.alias.default(flip);  flip = None\n",
       "                    return (alias,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_whisper_encoder_embed_positions_weight: PARAMETER target='whisper_encoder.embed_positions.weight'\n",
       "            p_whisper_encoder_conv1_weight: PARAMETER target='whisper_encoder.conv1.weight'\n",
       "            p_whisper_encoder_conv1_bias: PARAMETER target='whisper_encoder.conv1.bias'\n",
       "            p_whisper_encoder_conv2_weight: PARAMETER target='whisper_encoder.conv2.weight'\n",
       "            p_whisper_encoder_conv2_bias: PARAMETER target='whisper_encoder.conv2.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.0.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.0.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_0_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.0.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_0_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.0.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_0_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.0.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_0_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.0.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_0_fc1_weight: PARAMETER target='whisper_encoder.layers.0.fc1.weight'\n",
       "            p_whisper_encoder_layers_0_fc1_bias: PARAMETER target='whisper_encoder.layers.0.fc1.bias'\n",
       "            p_whisper_encoder_layers_0_fc2_weight: PARAMETER target='whisper_encoder.layers.0.fc2.weight'\n",
       "            p_whisper_encoder_layers_0_fc2_bias: PARAMETER target='whisper_encoder.layers.0.fc2.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.1.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.1.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_1_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.1.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_1_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.1.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_1_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.1.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_1_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.1.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_1_fc1_weight: PARAMETER target='whisper_encoder.layers.1.fc1.weight'\n",
       "            p_whisper_encoder_layers_1_fc1_bias: PARAMETER target='whisper_encoder.layers.1.fc1.bias'\n",
       "            p_whisper_encoder_layers_1_fc2_weight: PARAMETER target='whisper_encoder.layers.1.fc2.weight'\n",
       "            p_whisper_encoder_layers_1_fc2_bias: PARAMETER target='whisper_encoder.layers.1.fc2.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.2.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.2.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_2_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.2.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_2_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.2.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_2_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.2.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_2_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.2.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_2_fc1_weight: PARAMETER target='whisper_encoder.layers.2.fc1.weight'\n",
       "            p_whisper_encoder_layers_2_fc1_bias: PARAMETER target='whisper_encoder.layers.2.fc1.bias'\n",
       "            p_whisper_encoder_layers_2_fc2_weight: PARAMETER target='whisper_encoder.layers.2.fc2.weight'\n",
       "            p_whisper_encoder_layers_2_fc2_bias: PARAMETER target='whisper_encoder.layers.2.fc2.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_layer_norm_weight: PARAMETER target='whisper_encoder.layers.3.self_attn_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_layer_norm_bias: PARAMETER target='whisper_encoder.layers.3.self_attn_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_q_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.q_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_q_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.q_proj.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_k_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.k_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_v_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.v_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_v_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.v_proj.bias'\n",
       "            p_whisper_encoder_layers_3_self_attn_out_proj_weight: PARAMETER target='whisper_encoder.layers.3.self_attn.out_proj.weight'\n",
       "            p_whisper_encoder_layers_3_self_attn_out_proj_bias: PARAMETER target='whisper_encoder.layers.3.self_attn.out_proj.bias'\n",
       "            p_whisper_encoder_layers_3_final_layer_norm_weight: PARAMETER target='whisper_encoder.layers.3.final_layer_norm.weight'\n",
       "            p_whisper_encoder_layers_3_final_layer_norm_bias: PARAMETER target='whisper_encoder.layers.3.final_layer_norm.bias'\n",
       "            p_whisper_encoder_layers_3_fc1_weight: PARAMETER target='whisper_encoder.layers.3.fc1.weight'\n",
       "            p_whisper_encoder_layers_3_fc1_bias: PARAMETER target='whisper_encoder.layers.3.fc1.bias'\n",
       "            p_whisper_encoder_layers_3_fc2_weight: PARAMETER target='whisper_encoder.layers.3.fc2.weight'\n",
       "            p_whisper_encoder_layers_3_fc2_bias: PARAMETER target='whisper_encoder.layers.3.fc2.bias'\n",
       "            p_whisper_encoder_layer_norm_weight: PARAMETER target='whisper_encoder.layer_norm.weight'\n",
       "            p_whisper_encoder_layer_norm_bias: PARAMETER target='whisper_encoder.layer_norm.bias'\n",
       "            p_unet_time_embedding_linear_1_weight: PARAMETER target='unet.time_embedding.linear_1.weight'\n",
       "            p_unet_time_embedding_linear_1_bias: PARAMETER target='unet.time_embedding.linear_1.bias'\n",
       "            p_unet_time_embedding_linear_2_weight: PARAMETER target='unet.time_embedding.linear_2.weight'\n",
       "            p_unet_time_embedding_linear_2_bias: PARAMETER target='unet.time_embedding.linear_2.bias'\n",
       "            p_unet_conv_in_weight: PARAMETER target='unet.conv_in.weight'\n",
       "            p_unet_conv_in_bias: PARAMETER target='unet.conv_in.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.0.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.0.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.0.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.0.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.0.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.0.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.0.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.0.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.0.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_0_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.0.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.0.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.0.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.0.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.0.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.0.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.0.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.0.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.0.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_0_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.0.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_0_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.0.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.0.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.0.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.0.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.0.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.0.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_0_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.0.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.0.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.0.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.0.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.0.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.0.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_0_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.0.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_0_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.0.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_0_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.0.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.1.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.1.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.1.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.1.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.1.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.1.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_1_resnets_0_conv_shortcut_weight: PARAMETER target='unet.down_blocks.1.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_down_blocks_1_resnets_0_conv_shortcut_bias: PARAMETER target='unet.down_blocks.1.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.1.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.1.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.1.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.1.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.1.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.1.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.1.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.1.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_1_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.1.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_1_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.1.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.1.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.1.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.1.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.1.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.1.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_1_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.1.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.1.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.1.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.1.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.1.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.1.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_1_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.1.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_1_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.1.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_1_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.1.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.2.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.2.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.2.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.2.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.2.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.2.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='unet.down_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_down_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='unet.down_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.2.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.2.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.2.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.2.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.2.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.2.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.2.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.2.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_2_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.2.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_2_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.2.resnets.1.conv2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_norm_weight: PARAMETER target='unet.down_blocks.2.attentions.0.norm.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_norm_bias: PARAMETER target='unet.down_blocks.2.attentions.0.norm.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_in_weight: PARAMETER target='unet.down_blocks.2.attentions.0.proj_in.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_in_bias: PARAMETER target='unet.down_blocks.2.attentions.0.proj_in.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_out_weight: PARAMETER target='unet.down_blocks.2.attentions.0.proj_out.weight'\n",
       "            p_unet_down_blocks_2_attentions_0_proj_out_bias: PARAMETER target='unet.down_blocks.2.attentions.0.proj_out.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_norm_weight: PARAMETER target='unet.down_blocks.2.attentions.1.norm.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_norm_bias: PARAMETER target='unet.down_blocks.2.attentions.1.norm.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_in_weight: PARAMETER target='unet.down_blocks.2.attentions.1.proj_in.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_in_bias: PARAMETER target='unet.down_blocks.2.attentions.1.proj_in.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_out_weight: PARAMETER target='unet.down_blocks.2.attentions.1.proj_out.weight'\n",
       "            p_unet_down_blocks_2_attentions_1_proj_out_bias: PARAMETER target='unet.down_blocks.2.attentions.1.proj_out.bias'\n",
       "            p_unet_down_blocks_2_downsamplers_0_conv_weight: PARAMETER target='unet.down_blocks.2.downsamplers.0.conv.weight'\n",
       "            p_unet_down_blocks_2_downsamplers_0_conv_bias: PARAMETER target='unet.down_blocks.2.downsamplers.0.conv.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_norm1_weight: PARAMETER target='unet.down_blocks.3.resnets.0.norm1.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_norm1_bias: PARAMETER target='unet.down_blocks.3.resnets.0.norm1.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_conv1_weight: PARAMETER target='unet.down_blocks.3.resnets.0.conv1.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_conv1_bias: PARAMETER target='unet.down_blocks.3.resnets.0.conv1.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_time_emb_proj_weight: PARAMETER target='unet.down_blocks.3.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_time_emb_proj_bias: PARAMETER target='unet.down_blocks.3.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_norm2_weight: PARAMETER target='unet.down_blocks.3.resnets.0.norm2.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_norm2_bias: PARAMETER target='unet.down_blocks.3.resnets.0.norm2.bias'\n",
       "            p_unet_down_blocks_3_resnets_0_conv2_weight: PARAMETER target='unet.down_blocks.3.resnets.0.conv2.weight'\n",
       "            p_unet_down_blocks_3_resnets_0_conv2_bias: PARAMETER target='unet.down_blocks.3.resnets.0.conv2.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_norm1_weight: PARAMETER target='unet.down_blocks.3.resnets.1.norm1.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_norm1_bias: PARAMETER target='unet.down_blocks.3.resnets.1.norm1.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_conv1_weight: PARAMETER target='unet.down_blocks.3.resnets.1.conv1.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_conv1_bias: PARAMETER target='unet.down_blocks.3.resnets.1.conv1.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_time_emb_proj_weight: PARAMETER target='unet.down_blocks.3.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_time_emb_proj_bias: PARAMETER target='unet.down_blocks.3.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_norm2_weight: PARAMETER target='unet.down_blocks.3.resnets.1.norm2.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_norm2_bias: PARAMETER target='unet.down_blocks.3.resnets.1.norm2.bias'\n",
       "            p_unet_down_blocks_3_resnets_1_conv2_weight: PARAMETER target='unet.down_blocks.3.resnets.1.conv2.weight'\n",
       "            p_unet_down_blocks_3_resnets_1_conv2_bias: PARAMETER target='unet.down_blocks.3.resnets.1.conv2.bias'\n",
       "            p_unet_mid_block_resnets_0_norm1_weight: PARAMETER target='unet.mid_block.resnets.0.norm1.weight'\n",
       "            p_unet_mid_block_resnets_0_norm1_bias: PARAMETER target='unet.mid_block.resnets.0.norm1.bias'\n",
       "            p_unet_mid_block_resnets_0_conv1_weight: PARAMETER target='unet.mid_block.resnets.0.conv1.weight'\n",
       "            p_unet_mid_block_resnets_0_conv1_bias: PARAMETER target='unet.mid_block.resnets.0.conv1.bias'\n",
       "            p_unet_mid_block_resnets_0_time_emb_proj_weight: PARAMETER target='unet.mid_block.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_mid_block_resnets_0_time_emb_proj_bias: PARAMETER target='unet.mid_block.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_mid_block_resnets_0_norm2_weight: PARAMETER target='unet.mid_block.resnets.0.norm2.weight'\n",
       "            p_unet_mid_block_resnets_0_norm2_bias: PARAMETER target='unet.mid_block.resnets.0.norm2.bias'\n",
       "            p_unet_mid_block_resnets_0_conv2_weight: PARAMETER target='unet.mid_block.resnets.0.conv2.weight'\n",
       "            p_unet_mid_block_resnets_0_conv2_bias: PARAMETER target='unet.mid_block.resnets.0.conv2.bias'\n",
       "            p_unet_mid_block_resnets_1_norm1_weight: PARAMETER target='unet.mid_block.resnets.1.norm1.weight'\n",
       "            p_unet_mid_block_resnets_1_norm1_bias: PARAMETER target='unet.mid_block.resnets.1.norm1.bias'\n",
       "            p_unet_mid_block_resnets_1_conv1_weight: PARAMETER target='unet.mid_block.resnets.1.conv1.weight'\n",
       "            p_unet_mid_block_resnets_1_conv1_bias: PARAMETER target='unet.mid_block.resnets.1.conv1.bias'\n",
       "            p_unet_mid_block_resnets_1_time_emb_proj_weight: PARAMETER target='unet.mid_block.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_mid_block_resnets_1_time_emb_proj_bias: PARAMETER target='unet.mid_block.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_mid_block_resnets_1_norm2_weight: PARAMETER target='unet.mid_block.resnets.1.norm2.weight'\n",
       "            p_unet_mid_block_resnets_1_norm2_bias: PARAMETER target='unet.mid_block.resnets.1.norm2.bias'\n",
       "            p_unet_mid_block_resnets_1_conv2_weight: PARAMETER target='unet.mid_block.resnets.1.conv2.weight'\n",
       "            p_unet_mid_block_resnets_1_conv2_bias: PARAMETER target='unet.mid_block.resnets.1.conv2.bias'\n",
       "            p_unet_mid_block_attentions_0_norm_weight: PARAMETER target='unet.mid_block.attentions.0.norm.weight'\n",
       "            p_unet_mid_block_attentions_0_norm_bias: PARAMETER target='unet.mid_block.attentions.0.norm.bias'\n",
       "            p_unet_mid_block_attentions_0_proj_in_weight: PARAMETER target='unet.mid_block.attentions.0.proj_in.weight'\n",
       "            p_unet_mid_block_attentions_0_proj_in_bias: PARAMETER target='unet.mid_block.attentions.0.proj_in.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_mid_block_attentions_0_proj_out_weight: PARAMETER target='unet.mid_block.attentions.0.proj_out.weight'\n",
       "            p_unet_mid_block_attentions_0_proj_out_bias: PARAMETER target='unet.mid_block.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.0.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.0.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.0.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.0.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.0.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.0.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_0_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.0.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_0_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.0.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_0_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.0.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_0_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.0.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.1.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.1.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.1.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.1.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.1.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.1.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_1_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.1.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_1_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.1.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.1.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.1.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.1.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.1.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.1.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_1_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.1.attentions.2.proj_out.bias'\n",
       "            p_unet_up_blocks_1_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.1.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_1_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.1.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.2.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.2.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.2.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.2.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.2.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.2.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_2_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.2.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_2_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.2.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.2.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.2.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.2.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.2.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.2.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_2_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.2.attentions.2.proj_out.bias'\n",
       "            p_unet_up_blocks_2_upsamplers_0_conv_weight: PARAMETER target='unet.up_blocks.2.upsamplers.0.conv.weight'\n",
       "            p_unet_up_blocks_2_upsamplers_0_conv_bias: PARAMETER target='unet.up_blocks.2.upsamplers.0.conv.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.0.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.0.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_0_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.0.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_0_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.0.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.1.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.1.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.1.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.1.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.1.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.1.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_1_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.1.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_1_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.1.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_norm1_weight: PARAMETER target='unet.up_blocks.3.resnets.2.norm1.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_norm1_bias: PARAMETER target='unet.up_blocks.3.resnets.2.norm1.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv1_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv1.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv1_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv1.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_time_emb_proj_weight: PARAMETER target='unet.up_blocks.3.resnets.2.time_emb_proj.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_time_emb_proj_bias: PARAMETER target='unet.up_blocks.3.resnets.2.time_emb_proj.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_norm2_weight: PARAMETER target='unet.up_blocks.3.resnets.2.norm2.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_norm2_bias: PARAMETER target='unet.up_blocks.3.resnets.2.norm2.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv2_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv2.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv2_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv2.bias'\n",
       "            p_unet_up_blocks_3_resnets_2_conv_shortcut_weight: PARAMETER target='unet.up_blocks.3.resnets.2.conv_shortcut.weight'\n",
       "            p_unet_up_blocks_3_resnets_2_conv_shortcut_bias: PARAMETER target='unet.up_blocks.3.resnets.2.conv_shortcut.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.0.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.0.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.0.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.0.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.0.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_0_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.0.proj_out.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.1.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.1.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.1.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.1.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.1.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_1_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.1.proj_out.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_norm_weight: PARAMETER target='unet.up_blocks.3.attentions.2.norm.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_norm_bias: PARAMETER target='unet.up_blocks.3.attentions.2.norm.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_in_weight: PARAMETER target='unet.up_blocks.3.attentions.2.proj_in.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_in_bias: PARAMETER target='unet.up_blocks.3.attentions.2.proj_in.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias: PARAMETER target='unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_out_weight: PARAMETER target='unet.up_blocks.3.attentions.2.proj_out.weight'\n",
       "            p_unet_up_blocks_3_attentions_2_proj_out_bias: PARAMETER target='unet.up_blocks.3.attentions.2.proj_out.bias'\n",
       "            p_unet_conv_norm_out_weight: PARAMETER target='unet.conv_norm_out.weight'\n",
       "            p_unet_conv_norm_out_bias: PARAMETER target='unet.conv_norm_out.bias'\n",
       "            p_unet_conv_out_weight: PARAMETER target='unet.conv_out.weight'\n",
       "            p_unet_conv_out_bias: PARAMETER target='unet.conv_out.bias'\n",
       "            p_vae_post_quant_conv_weight: PARAMETER target='vae.post_quant_conv.weight'\n",
       "            p_vae_post_quant_conv_bias: PARAMETER target='vae.post_quant_conv.bias'\n",
       "            p_vae_decoder_conv_in_weight: PARAMETER target='vae.decoder.conv_in.weight'\n",
       "            p_vae_decoder_conv_in_bias: PARAMETER target='vae.decoder.conv_in.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm1_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm1_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv1_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv1_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm2_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_norm2_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv2_weight: PARAMETER target='vae.decoder.mid_block.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_0_conv2_bias: PARAMETER target='vae.decoder.mid_block.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm1_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm1_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv1_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv1_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm2_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_norm2_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv2_weight: PARAMETER target='vae.decoder.mid_block.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_mid_block_resnets_1_conv2_bias: PARAMETER target='vae.decoder.mid_block.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_group_norm_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.group_norm.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_group_norm_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.group_norm.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_q_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_q.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_q_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_q.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_k_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_k.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_k_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_k.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_v_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_v.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_v_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_v.bias'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_out_0_weight: PARAMETER target='vae.decoder.mid_block.attentions.0.to_out.0.weight'\n",
       "            p_vae_decoder_mid_block_attentions_0_to_out_0_bias: PARAMETER target='vae.decoder.mid_block.attentions.0.to_out.0.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_0_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.0.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_0_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.0.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_0_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.0.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_1_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.1.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_1_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.1.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_1_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.1.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_0_conv_shortcut_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_2_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.2.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_2_upsamplers_0_conv_weight: PARAMETER target='vae.decoder.up_blocks.2.upsamplers.0.conv.weight'\n",
       "            p_vae_decoder_up_blocks_2_upsamplers_0_conv_bias: PARAMETER target='vae.decoder.up_blocks.2.upsamplers.0.conv.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_0_conv_shortcut_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_1_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.1.conv2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv1_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv1.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv1_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv1.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_norm2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.norm2.bias'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv2_weight: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv2.weight'\n",
       "            p_vae_decoder_up_blocks_3_resnets_2_conv2_bias: PARAMETER target='vae.decoder.up_blocks.3.resnets.2.conv2.bias'\n",
       "            p_vae_decoder_conv_norm_out_weight: PARAMETER target='vae.decoder.conv_norm_out.weight'\n",
       "            p_vae_decoder_conv_norm_out_bias: PARAMETER target='vae.decoder.conv_norm_out.bias'\n",
       "            p_vae_decoder_conv_out_weight: PARAMETER target='vae.decoder.conv_out.weight'\n",
       "            p_vae_decoder_conv_out_bias: PARAMETER target='vae.decoder.conv_out.bias'\n",
       "            c_timesteps: CONSTANT_TENSOR target='timesteps'\n",
       "            c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n",
       "            b_pe_pe: BUFFER target='pe.pe' persistent=True\n",
       "            whisper_input_features_0: USER_INPUT\n",
       "            latent_inputs: USER_INPUT\n",
       "            frame_idx: USER_INPUT\n",
       "            librosa_length: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            alias: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a1164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "whisper_input_features_0 [1, 80, 3000] tensor(float)\n",
      "latent_inputs [8, 8, 32, 32] tensor(float)\n",
      "Outputs:\n",
      "alias [8, 256, 256, 3] tensor(uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-11-14 17:12:06.158786194 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-11-14 17:12:06.164165445 [W:onnxruntime:, session_state.cc:1316 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-11-14 17:12:06.164169425 [W:onnxruntime:, session_state.cc:1318 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load session\n",
    "session = ort.InferenceSession(\"musetalk_model.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "print(\"Inputs:\")\n",
    "for inp in session.get_inputs():\n",
    "    print(inp.name, inp.shape, inp.type)\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for out in session.get_outputs():\n",
    "    print(out.name, out.shape, out.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1235cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vae.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb58b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def run_torch_model(\n",
    "    model: MuseTalkGenerator,\n",
    "    whisper_feats: torch.Tensor,\n",
    "    latent_inputs: torch.Tensor,\n",
    "    frame_idx: int,\n",
    "    librosa_length: int,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # MuseTalkGenerator expects a *list* of tensors\n",
    "        whisper_list = [whisper_feats.to(device)]\n",
    "        latent_inputs = latent_inputs.to(device)\n",
    "\n",
    "        out_np = model(\n",
    "            whisper_input_features=whisper_list,\n",
    "            latent_inputs=latent_inputs,\n",
    "            frame_idx=frame_idx,\n",
    "            librosa_length=librosa_length,\n",
    "        )  # returns np.uint8 [B, H, W, 3]\n",
    "\n",
    "    assert isinstance(out_np, np.ndarray)\n",
    "    return out_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dfb8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def run_onnx_model(\n",
    "    onnx_path: str,\n",
    "    whisper_feats: torch.Tensor,\n",
    "    latent_inputs: torch.Tensor,\n",
    "    use_cuda: bool = True,\n",
    "):\n",
    "    providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if use_cuda else [\"CPUExecutionProvider\"]\n",
    "\n",
    "    sess = ort.InferenceSession(onnx_path, providers=providers)\n",
    "\n",
    "    # Names from your log:\n",
    "    #   whisper_input_features_0 [1, 80, 3000]\n",
    "    #   latent_inputs            [8, 8, 32, 32]\n",
    "    inputs = {\n",
    "        \"whisper_input_features_0\": whisper_feats.detach().cpu().numpy().astype(np.float32),\n",
    "        \"latent_inputs\": latent_inputs.detach().cpu().numpy().astype(np.float32),\n",
    "    }\n",
    "\n",
    "    (out_np,) = sess.run(None, inputs)  # alias: uint8 [B, H, W, 3]\n",
    "    return out_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f535e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_torch_onnx(\n",
    "    model: MuseTalkGenerator,\n",
    "    onnx_path: str,\n",
    "    frame_idx: int = 0,\n",
    "    librosa_length: int = 106_496,\n",
    "):\n",
    "    # Use exactly the same shapes as in your export\n",
    "    torch.manual_seed(0)\n",
    "    whisper_feats = torch.randn(1, 80, 3000)       # float32\n",
    "    latent_inputs = torch.randn(8, 8, 32, 32)      # float32\n",
    "\n",
    "    torch_out = run_torch_model(\n",
    "        model=model,\n",
    "        whisper_feats=whisper_feats,\n",
    "        latent_inputs=latent_inputs,\n",
    "        frame_idx=frame_idx,\n",
    "        librosa_length=librosa_length,\n",
    "    )\n",
    "\n",
    "    onnx_out = run_onnx_model(\n",
    "        onnx_path=onnx_path,\n",
    "        whisper_feats=whisper_feats,\n",
    "        latent_inputs=latent_inputs,\n",
    "        use_cuda=True,   # or False if you want pure CPU\n",
    "    )\n",
    "\n",
    "    print(\"Torch output shape:\", torch_out.shape, torch_out.dtype)\n",
    "    print(\"ONNX  output shape:\", onnx_out.shape, onnx_out.dtype)\n",
    "\n",
    "    # Convert to int16 to avoid uint8 wraparound on subtraction\n",
    "    diff = torch_out.astype(np.int16) - onnx_out.astype(np.int16)\n",
    "    abs_diff = np.abs(diff)\n",
    "\n",
    "    print(\"max |diff|:\", abs_diff.max())\n",
    "    print(\"mean |diff|:\", abs_diff.mean())\n",
    "    print(\"all equal:\", np.array_equal(torch_out, onnx_out))\n",
    "\n",
    "    return torch_out, onnx_out, abs_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163b6b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare_torch_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmusetalk_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mcompare_torch_onnx\u001b[0;34m(model, onnx_path, frame_idx, librosa_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m whisper_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m3000\u001b[39m)       \u001b[38;5;66;03m# float32\u001b[39;00m\n\u001b[1;32m     10\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)      \u001b[38;5;66;03m# float32\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m torch_out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_torch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhisper_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhisper_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrosa_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrosa_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m onnx_out \u001b[38;5;241m=\u001b[39m run_onnx_model(\n\u001b[1;32m     21\u001b[0m     onnx_path\u001b[38;5;241m=\u001b[39monnx_path,\n\u001b[1;32m     22\u001b[0m     whisper_feats\u001b[38;5;241m=\u001b[39mwhisper_feats,\n\u001b[1;32m     23\u001b[0m     latent_inputs\u001b[38;5;241m=\u001b[39mlatent_inputs,\n\u001b[1;32m     24\u001b[0m     use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,   \u001b[38;5;66;03m# or False if you want pure CPU\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_out\u001b[38;5;241m.\u001b[39mshape, torch_out\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mrun_torch_model\u001b[0;34m(model, whisper_feats, latent_inputs, frame_idx, librosa_length, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     whisper_list \u001b[38;5;241m=\u001b[39m [whisper_feats\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[1;32m     18\u001b[0m     latent_inputs \u001b[38;5;241m=\u001b[39m latent_inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m     out_np \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhisper_input_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhisper_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrosa_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrosa_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# returns np.uint8 [B, H, W, 3]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out_np, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_np\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m, in \u001b[0;36mMuseTalkGenerator.forward\u001b[0;34m(self, whisper_input_features, latent_inputs, frame_idx, librosa_length)\u001b[0m\n\u001b[1;32m    124\u001b[0m pred_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(\n\u001b[1;32m    125\u001b[0m     latent_batch,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps,\n\u001b[1;32m    127\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39maudio_feature_batch,\n\u001b[1;32m    128\u001b[0m )\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    130\u001b[0m pred_latents \u001b[38;5;241m=\u001b[39m pred_latents\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_device, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 131\u001b[0m recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_latents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recon\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mMuseTalkGenerator.decode_latents\u001b[0;34m(self, latents)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mDecode latent variables back into an image.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m:param latents: The latent variables to decode.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m:return: A NumPy array representing the decoded image.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m latents \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor) \u001b[38;5;241m*\u001b[39m latents\n\u001b[0;32m---> 49\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     50\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:321\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:290\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiled_decode(z, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_quant_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MuseTalk/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "compare_torch_onnx(model, \"musetalk_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f18f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exactly the same shapes as in your export\n",
    "onnx_path = \"musetalk_model.onnx\"\n",
    "frame_idx: int = 0\n",
    "librosa_length: int = 106_496\n",
    "torch.manual_seed(0)\n",
    "whisper_feats = torch.randn(1, 80, 3000)       # float32\n",
    "latent_inputs = torch.randn(8, 8, 32, 32)      # float32\n",
    "\n",
    "torch_out = run_torch_model(\n",
    "    model=model,\n",
    "    whisper_feats=whisper_feats,\n",
    "    latent_inputs=latent_inputs,\n",
    "    frame_idx=frame_idx,\n",
    "    librosa_length=librosa_length,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del vae\n",
    "del unet\n",
    "del whisper\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92a52946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-11-14 17:27:51.963952150 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch output shape: (8, 256, 256, 3) uint8\n",
      "ONNX  output shape: (8, 256, 256, 3) uint8\n",
      "max |diff|: 255\n",
      "mean |diff|: 98.86735979715984\n",
      "all equal: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[ 50,  61,  92],\n",
       "          [  5,   9,  37],\n",
       "          [  0,   0,  14],\n",
       "          ...,\n",
       "          [175, 152, 159],\n",
       "          [165, 150, 158],\n",
       "          [149, 138, 150]],\n",
       " \n",
       "         [[ 25,  32,  75],\n",
       "          [  0,   0,  15],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [188, 152, 160],\n",
       "          [180, 152, 162],\n",
       "          [159, 140, 153]],\n",
       " \n",
       "         [[  6,   5,  62],\n",
       "          [  0,   0,  15],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [189, 154, 160],\n",
       "          [178, 150, 158],\n",
       "          [161, 137, 152]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 31,  45,  67],\n",
       "          [ 21,  34,  58],\n",
       "          [ 17,  38,  67],\n",
       "          ...,\n",
       "          [124, 124, 123],\n",
       "          [106, 103, 105],\n",
       "          [115, 111, 116]],\n",
       " \n",
       "         [[ 44,  56,  76],\n",
       "          [ 26,  35,  59],\n",
       "          [ 22,  37,  62],\n",
       "          ...,\n",
       "          [126, 124, 128],\n",
       "          [110, 105, 111],\n",
       "          [118, 114, 121]],\n",
       " \n",
       "         [[ 59,  68,  88],\n",
       "          [ 37,  45,  69],\n",
       "          [ 28,  37,  62],\n",
       "          ...,\n",
       "          [123, 124, 132],\n",
       "          [115, 118, 126],\n",
       "          [120, 121, 130]]],\n",
       " \n",
       " \n",
       "        [[[129, 141, 144],\n",
       "          [130, 142, 141],\n",
       "          [128, 134, 129],\n",
       "          ...,\n",
       "          [  0,   0,  10],\n",
       "          [  0,   5,  18],\n",
       "          [ 28,  42,  57]],\n",
       " \n",
       "         [[142, 153, 158],\n",
       "          [144, 151, 153],\n",
       "          [134, 132, 133],\n",
       "          ...,\n",
       "          [  3,   0,   7],\n",
       "          [  3,   0,   2],\n",
       "          [ 12,  17,  29]],\n",
       " \n",
       "         [[154, 160, 164],\n",
       "          [155, 161, 157],\n",
       "          [131, 134, 129],\n",
       "          ...,\n",
       "          [ 31,  32,  37],\n",
       "          [ 22,  18,  17],\n",
       "          [ 18,  21,  29]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 97,  95,  80],\n",
       "          [ 77,  70,  53],\n",
       "          [ 69,  61,  45],\n",
       "          ...,\n",
       "          [ 92, 139, 180],\n",
       "          [101, 130, 167],\n",
       "          [105, 120, 147]],\n",
       " \n",
       "         [[ 85,  86,  74],\n",
       "          [ 54,  47,  37],\n",
       "          [ 52,  40,  36],\n",
       "          ...,\n",
       "          [107, 144, 184],\n",
       "          [112, 137, 172],\n",
       "          [110, 126, 151]],\n",
       " \n",
       "         [[ 92,  88,  83],\n",
       "          [ 59,  52,  44],\n",
       "          [ 56,  41,  37],\n",
       "          ...,\n",
       "          [103, 138, 174],\n",
       "          [110, 139, 167],\n",
       "          [113, 131, 151]]],\n",
       " \n",
       " \n",
       "        [[[129, 161, 181],\n",
       "          [133, 172, 193],\n",
       "          [150, 182, 196],\n",
       "          ...,\n",
       "          [158, 164, 167],\n",
       "          [139, 148, 156],\n",
       "          [126, 136, 149]],\n",
       " \n",
       "         [[127, 170, 196],\n",
       "          [138, 180, 211],\n",
       "          [168, 197, 216],\n",
       "          ...,\n",
       "          [184, 177, 178],\n",
       "          [166, 163, 169],\n",
       "          [133, 136, 148]],\n",
       " \n",
       "         [[123, 167, 198],\n",
       "          [138, 181, 211],\n",
       "          [178, 207, 213],\n",
       "          ...,\n",
       "          [197, 191, 182],\n",
       "          [175, 174, 170],\n",
       "          [143, 140, 151]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 35,  51,  69],\n",
       "          [ 44,  60,  81],\n",
       "          [ 62,  84, 105],\n",
       "          ...,\n",
       "          [255, 215, 160],\n",
       "          [248, 193, 144],\n",
       "          [229, 170, 130]],\n",
       " \n",
       "         [[ 46,  60,  78],\n",
       "          [ 45,  58,  80],\n",
       "          [ 64,  81, 102],\n",
       "          ...,\n",
       "          [255, 206, 159],\n",
       "          [252, 199, 155],\n",
       "          [230, 176, 143]],\n",
       " \n",
       "         [[ 61,  73,  91],\n",
       "          [ 52,  65,  87],\n",
       "          [ 64,  77,  98],\n",
       "          ...,\n",
       "          [249, 202, 160],\n",
       "          [234, 197, 155],\n",
       "          [210, 172, 146]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[118, 126, 125],\n",
       "          [134, 142, 134],\n",
       "          [165, 163, 148],\n",
       "          ...,\n",
       "          [218, 221, 217],\n",
       "          [186, 192, 191],\n",
       "          [154, 159, 167]],\n",
       " \n",
       "         [[118, 127, 126],\n",
       "          [141, 143, 139],\n",
       "          [177, 171, 158],\n",
       "          ...,\n",
       "          [255, 248, 249],\n",
       "          [230, 226, 226],\n",
       "          [180, 178, 185]],\n",
       " \n",
       "         [[125, 131, 130],\n",
       "          [149, 150, 143],\n",
       "          [180, 176, 159],\n",
       "          ...,\n",
       "          [255, 249, 246],\n",
       "          [237, 230, 227],\n",
       "          [187, 181, 186]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 83, 109, 107],\n",
       "          [ 92, 117, 112],\n",
       "          [103, 134, 126],\n",
       "          ...,\n",
       "          [ 86, 126, 141],\n",
       "          [ 85, 120, 134],\n",
       "          [ 80, 104, 119]],\n",
       " \n",
       "         [[ 89, 109, 109],\n",
       "          [ 83, 102, 101],\n",
       "          [ 92, 117, 114],\n",
       "          ...,\n",
       "          [ 87, 119, 136],\n",
       "          [ 92, 121, 136],\n",
       "          [ 87, 110, 125]],\n",
       " \n",
       "         [[ 98, 115, 118],\n",
       "          [ 91, 111, 111],\n",
       "          [ 93, 114, 114],\n",
       "          ...,\n",
       "          [ 87, 116, 134],\n",
       "          [ 92, 122, 137],\n",
       "          [ 96, 117, 129]]],\n",
       " \n",
       " \n",
       "        [[[110, 126, 154],\n",
       "          [118, 137, 168],\n",
       "          [133, 145, 176],\n",
       "          ...,\n",
       "          [ 74,  56,  39],\n",
       "          [ 61,  52,  45],\n",
       "          [ 67,  66,  69]],\n",
       " \n",
       "         [[109, 124, 162],\n",
       "          [117, 131, 178],\n",
       "          [137, 150, 194],\n",
       "          ...,\n",
       "          [ 80,  46,  22],\n",
       "          [ 64,  34,  22],\n",
       "          [ 53,  39,  40]],\n",
       " \n",
       "         [[106, 118, 162],\n",
       "          [112, 124, 177],\n",
       "          [132, 149, 200],\n",
       "          ...,\n",
       "          [ 78,  41,  11],\n",
       "          [ 57,  24,   8],\n",
       "          [ 49,  31,  30]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 40,  45,  64],\n",
       "          [ 26,  29,  46],\n",
       "          [ 42,  43,  59],\n",
       "          ...,\n",
       "          [ 36,  43,  66],\n",
       "          [ 42,  45,  62],\n",
       "          [ 75,  70,  80]],\n",
       " \n",
       "         [[ 67,  71,  83],\n",
       "          [ 54,  52,  67],\n",
       "          [ 88,  81,  93],\n",
       "          ...,\n",
       "          [ 51,  52,  78],\n",
       "          [ 61,  59,  76],\n",
       "          [ 82,  76,  87]],\n",
       " \n",
       "         [[ 91,  94, 107],\n",
       "          [ 89,  85,  96],\n",
       "          [112,  99, 107],\n",
       "          ...,\n",
       "          [ 64,  68,  94],\n",
       "          [ 74,  78,  96],\n",
       "          [ 91,  92, 103]]],\n",
       " \n",
       " \n",
       "        [[[ 88, 109, 137],\n",
       "          [ 90, 116, 146],\n",
       "          [102, 125, 154],\n",
       "          ...,\n",
       "          [123, 133, 148],\n",
       "          [123, 139, 150],\n",
       "          [117, 131, 146]],\n",
       " \n",
       "         [[ 81, 105, 141],\n",
       "          [ 85, 109, 150],\n",
       "          [102, 130, 168],\n",
       "          ...,\n",
       "          [143, 154, 174],\n",
       "          [149, 163, 177],\n",
       "          [130, 145, 162]],\n",
       " \n",
       "         [[ 71,  94, 136],\n",
       "          [ 74, 100, 145],\n",
       "          [ 92, 128, 168],\n",
       "          ...,\n",
       "          [155, 171, 191],\n",
       "          [158, 176, 187],\n",
       "          [141, 156, 171]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[104, 116, 135],\n",
       "          [109, 122, 142],\n",
       "          [117, 134, 156],\n",
       "          ...,\n",
       "          [ 53,  72,  97],\n",
       "          [ 52,  69,  92],\n",
       "          [ 66,  77,  96]],\n",
       " \n",
       "         [[108, 118, 135],\n",
       "          [102, 110, 132],\n",
       "          [109, 119, 145],\n",
       "          ...,\n",
       "          [ 61,  77, 105],\n",
       "          [ 64,  79, 102],\n",
       "          [ 71,  82, 102]],\n",
       " \n",
       "         [[109, 119, 137],\n",
       "          [103, 114, 133],\n",
       "          [107, 115, 138],\n",
       "          ...,\n",
       "          [ 69,  86, 110],\n",
       "          [ 72,  89, 110],\n",
       "          [ 83,  95, 111]]]], dtype=uint8),\n",
       " array([[[[ 88,  68,  59],\n",
       "          [ 69,  45,  37],\n",
       "          [ 62,  37,  28],\n",
       "          ...,\n",
       "          [132, 124, 123],\n",
       "          [126, 118, 115],\n",
       "          [130, 121, 120]],\n",
       " \n",
       "         [[ 76,  56,  44],\n",
       "          [ 58,  35,  26],\n",
       "          [ 62,  37,  22],\n",
       "          ...,\n",
       "          [128, 124, 126],\n",
       "          [111, 105, 109],\n",
       "          [121, 114, 117]],\n",
       " \n",
       "         [[ 67,  45,  31],\n",
       "          [ 58,  34,  21],\n",
       "          [ 67,  38,  17],\n",
       "          ...,\n",
       "          [123, 124, 124],\n",
       "          [105, 103, 106],\n",
       "          [116, 111, 115]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 62,   5,   6],\n",
       "          [ 15,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [160, 154, 189],\n",
       "          [158, 150, 178],\n",
       "          [152, 137, 161]],\n",
       " \n",
       "         [[ 75,  32,  25],\n",
       "          [ 15,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [160, 152, 188],\n",
       "          [162, 152, 180],\n",
       "          [153, 140, 159]],\n",
       " \n",
       "         [[ 92,  61,  50],\n",
       "          [ 37,   9,   5],\n",
       "          [ 14,   0,   0],\n",
       "          ...,\n",
       "          [159, 152, 175],\n",
       "          [158, 150, 165],\n",
       "          [150, 138, 149]]],\n",
       " \n",
       " \n",
       "        [[[ 83,  88,  92],\n",
       "          [ 44,  52,  60],\n",
       "          [ 37,  41,  56],\n",
       "          ...,\n",
       "          [174, 138, 103],\n",
       "          [167, 139, 110],\n",
       "          [151, 131, 113]],\n",
       " \n",
       "         [[ 74,  86,  85],\n",
       "          [ 37,  47,  54],\n",
       "          [ 36,  40,  52],\n",
       "          ...,\n",
       "          [184, 144, 107],\n",
       "          [172, 137, 112],\n",
       "          [151, 126, 110]],\n",
       " \n",
       "         [[ 80,  95,  97],\n",
       "          [ 53,  70,  77],\n",
       "          [ 45,  61,  69],\n",
       "          ...,\n",
       "          [180, 139,  92],\n",
       "          [167, 130, 101],\n",
       "          [147, 120, 105]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[164, 160, 154],\n",
       "          [157, 161, 155],\n",
       "          [129, 134, 131],\n",
       "          ...,\n",
       "          [ 37,  32,  31],\n",
       "          [ 17,  18,  22],\n",
       "          [ 29,  21,  18]],\n",
       " \n",
       "         [[158, 153, 142],\n",
       "          [153, 151, 144],\n",
       "          [133, 132, 134],\n",
       "          ...,\n",
       "          [  7,   0,   3],\n",
       "          [  2,   0,   3],\n",
       "          [ 29,  17,  12]],\n",
       " \n",
       "         [[144, 141, 129],\n",
       "          [141, 142, 130],\n",
       "          [129, 134, 128],\n",
       "          ...,\n",
       "          [ 10,   0,   0],\n",
       "          [ 18,   5,   0],\n",
       "          [ 57,  42,  28]]],\n",
       " \n",
       " \n",
       "        [[[ 91,  73,  61],\n",
       "          [ 87,  65,  52],\n",
       "          [ 98,  77,  64],\n",
       "          ...,\n",
       "          [160, 202, 249],\n",
       "          [155, 197, 234],\n",
       "          [146, 172, 210]],\n",
       " \n",
       "         [[ 78,  60,  46],\n",
       "          [ 80,  58,  45],\n",
       "          [102,  81,  64],\n",
       "          ...,\n",
       "          [159, 206, 255],\n",
       "          [155, 199, 252],\n",
       "          [143, 176, 230]],\n",
       " \n",
       "         [[ 69,  51,  35],\n",
       "          [ 81,  60,  44],\n",
       "          [105,  84,  62],\n",
       "          ...,\n",
       "          [160, 215, 255],\n",
       "          [144, 193, 248],\n",
       "          [130, 170, 229]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[198, 167, 123],\n",
       "          [211, 181, 138],\n",
       "          [213, 207, 178],\n",
       "          ...,\n",
       "          [182, 191, 197],\n",
       "          [170, 174, 175],\n",
       "          [151, 140, 143]],\n",
       " \n",
       "         [[196, 170, 128],\n",
       "          [211, 180, 138],\n",
       "          [216, 197, 168],\n",
       "          ...,\n",
       "          [178, 177, 184],\n",
       "          [169, 163, 166],\n",
       "          [148, 136, 133]],\n",
       " \n",
       "         [[181, 161, 129],\n",
       "          [193, 172, 133],\n",
       "          [196, 182, 150],\n",
       "          ...,\n",
       "          [167, 164, 158],\n",
       "          [156, 148, 139],\n",
       "          [149, 136, 126]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[118, 115,  98],\n",
       "          [111, 110,  91],\n",
       "          [114, 114,  93],\n",
       "          ...,\n",
       "          [134, 116,  88],\n",
       "          [137, 122,  92],\n",
       "          [129, 117,  96]],\n",
       " \n",
       "         [[109, 109,  89],\n",
       "          [101, 102,  83],\n",
       "          [114, 117,  92],\n",
       "          ...,\n",
       "          [136, 119,  87],\n",
       "          [136, 121,  92],\n",
       "          [125, 110,  87]],\n",
       " \n",
       "         [[107, 109,  83],\n",
       "          [112, 116,  92],\n",
       "          [126, 134, 103],\n",
       "          ...,\n",
       "          [141, 126,  86],\n",
       "          [134, 120,  85],\n",
       "          [119, 104,  80]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[130, 131, 125],\n",
       "          [143, 150, 149],\n",
       "          [159, 176, 180],\n",
       "          ...,\n",
       "          [246, 249, 255],\n",
       "          [227, 230, 237],\n",
       "          [186, 181, 187]],\n",
       " \n",
       "         [[126, 127, 118],\n",
       "          [139, 143, 141],\n",
       "          [158, 171, 177],\n",
       "          ...,\n",
       "          [249, 248, 255],\n",
       "          [226, 226, 230],\n",
       "          [185, 178, 180]],\n",
       " \n",
       "         [[125, 126, 118],\n",
       "          [134, 142, 134],\n",
       "          [148, 163, 165],\n",
       "          ...,\n",
       "          [217, 221, 218],\n",
       "          [191, 192, 186],\n",
       "          [167, 159, 154]]],\n",
       " \n",
       " \n",
       "        [[[107,  93,  91],\n",
       "          [ 96,  85,  89],\n",
       "          [107,  99, 112],\n",
       "          ...,\n",
       "          [ 94,  68,  64],\n",
       "          [ 96,  78,  74],\n",
       "          [103,  92,  91]],\n",
       " \n",
       "         [[ 83,  71,  67],\n",
       "          [ 67,  52,  54],\n",
       "          [ 93,  81,  88],\n",
       "          ...,\n",
       "          [ 78,  52,  51],\n",
       "          [ 76,  59,  61],\n",
       "          [ 87,  76,  82]],\n",
       " \n",
       "         [[ 64,  45,  40],\n",
       "          [ 46,  29,  26],\n",
       "          [ 59,  43,  42],\n",
       "          ...,\n",
       "          [ 66,  43,  36],\n",
       "          [ 62,  45,  42],\n",
       "          [ 80,  70,  75]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[162, 118, 106],\n",
       "          [177, 124, 112],\n",
       "          [200, 149, 132],\n",
       "          ...,\n",
       "          [ 11,  41,  78],\n",
       "          [  8,  25,  57],\n",
       "          [ 30,  31,  49]],\n",
       " \n",
       "         [[162, 124, 109],\n",
       "          [178, 131, 117],\n",
       "          [194, 150, 137],\n",
       "          ...,\n",
       "          [ 22,  46,  80],\n",
       "          [ 22,  34,  64],\n",
       "          [ 40,  40,  53]],\n",
       " \n",
       "         [[154, 126, 110],\n",
       "          [168, 137, 118],\n",
       "          [176, 145, 133],\n",
       "          ...,\n",
       "          [ 39,  56,  74],\n",
       "          [ 45,  52,  61],\n",
       "          [ 69,  66,  67]]],\n",
       " \n",
       " \n",
       "        [[[136, 119, 109],\n",
       "          [133, 114, 103],\n",
       "          [138, 115, 107],\n",
       "          ...,\n",
       "          [110,  86,  69],\n",
       "          [110,  89,  72],\n",
       "          [111,  95,  83]],\n",
       " \n",
       "         [[135, 118, 108],\n",
       "          [132, 110, 102],\n",
       "          [145, 119, 109],\n",
       "          ...,\n",
       "          [105,  77,  61],\n",
       "          [102,  78,  64],\n",
       "          [102,  82,  71]],\n",
       " \n",
       "         [[135, 116, 104],\n",
       "          [142, 122, 109],\n",
       "          [156, 134, 117],\n",
       "          ...,\n",
       "          [ 97,  72,  53],\n",
       "          [ 92,  69,  52],\n",
       "          [ 96,  77,  66]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[136,  94,  71],\n",
       "          [145, 100,  74],\n",
       "          [168, 128,  91],\n",
       "          ...,\n",
       "          [191, 171, 155],\n",
       "          [187, 176, 158],\n",
       "          [171, 156, 141]],\n",
       " \n",
       "         [[141, 105,  81],\n",
       "          [150, 109,  85],\n",
       "          [168, 130, 102],\n",
       "          ...,\n",
       "          [174, 154, 143],\n",
       "          [177, 163, 149],\n",
       "          [162, 145, 130]],\n",
       " \n",
       "         [[137, 109,  88],\n",
       "          [146, 116,  90],\n",
       "          [154, 125, 102],\n",
       "          ...,\n",
       "          [148, 133, 123],\n",
       "          [150, 139, 123],\n",
       "          [146, 131, 117]]]], dtype=uint8),\n",
       " array([[[[ 38,   7,  33],\n",
       "          [ 64,  36,   0],\n",
       "          [ 62,  37,  14],\n",
       "          ...,\n",
       "          [ 43,  28,  36],\n",
       "          [ 39,  32,  43],\n",
       "          [ 19,  17,  30]],\n",
       " \n",
       "         [[ 51,  24,  31],\n",
       "          [ 58,  35,  11],\n",
       "          [ 62,  37,  22],\n",
       "          ...,\n",
       "          [ 60,  28,  34],\n",
       "          [ 69,  47,  53],\n",
       "          [ 38,  26,  36]],\n",
       " \n",
       "         [[ 61,  40,  31],\n",
       "          [ 58,  34,   6],\n",
       "          [ 67,  38,  17],\n",
       "          ...,\n",
       "          [ 66,  30,  36],\n",
       "          [ 73,  47,  52],\n",
       "          [ 45,  26,  37]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 31,  40,  61],\n",
       "          [  6,  34,  58],\n",
       "          [ 17,  38,  67],\n",
       "          ...,\n",
       "          [ 36,  30,  66],\n",
       "          [ 52,  47,  73],\n",
       "          [ 37,  26,  45]],\n",
       " \n",
       "         [[ 31,  24,  51],\n",
       "          [ 11,  35,  59],\n",
       "          [ 22,  37,  62],\n",
       "          ...,\n",
       "          [ 34,  28,  60],\n",
       "          [ 52,  47,  69],\n",
       "          [ 35,  26,  38]],\n",
       " \n",
       "         [[ 33,   7,  38],\n",
       "          [  0,  36,  64],\n",
       "          [ 14,  37,  62],\n",
       "          ...,\n",
       "          [ 36,  28,  43],\n",
       "          [ 43,  32,  39],\n",
       "          [ 30,  17,  19]]],\n",
       " \n",
       " \n",
       "        [[[ 46,  53,  52],\n",
       "          [ 86,  90,  81],\n",
       "          [ 91,  93,  73],\n",
       "          ...,\n",
       "          [174, 138,  93],\n",
       "          [167, 134,  92],\n",
       "          [123,  89,  56]],\n",
       " \n",
       "         [[ 68,  67,  73],\n",
       "          [107, 104,  99],\n",
       "          [ 98,  92,  81],\n",
       "          ...,\n",
       "          [181, 144, 100],\n",
       "          [169, 137, 110],\n",
       "          [139, 109,  81]],\n",
       " \n",
       "         [[ 74,  65,  67],\n",
       "          [102,  91,  80],\n",
       "          [ 86,  73,  60],\n",
       "          ...,\n",
       "          [149, 107,  55],\n",
       "          [145, 112,  84],\n",
       "          [129,  99,  76]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 67,  65,  74],\n",
       "          [ 80,  91, 102],\n",
       "          [ 60,  73,  86],\n",
       "          ...,\n",
       "          [ 55, 107, 149],\n",
       "          [ 84, 112, 145],\n",
       "          [ 76,  99, 129]],\n",
       " \n",
       "         [[ 73,  67,  68],\n",
       "          [ 99, 104, 107],\n",
       "          [ 81,  92,  98],\n",
       "          ...,\n",
       "          [100, 144, 181],\n",
       "          [110, 137, 169],\n",
       "          [ 81, 109, 139]],\n",
       " \n",
       "         [[ 52,  53,  46],\n",
       "          [ 82,  90,  86],\n",
       "          [ 73,  93,  91],\n",
       "          ...,\n",
       "          [ 93, 138, 174],\n",
       "          [ 92, 134, 167],\n",
       "          [ 56,  89, 123]]],\n",
       " \n",
       " \n",
       "        [[[ 38,  88, 120],\n",
       "          [ 46, 107, 141],\n",
       "          [ 52, 105, 132],\n",
       "          ...,\n",
       "          [  2,  38,  82],\n",
       "          [ 16,  49,  78],\n",
       "          [ 20,  36,  61]],\n",
       " \n",
       "         [[ 49, 110, 150],\n",
       "          [ 58, 122, 166],\n",
       "          [ 66, 116, 152],\n",
       "          ...,\n",
       "          [ 25,  29,  77],\n",
       "          [ 11,  36,  83],\n",
       "          [ 10,  40,  82]],\n",
       " \n",
       "         [[ 54, 116, 163],\n",
       "          [ 57, 121, 167],\n",
       "          [ 73, 123, 151],\n",
       "          ...,\n",
       "          [ 37,  24,  73],\n",
       "          [ 31,  19,  78],\n",
       "          [ 13,  30,  78]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[163, 116,  54],\n",
       "          [167, 121,  57],\n",
       "          [151, 123,  73],\n",
       "          ...,\n",
       "          [ 73,  24,  37],\n",
       "          [ 78,  19,  31],\n",
       "          [ 78,  30,  13]],\n",
       " \n",
       "         [[150, 110,  50],\n",
       "          [166, 122,  58],\n",
       "          [152, 116,  66],\n",
       "          ...,\n",
       "          [ 77,  29,  25],\n",
       "          [ 83,  36,  11],\n",
       "          [ 82,  40,  10]],\n",
       " \n",
       "         [[120,  88,  38],\n",
       "          [141, 107,  46],\n",
       "          [132, 105,  52],\n",
       "          ...,\n",
       "          [ 82,  38,   2],\n",
       "          [ 78,  49,  16],\n",
       "          [ 61,  36,  20]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[  0,  11,  27],\n",
       "          [ 23,  32,  43],\n",
       "          [ 51,  49,  55],\n",
       "          ...,\n",
       "          [ 84, 105, 129],\n",
       "          [ 49,  70,  99],\n",
       "          [ 25,  42,  71]],\n",
       " \n",
       "         [[  9,  18,  37],\n",
       "          [ 40,  41,  56],\n",
       "          [ 63,  54,  66],\n",
       "          ...,\n",
       "          [119, 129, 162],\n",
       "          [ 94, 105, 134],\n",
       "          [ 55,  68,  98]],\n",
       " \n",
       "         [[ 18,  22,  47],\n",
       "          [ 37,  34,  51],\n",
       "          [ 54,  42,  56],\n",
       "          ...,\n",
       "          [114, 123, 160],\n",
       "          [103, 110, 142],\n",
       "          [ 68,  77, 106]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 47,  22,  18],\n",
       "          [ 51,  33,  37],\n",
       "          [ 56,  42,  54],\n",
       "          ...,\n",
       "          [160, 123, 114],\n",
       "          [142, 110, 103],\n",
       "          [106,  77,  68]],\n",
       " \n",
       "         [[ 37,  18,   9],\n",
       "          [ 56,  41,  40],\n",
       "          [ 66,  54,  63],\n",
       "          ...,\n",
       "          [162, 129, 119],\n",
       "          [134, 105,  94],\n",
       "          [ 98,  68,  55]],\n",
       " \n",
       "         [[ 27,  11,   0],\n",
       "          [ 43,  31,  23],\n",
       "          [ 55,  49,  51],\n",
       "          ...,\n",
       "          [130, 105,  84],\n",
       "          [ 99,  70,  49],\n",
       "          [ 71,  42,  25]]],\n",
       " \n",
       " \n",
       "        [[[  3,  33,  63],\n",
       "          [ 22,  52,  79],\n",
       "          [ 26,  46,  64],\n",
       "          ...,\n",
       "          [ 20,  12,  25],\n",
       "          [ 35,  26,  29],\n",
       "          [ 36,  26,  22]],\n",
       " \n",
       "         [[ 26,  53,  95],\n",
       "          [ 50,  79, 124],\n",
       "          [ 44,  69, 106],\n",
       "          ...,\n",
       "          [  2,   6,  29],\n",
       "          [ 12,  25,  39],\n",
       "          [ 34,  37,  42]],\n",
       " \n",
       "         [[ 42,  73, 122],\n",
       "          [ 66,  95, 151],\n",
       "          [ 73, 106, 158],\n",
       "          ...,\n",
       "          [ 12,   2,  25],\n",
       "          [  5,  21,  34],\n",
       "          [ 31,  39,  45]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[122,  73,  42],\n",
       "          [151,  95,  66],\n",
       "          [158, 106,  73],\n",
       "          ...,\n",
       "          [ 25,   2,  12],\n",
       "          [ 34,  20,   5],\n",
       "          [ 45,  39,  31]],\n",
       " \n",
       "         [[ 95,  53,  26],\n",
       "          [124,  79,  50],\n",
       "          [106,  69,  44],\n",
       "          ...,\n",
       "          [ 29,   6,   2],\n",
       "          [ 39,  25,  12],\n",
       "          [ 42,  36,  34]],\n",
       " \n",
       "         [[ 63,  32,   3],\n",
       "          [ 79,  52,  22],\n",
       "          [ 64,  46,  26],\n",
       "          ...,\n",
       "          [ 25,  12,  20],\n",
       "          [ 29,  26,  35],\n",
       "          [ 22,  26,  36]]],\n",
       " \n",
       " \n",
       "        [[[ 48,  10,  28],\n",
       "          [ 43,   2,  43],\n",
       "          [ 36,  10,  47],\n",
       "          ...,\n",
       "          [ 13,  47,  79],\n",
       "          [ 13,  50,  78],\n",
       "          [  6,  36,  63]],\n",
       " \n",
       "         [[ 54,  13,  33],\n",
       "          [ 47,   1,  48],\n",
       "          [ 43,  11,  59],\n",
       "          ...,\n",
       "          [ 38,  77, 113],\n",
       "          [ 47,  85, 113],\n",
       "          [ 28,  63,  91]],\n",
       " \n",
       "         [[ 64,  22,  32],\n",
       "          [ 68,  22,  36],\n",
       "          [ 64,   6,  51],\n",
       "          ...,\n",
       "          [ 58,  99, 138],\n",
       "          [ 66, 107, 135],\n",
       "          [ 45,  79, 105]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 32,  22,  64],\n",
       "          [ 36,  22,  68],\n",
       "          [ 51,   6,  65],\n",
       "          ...,\n",
       "          [138,  99,  58],\n",
       "          [135, 107,  66],\n",
       "          [105,  79,  45]],\n",
       " \n",
       "         [[ 33,  13,  54],\n",
       "          [ 48,   1,  47],\n",
       "          [ 59,  11,  43],\n",
       "          ...,\n",
       "          [113,  77,  38],\n",
       "          [113,  84,  47],\n",
       "          [ 91,  63,  28]],\n",
       " \n",
       "         [[ 28,  10,  49],\n",
       "          [ 43,   2,  43],\n",
       "          [ 47,  10,  36],\n",
       "          ...,\n",
       "          [ 79,  47,  13],\n",
       "          [ 78,  50,  13],\n",
       "          [ 63,  36,   6]]]], dtype=int16))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "onnx_out = run_onnx_model(\n",
    "    onnx_path=onnx_path,\n",
    "    whisper_feats=whisper_feats,\n",
    "    latent_inputs=latent_inputs,\n",
    "    use_cuda=True,   # or False if you want pure CPU\n",
    ")\n",
    "\n",
    "print(\"Torch output shape:\", torch_out.shape, torch_out.dtype)\n",
    "print(\"ONNX  output shape:\", onnx_out.shape, onnx_out.dtype)\n",
    "\n",
    "# Convert to int16 to avoid uint8 wraparound on subtraction\n",
    "diff = torch_out.astype(np.int16) - onnx_out.astype(np.int16)\n",
    "abs_diff = np.abs(diff)\n",
    "\n",
    "print(\"max |diff|:\", abs_diff.max())\n",
    "print(\"mean |diff|:\", abs_diff.mean())\n",
    "print(\"all equal:\", np.array_equal(torch_out, onnx_out))\n",
    "\n",
    "torch_out, onnx_out, abs_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
